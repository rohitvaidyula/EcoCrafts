{"ast":null,"code":"/**\r\n * @license\r\n * Copyright 2018 Google LLC\r\n *\r\n * Use of this source code is governed by an MIT-style\r\n * license that can be found in the LICENSE file or at\r\n * https://opensource.org/licenses/MIT.\r\n * =============================================================================\r\n */\nimport { InputLayer } from './engine/input_layer';\nimport { Layer } from './engine/topology';\nimport { input } from './exports';\nimport { ELU, LeakyReLU, PReLU, ReLU, Softmax, ThresholdedReLU } from './layers/advanced_activations';\nimport { Conv1D, Conv2D, Conv2DTranspose, Conv3D, Cropping2D, SeparableConv2D, UpSampling2D } from './layers/convolutional';\nimport { DepthwiseConv2D } from './layers/convolutional_depthwise';\nimport { ConvLSTM2D, ConvLSTM2DCell } from './layers/convolutional_recurrent';\nimport { Activation, Dense, Dropout, Flatten, Masking, Permute, RepeatVector, Reshape, SpatialDropout1D } from './layers/core';\nimport { Embedding } from './layers/embeddings';\nimport { Add, Average, Concatenate, Dot, Maximum, Minimum, Multiply } from './layers/merge';\nimport { AlphaDropout, GaussianDropout, GaussianNoise } from './layers/noise';\nimport { BatchNormalization, LayerNormalization } from './layers/normalization';\nimport { ZeroPadding2D } from './layers/padding';\nimport { AveragePooling1D, AveragePooling2D, AveragePooling3D, GlobalAveragePooling1D, GlobalAveragePooling2D, GlobalMaxPooling1D, GlobalMaxPooling2D, MaxPooling1D, MaxPooling2D, MaxPooling3D } from './layers/pooling';\nimport { GRU, GRUCell, LSTM, LSTMCell, RNN, RNNCell, SimpleRNN, SimpleRNNCell, StackedRNNCells } from './layers/recurrent';\nimport { Bidirectional, TimeDistributed } from './layers/wrappers';\n// TODO(cais): Add doc string to all the public static functions in this\n//   class; include exectuable JavaScript code snippets where applicable\n//   (b/74074458).\n// Input Layer.\n/**\r\n * An input layer is an entry point into a `tf.LayersModel`.\r\n *\r\n * `InputLayer` is generated automatically for `tf.Sequential`` models by\r\n * specifying the `inputshape` or `batchInputShape` for the first layer.  It\r\n * should not be specified explicitly. However, it can be useful sometimes,\r\n * e.g., when constructing a sequential model from a subset of another\r\n * sequential model's layers. Like the code snippet below shows.\r\n *\r\n * ```js\r\n * // Define a model which simply adds two inputs.\r\n * const model1 = tf.sequential();\r\n * model1.add(tf.layers.dense({inputShape: [4], units: 3, activation: 'relu'}));\r\n * model1.add(tf.layers.dense({units: 1, activation: 'sigmoid'}));\r\n * model1.summary();\r\n * model1.predict(tf.zeros([1, 4])).print();\r\n *\r\n * // Construct another model, reusing the second layer of `model1` while\r\n * // not using the first layer of `model1`. Note that you cannot add the second\r\n * // layer of `model` directly as the first layer of the new sequential model,\r\n * // because doing so will lead to an error related to the fact that the layer\r\n * // is not an input layer. Instead, you need to create an `inputLayer` and add\r\n * // it to the new sequential model before adding the reused layer.\r\n * const model2 = tf.sequential();\r\n * // Use an inputShape that matches the input shape of `model1`'s second\r\n * // layer.\r\n * model2.add(tf.layers.inputLayer({inputShape: [3]}));\r\n * model2.add(model1.layers[1]);\r\n * model2.summary();\r\n * model2.predict(tf.zeros([1, 3])).print();\r\n * ```\r\n *\r\n * @doc {heading: 'Layers', subheading: 'Inputs', namespace: 'layers'}\r\n */\nexport function inputLayer(args) {\n  return new InputLayer(args);\n}\n// Advanced Activation Layers.\n/**\r\n * Exponetial Linear Unit (ELU).\r\n *\r\n * It follows:\r\n * `f(x) =  alpha * (exp(x) - 1.) for x < 0`,\r\n * `f(x) = x for x >= 0`.\r\n *\r\n * Input shape:\r\n *   Arbitrary. Use the configuration `inputShape` when using this layer as the\r\n *   first layer in a model.\r\n *\r\n * Output shape:\r\n *   Same shape as the input.\r\n *\r\n * References:\r\n *   - [Fast and Accurate Deep Network Learning by Exponential Linear Units\r\n * (ELUs)](https://arxiv.org/abs/1511.07289v1)\r\n *\r\n * @doc {\r\n *   heading: 'Layers',\r\n *   subheading: 'Advanced Activation',\r\n *   namespace: 'layers'\r\n * }\r\n */\nexport function elu(args) {\n  return new ELU(args);\n}\n/**\r\n * Rectified Linear Unit activation function.\r\n *\r\n * Input shape:\r\n *   Arbitrary. Use the config field `inputShape` (Array of integers, does\r\n *   not include the sample axis) when using this layer as the first layer\r\n *   in a model.\r\n *\r\n * Output shape:\r\n *   Same shape as the input.\r\n *\r\n * @doc {\r\n *   heading: 'Layers',\r\n *   subheading: 'Advanced Activation',\r\n *   namespace: 'layers'\r\n * }\r\n */\nexport function reLU(args) {\n  return new ReLU(args);\n}\n/**\r\n * Leaky version of a rectified linear unit.\r\n *\r\n * It allows a small gradient when the unit is not active:\r\n * `f(x) = alpha * x for x < 0.`\r\n * `f(x) = x for x >= 0.`\r\n *\r\n * Input shape:\r\n *   Arbitrary. Use the configuration `inputShape` when using this layer as the\r\n *   first layer in a model.\r\n *\r\n * Output shape:\r\n *   Same shape as the input.\r\n *\r\n * @doc {\r\n *   heading: 'Layers',\r\n *   subheading: 'Advanced Activation',\r\n *   namespace: 'layers'\r\n * }\r\n */\nexport function leakyReLU(args) {\n  return new LeakyReLU(args);\n}\n/**\r\n * Parameterized version of a leaky rectified linear unit.\r\n *\r\n * It follows\r\n * `f(x) = alpha * x for x < 0.`\r\n * `f(x) = x for x >= 0.`\r\n * wherein `alpha` is a trainable weight.\r\n *\r\n * Input shape:\r\n *   Arbitrary. Use the configuration `inputShape` when using this layer as the\r\n *   first layer in a model.\r\n *\r\n * Output shape:\r\n *   Same shape as the input.\r\n *\r\n * @doc {\r\n *   heading: 'Layers',\r\n *   subheading: 'Advanced Activation',\r\n *   namespace: 'layers'\r\n * }\r\n */\nexport function prelu(args) {\n  return new PReLU(args);\n}\n/**\r\n * Softmax activation layer.\r\n *\r\n * Input shape:\r\n *   Arbitrary. Use the configuration `inputShape` when using this layer as the\r\n *   first layer in a model.\r\n *\r\n * Output shape:\r\n *   Same shape as the input.\r\n *\r\n * @doc {\r\n *   heading: 'Layers',\r\n *   subheading: 'Advanced Activation',\r\n *   namespace: 'layers'\r\n * }\r\n */\nexport function softmax(args) {\n  return new Softmax(args);\n}\n/**\r\n * Thresholded Rectified Linear Unit.\r\n *\r\n * It follows:\r\n * `f(x) = x for x > theta`,\r\n * `f(x) = 0 otherwise`.\r\n *\r\n * Input shape:\r\n *   Arbitrary. Use the configuration `inputShape` when using this layer as the\r\n *   first layer in a model.\r\n *\r\n * Output shape:\r\n *   Same shape as the input.\r\n *\r\n * References:\r\n *   - [Zero-Bias Autoencoders and the Benefits of Co-Adapting\r\n * Features](http://arxiv.org/abs/1402.3337)\r\n *\r\n * @doc {\r\n *   heading: 'Layers',\r\n *   subheading: 'Advanced Activation',\r\n *   namespace: 'layers'\r\n * }\r\n */\nexport function thresholdedReLU(args) {\n  return new ThresholdedReLU(args);\n}\n// Convolutional Layers.\n/**\r\n * 1D convolution layer (e.g., temporal convolution).\r\n *\r\n * This layer creates a convolution kernel that is convolved\r\n * with the layer input over a single spatial (or temporal) dimension\r\n * to produce a tensor of outputs.\r\n *\r\n * If `use_bias` is True, a bias vector is created and added to the outputs.\r\n *\r\n * If `activation` is not `null`, it is applied to the outputs as well.\r\n *\r\n * When using this layer as the first layer in a model, provide an\r\n * `inputShape` argument `Array` or `null`.\r\n *\r\n * For example, `inputShape` would be:\r\n * - `[10, 128]` for sequences of 10 vectors of 128-dimensional vectors\r\n * - `[null, 128]` for variable-length sequences of 128-dimensional vectors.\r\n *\r\n * @doc {heading: 'Layers', subheading: 'Convolutional',  namespace: 'layers'}\r\n */\nexport function conv1d(args) {\n  return new Conv1D(args);\n}\n/**\r\n * 2D convolution layer (e.g. spatial convolution over images).\r\n *\r\n * This layer creates a convolution kernel that is convolved\r\n * with the layer input to produce a tensor of outputs.\r\n *\r\n * If `useBias` is True, a bias vector is created and added to the outputs.\r\n *\r\n * If `activation` is not `null`, it is applied to the outputs as well.\r\n *\r\n * When using this layer as the first layer in a model,\r\n * provide the keyword argument `inputShape`\r\n * (Array of integers, does not include the sample axis),\r\n * e.g. `inputShape=[128, 128, 3]` for 128x128 RGB pictures\r\n * in `dataFormat='channelsLast'`.\r\n *\r\n * @doc {heading: 'Layers', subheading: 'Convolutional', namespace: 'layers'}\r\n */\nexport function conv2d(args) {\n  return new Conv2D(args);\n}\n/**\r\n * Transposed convolutional layer (sometimes called Deconvolution).\r\n *\r\n * The need for transposed convolutions generally arises\r\n * from the desire to use a transformation going in the opposite direction of\r\n * a normal convolution, i.e., from something that has the shape of the output\r\n * of some convolution to something that has the shape of its input while\r\n * maintaining a connectivity pattern that is compatible with said\r\n * convolution.\r\n *\r\n * When using this layer as the first layer in a model, provide the\r\n * configuration `inputShape` (`Array` of integers, does not include the\r\n * sample axis), e.g., `inputShape: [128, 128, 3]` for 128x128 RGB pictures in\r\n * `dataFormat: 'channelsLast'`.\r\n *\r\n * Input shape:\r\n *   4D tensor with shape:\r\n *   `[batch, channels, rows, cols]` if `dataFormat` is `'channelsFirst'`.\r\n *   or 4D tensor with shape\r\n *   `[batch, rows, cols, channels]` if `dataFormat` is `'channelsLast`.\r\n *\r\n * Output shape:\r\n *   4D tensor with shape:\r\n *   `[batch, filters, newRows, newCols]` if `dataFormat` is\r\n * `'channelsFirst'`. or 4D tensor with shape:\r\n *   `[batch, newRows, newCols, filters]` if `dataFormat` is `'channelsLast'`.\r\n *\r\n * References:\r\n *   - [A guide to convolution arithmetic for deep\r\n * learning](https://arxiv.org/abs/1603.07285v1)\r\n *   - [Deconvolutional\r\n * Networks](http://www.matthewzeiler.com/pubs/cvpr2010/cvpr2010.pdf)\r\n *\r\n * @doc {heading: 'Layers', subheading: 'Convolutional', namespace: 'layers'}\r\n */\nexport function conv2dTranspose(args) {\n  return new Conv2DTranspose(args);\n}\n/**\r\n * 3D convolution layer (e.g. spatial convolution over volumes).\r\n *\r\n * This layer creates a convolution kernel that is convolved\r\n * with the layer input to produce a tensor of outputs.\r\n *\r\n * If `useBias` is True, a bias vector is created and added to the outputs.\r\n *\r\n * If `activation` is not `null`, it is applied to the outputs as well.\r\n *\r\n * When using this layer as the first layer in a model,\r\n * provide the keyword argument `inputShape`\r\n * (Array of integers, does not include the sample axis),\r\n * e.g. `inputShape=[128, 128, 128, 1]` for 128x128x128 grayscale volumes\r\n * in `dataFormat='channelsLast'`.\r\n *\r\n * @doc {heading: 'Layers', subheading: 'Convolutional', namespace: 'layers'}\r\n */\nexport function conv3d(args) {\n  return new Conv3D(args);\n}\n/**\r\n * Depthwise separable 2D convolution.\r\n *\r\n * Separable convolution consists of first performing\r\n * a depthwise spatial convolution\r\n * (which acts on each input channel separately)\r\n * followed by a pointwise convolution which mixes together the resulting\r\n * output channels. The `depthMultiplier` argument controls how many\r\n * output channels are generated per input channel in the depthwise step.\r\n *\r\n * Intuitively, separable convolutions can be understood as\r\n * a way to factorize a convolution kernel into two smaller kernels,\r\n * or as an extreme version of an Inception block.\r\n *\r\n * Input shape:\r\n *   4D tensor with shape:\r\n *     `[batch, channels, rows, cols]` if data_format='channelsFirst'\r\n *   or 4D tensor with shape:\r\n *     `[batch, rows, cols, channels]` if data_format='channelsLast'.\r\n *\r\n * Output shape:\r\n *   4D tensor with shape:\r\n *     `[batch, filters, newRows, newCols]` if data_format='channelsFirst'\r\n *   or 4D tensor with shape:\r\n *     `[batch, newRows, newCols, filters]` if data_format='channelsLast'.\r\n *     `rows` and `cols` values might have changed due to padding.\r\n *\r\n * @doc {heading: 'Layers', subheading: 'Convolutional', namespace: 'layers'}\r\n */\nexport function separableConv2d(args) {\n  return new SeparableConv2D(args);\n}\n/**\r\n * Cropping layer for 2D input (e.g., image).\r\n *\r\n * This layer can crop an input\r\n * at the top, bottom, left and right side of an image tensor.\r\n *\r\n * Input shape:\r\n *   4D tensor with shape:\r\n *   - If `dataFormat` is `\"channelsLast\"`:\r\n *     `[batch, rows, cols, channels]`\r\n *   - If `data_format` is `\"channels_first\"`:\r\n *     `[batch, channels, rows, cols]`.\r\n *\r\n * Output shape:\r\n *   4D with shape:\r\n *   - If `dataFormat` is `\"channelsLast\"`:\r\n *     `[batch, croppedRows, croppedCols, channels]`\r\n *    - If `dataFormat` is `\"channelsFirst\"`:\r\n *     `[batch, channels, croppedRows, croppedCols]`.\r\n *\r\n * Examples\r\n * ```js\r\n *\r\n * const model = tf.sequential();\r\n * model.add(tf.layers.cropping2D({cropping:[[2, 2], [2, 2]],\r\n *                                inputShape: [128, 128, 3]}));\r\n * //now output shape is [batch, 124, 124, 3]\r\n * ```\r\n *\r\n * @doc {heading: 'Layers', subheading: 'Convolutional', namespace: 'layers'}\r\n */\nexport function cropping2D(args) {\n  return new Cropping2D(args);\n}\n/**\r\n * Upsampling layer for 2D inputs.\r\n *\r\n * Repeats the rows and columns of the data\r\n * by size[0] and size[1] respectively.\r\n *\r\n *\r\n * Input shape:\r\n *    4D tensor with shape:\r\n *     - If `dataFormat` is `\"channelsLast\"`:\r\n *         `[batch, rows, cols, channels]`\r\n *     - If `dataFormat` is `\"channelsFirst\"`:\r\n *        `[batch, channels, rows, cols]`\r\n *\r\n * Output shape:\r\n *     4D tensor with shape:\r\n *     - If `dataFormat` is `\"channelsLast\"`:\r\n *        `[batch, upsampledRows, upsampledCols, channels]`\r\n *     - If `dataFormat` is `\"channelsFirst\"`:\r\n *         `[batch, channels, upsampledRows, upsampledCols]`\r\n *\r\n *\r\n * @doc {heading: 'Layers', subheading: 'Convolutional', namespace: 'layers'}\r\n */\nexport function upSampling2d(args) {\n  return new UpSampling2D(args);\n}\n// Convolutional(depthwise) Layers.\n/**\r\n * Depthwise separable 2D convolution.\r\n *\r\n * Depthwise Separable convolutions consists in performing just the first step\r\n * in a depthwise spatial convolution (which acts on each input channel\r\n * separately). The `depthMultplier` argument controls how many output channels\r\n * are generated per input channel in the depthwise step.\r\n *\r\n * @doc {heading: 'Layers', subheading: 'Convolutional', namespace: 'layers'}\r\n */\nexport function depthwiseConv2d(args) {\n  return new DepthwiseConv2D(args);\n}\n// Basic Layers.\n/**\r\n * Applies an activation function to an output.\r\n *\r\n * This layer applies element-wise activation function.  Other layers, notably\r\n * `dense` can also apply activation functions.  Use this isolated activation\r\n * function to extract the values before and after the\r\n * activation. For instance:\r\n *\r\n * ```js\r\n * const input = tf.input({shape: [5]});\r\n * const denseLayer = tf.layers.dense({units: 1});\r\n * const activationLayer = tf.layers.activation({activation: 'relu6'});\r\n *\r\n * // Obtain the output symbolic tensors by applying the layers in order.\r\n * const denseOutput = denseLayer.apply(input);\r\n * const activationOutput = activationLayer.apply(denseOutput);\r\n *\r\n * // Create the model based on the inputs.\r\n * const model = tf.model({\r\n *     inputs: input,\r\n *     outputs: [denseOutput, activationOutput]\r\n * });\r\n *\r\n * // Collect both outputs and print separately.\r\n * const [denseOut, activationOut] = model.predict(tf.randomNormal([6, 5]));\r\n * denseOut.print();\r\n * activationOut.print();\r\n * ```\r\n *\r\n * @doc {heading: 'Layers', subheading: 'Basic', namespace: 'layers'}\r\n */\nexport function activation(args) {\n  return new Activation(args);\n}\n/**\r\n * Creates a dense (fully connected) layer.\r\n *\r\n * This layer implements the operation:\r\n *   `output = activation(dot(input, kernel) + bias)`\r\n *\r\n * `activation` is the element-wise activation function\r\n *   passed as the `activation` argument.\r\n *\r\n * `kernel` is a weights matrix created by the layer.\r\n *\r\n * `bias` is a bias vector created by the layer (only applicable if `useBias`\r\n * is `true`).\r\n *\r\n * **Input shape:**\r\n *\r\n *   nD `tf.Tensor` with shape: `(batchSize, ..., inputDim)`.\r\n *\r\n *   The most common situation would be\r\n *   a 2D input with shape `(batchSize, inputDim)`.\r\n *\r\n * **Output shape:**\r\n *\r\n *   nD tensor with shape: `(batchSize, ..., units)`.\r\n *\r\n *   For instance, for a 2D input with shape `(batchSize, inputDim)`,\r\n *   the output would have shape `(batchSize, units)`.\r\n *\r\n * Note: if the input to the layer has a rank greater than 2, then it is\r\n * flattened prior to the initial dot product with the kernel.\r\n *\r\n * @doc {heading: 'Layers', subheading: 'Basic', namespace: 'layers'}\r\n */\nexport function dense(args) {\n  return new Dense(args);\n}\n/**\r\n * Applies\r\n * [dropout](http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf) to\r\n * the input.\r\n *\r\n * Dropout consists in randomly setting a fraction `rate` of input units to 0 at\r\n * each update during training time, which helps prevent overfitting.\r\n *\r\n * @doc {heading: 'Layers', subheading: 'Basic', namespace: 'layers'}\r\n */\nexport function dropout(args) {\n  return new Dropout(args);\n}\n/**\r\n * Spatial 1D version of Dropout.\r\n *\r\n * This Layer type performs the same function as the Dropout layer, but it drops\r\n * entire 1D feature maps instead of individual elements. For example, if an\r\n * input example consists of 3 timesteps and the feature map for each timestep\r\n * has a size of 4, a `spatialDropout1d` layer may zero out the feature maps\r\n * of the 1st timesteps and 2nd timesteps completely while sparing all feature\r\n * elements of the 3rd timestep.\r\n *\r\n * If adjacent frames (timesteps) are strongly correlated (as is normally the\r\n * case in early convolution layers), regular dropout will not regularize the\r\n * activation and will otherwise just result in merely an effective learning\r\n * rate decrease. In this case, `spatialDropout1d` will help promote\r\n * independence among feature maps and should be used instead.\r\n *\r\n * **Arguments:**\r\n *   rate: A floating-point number >=0 and <=1. Fraction of the input elements\r\n *     to drop.\r\n *\r\n * **Input shape:**\r\n *   3D tensor with shape `(samples, timesteps, channels)`.\r\n *\r\n * **Output shape:**\r\n *   Same as the input shape.\r\n *\r\n * References:\r\n *   - [Efficient Object Localization Using Convolutional\r\n *      Networks](https://arxiv.org/abs/1411.4280)\r\n *\r\n * @doc {heading: 'Layers', subheading: 'Basic', namespace: 'layers'}\r\n */\nexport function spatialDropout1d(args) {\n  return new SpatialDropout1D(args);\n}\n/**\r\n * Flattens the input. Does not affect the batch size.\r\n *\r\n * A `Flatten` layer flattens each batch in its inputs to 1D (making the output\r\n * 2D).\r\n *\r\n * For example:\r\n *\r\n * ```js\r\n * const input = tf.input({shape: [4, 3]});\r\n * const flattenLayer = tf.layers.flatten();\r\n * // Inspect the inferred output shape of the flatten layer, which\r\n * // equals `[null, 12]`. The 2nd dimension is 4 * 3, i.e., the result of the\r\n * // flattening. (The 1st dimension is the undermined batch size.)\r\n * console.log(JSON.stringify(flattenLayer.apply(input).shape));\r\n * ```\r\n *\r\n * @doc {heading: 'Layers', subheading: 'Basic', namespace: 'layers'}\r\n */\nexport function flatten(args) {\n  return new Flatten(args);\n}\n/**\r\n * Repeats the input n times in a new dimension.\r\n *\r\n * ```js\r\n *  const model = tf.sequential();\r\n *  model.add(tf.layers.repeatVector({n: 4, inputShape: [2]}));\r\n *  const x = tf.tensor2d([[10, 20]]);\r\n *  // Use the model to do inference on a data point the model hasn't see\r\n *  model.predict(x).print();\r\n *  // output shape is now [batch, 2, 4]\r\n * ```\r\n *\r\n * @doc {heading: 'Layers', subheading: 'Basic', namespace: 'layers'}\r\n */\nexport function repeatVector(args) {\n  return new RepeatVector(args);\n}\n/**\r\n * Reshapes an input to a certain shape.\r\n *\r\n * ```js\r\n * const input = tf.input({shape: [4, 3]});\r\n * const reshapeLayer = tf.layers.reshape({targetShape: [2, 6]});\r\n * // Inspect the inferred output shape of the Reshape layer, which\r\n * // equals `[null, 2, 6]`. (The 1st dimension is the undermined batch size.)\r\n * console.log(JSON.stringify(reshapeLayer.apply(input).shape));\r\n * ```\r\n *\r\n * Input shape:\r\n *   Arbitrary, although all dimensions in the input shape must be fixed.\r\n *   Use the configuration `inputShape` when using this layer as the\r\n *   first layer in a model.\r\n *\r\n *\r\n * Output shape:\r\n *   [batchSize, targetShape[0], targetShape[1], ...,\r\n *    targetShape[targetShape.length - 1]].\r\n *\r\n * @doc {heading: 'Layers', subheading: 'Basic', namespace: 'layers'}\r\n */\nexport function reshape(args) {\n  return new Reshape(args);\n}\n/**\r\n * Permutes the dimensions of the input according to a given pattern.\r\n *\r\n * Useful for, e.g., connecting RNNs and convnets together.\r\n *\r\n * Example:\r\n *\r\n * ```js\r\n * const model = tf.sequential();\r\n * model.add(tf.layers.permute({\r\n *   dims: [2, 1],\r\n *   inputShape: [10, 64]\r\n * }));\r\n * console.log(model.outputShape);\r\n * // Now model's output shape is [null, 64, 10], where null is the\r\n * // unpermuted sample (batch) dimension.\r\n * ```\r\n *\r\n * Input shape:\r\n *   Arbitrary. Use the configuration field `inputShape` when using this\r\n *   layer as the first layer in a model.\r\n *\r\n * Output shape:\r\n *   Same rank as the input shape, but with the dimensions re-ordered (i.e.,\r\n *   permuted) according to the `dims` configuration of this layer.\r\n *\r\n * @doc {heading: 'Layers', subheading: 'Basic', namespace: 'layers'}\r\n */\nexport function permute(args) {\n  return new Permute(args);\n}\n/**\r\n * Maps positive integers (indices) into dense vectors of fixed size.\r\n * eg. [[4], [20]] -> [[0.25, 0.1], [0.6, -0.2]]\r\n *\r\n * **Input shape:** 2D tensor with shape: `[batchSize, sequenceLength]`.\r\n *\r\n * **Output shape:** 3D tensor with shape: `[batchSize, sequenceLength,\r\n * outputDim]`.\r\n *\r\n * @doc {heading: 'Layers', subheading: 'Basic', namespace: 'layers'}\r\n */\nexport function embedding(args) {\n  return new Embedding(args);\n}\n// Merge Layers.\n/**\r\n * Layer that performs element-wise addition on an `Array` of inputs.\r\n *\r\n * It takes as input a list of tensors, all of the same shape, and returns a\r\n * single tensor (also of the same shape). The inputs are specified as an\r\n * `Array` when the `apply` method of the `Add` layer instance is called. For\r\n * example:\r\n *\r\n * ```js\r\n * const input1 = tf.input({shape: [2, 2]});\r\n * const input2 = tf.input({shape: [2, 2]});\r\n * const addLayer = tf.layers.add();\r\n * const sum = addLayer.apply([input1, input2]);\r\n * console.log(JSON.stringify(sum.shape));\r\n * // You get [null, 2, 2], with the first dimension as the undetermined batch\r\n * // dimension.\r\n * ```\r\n *\r\n * @doc {heading: 'Layers', subheading: 'Merge', namespace: 'layers'}\r\n */\nexport function add(args) {\n  return new Add(args);\n}\n/**\r\n * Layer that performs element-wise averaging on an `Array` of inputs.\r\n *\r\n * It takes as input a list of tensors, all of the same shape, and returns a\r\n * single tensor (also of the same shape). For example:\r\n *\r\n * ```js\r\n * const input1 = tf.input({shape: [2, 2]});\r\n * const input2 = tf.input({shape: [2, 2]});\r\n * const averageLayer = tf.layers.average();\r\n * const average = averageLayer.apply([input1, input2]);\r\n * console.log(JSON.stringify(average.shape));\r\n * // You get [null, 2, 2], with the first dimension as the undetermined batch\r\n * // dimension.\r\n * ```\r\n *\r\n * @doc {heading: 'Layers', subheading: 'Merge', namespace: 'layers'}\r\n */\nexport function average(args) {\n  return new Average(args);\n}\n/**\r\n * Layer that concatenates an `Array` of inputs.\r\n *\r\n * It takes a list of tensors, all of the same shape except for the\r\n * concatenation axis, and returns a single tensor, the concatenation\r\n * of all inputs. For example:\r\n *\r\n * ```js\r\n * const input1 = tf.input({shape: [2, 2]});\r\n * const input2 = tf.input({shape: [2, 3]});\r\n * const concatLayer = tf.layers.concatenate();\r\n * const output = concatLayer.apply([input1, input2]);\r\n * console.log(JSON.stringify(output.shape));\r\n * // You get [null, 2, 5], with the first dimension as the undetermined batch\r\n * // dimension. The last dimension (5) is the result of concatenating the\r\n * // last dimensions of the inputs (2 and 3).\r\n * ```\r\n *\r\n * @doc {heading: 'Layers', subheading: 'Merge', namespace: 'layers'}\r\n */\nexport function concatenate(args) {\n  return new Concatenate(args);\n}\n/**\r\n * Layer that computes the element-wise maximum an `Array` of inputs.\r\n *\r\n * It takes as input a list of tensors, all of the same shape and returns a\r\n * single tensor (also of the same shape). For example:\r\n *\r\n * ```js\r\n * const input1 = tf.input({shape: [2, 2]});\r\n * const input2 = tf.input({shape: [2, 2]});\r\n * const maxLayer = tf.layers.maximum();\r\n * const max = maxLayer.apply([input1, input2]);\r\n * console.log(JSON.stringify(max.shape));\r\n * // You get [null, 2, 2], with the first dimension as the undetermined batch\r\n * // dimension.\r\n * ```\r\n *\r\n * @doc {heading: 'Layers', subheading: 'Merge', namespace: 'layers'}\r\n */\nexport function maximum(args) {\n  return new Maximum(args);\n}\n/**\r\n * Layer that computes the element-wise minimum of an `Array` of inputs.\r\n *\r\n * It takes as input a list of tensors, all of the same shape and returns a\r\n * single tensor (also of the same shape). For example:\r\n *\r\n * ```js\r\n * const input1 = tf.input({shape: [2, 2]});\r\n * const input2 = tf.input({shape: [2, 2]});\r\n * const minLayer = tf.layers.minimum();\r\n * const min = minLayer.apply([input1, input2]);\r\n * console.log(JSON.stringify(min.shape));\r\n * // You get [null, 2, 2], with the first dimension as the undetermined batch\r\n * // dimension.\r\n * ```\r\n *\r\n * @doc {heading: 'Layers', subheading: 'Merge', namespace: 'layers'}\r\n */\nexport function minimum(args) {\n  return new Minimum(args);\n}\n/**\r\n * Layer that multiplies (element-wise) an `Array` of inputs.\r\n *\r\n * It takes as input an Array of tensors, all of the same\r\n * shape, and returns a single tensor (also of the same shape).\r\n * For example:\r\n *\r\n * ```js\r\n * const input1 = tf.input({shape: [2, 2]});\r\n * const input2 = tf.input({shape: [2, 2]});\r\n * const input3 = tf.input({shape: [2, 2]});\r\n * const multiplyLayer = tf.layers.multiply();\r\n * const product = multiplyLayer.apply([input1, input2, input3]);\r\n * console.log(product.shape);\r\n * // You get [null, 2, 2], with the first dimension as the undetermined batch\r\n * // dimension.\r\n *\r\n * @doc {heading: 'Layers', subheading: 'Merge', namespace: 'layers'}\r\n */\nexport function multiply(args) {\n  return new Multiply(args);\n}\n/**\r\n * Layer that computes a dot product between samples in two tensors.\r\n *\r\n * E.g., if applied to a list of two tensors `a` and `b` both of shape\r\n * `[batchSize, n]`, the output will be a tensor of shape `[batchSize, 1]`,\r\n * where each entry at index `[i, 0]` will be the dot product between\r\n * `a[i, :]` and `b[i, :]`.\r\n *\r\n * Example:\r\n *\r\n * ```js\r\n * const dotLayer = tf.layers.dot({axes: -1});\r\n * const x1 = tf.tensor2d([[10, 20], [30, 40]]);\r\n * const x2 = tf.tensor2d([[-1, -2], [-3, -4]]);\r\n *\r\n * // Invoke the layer's apply() method in eager (imperative) mode.\r\n * const y = dotLayer.apply([x1, x2]);\r\n * y.print();\r\n * ```\r\n *\r\n * @doc {heading: 'Layers', subheading: 'Merge', namespace: 'layers'}\r\n */\nexport function dot(args) {\n  return new Dot(args);\n}\n// Normalization Layers.\n/**\r\n * Batch normalization layer (Ioffe and Szegedy, 2014).\r\n *\r\n * Normalize the activations of the previous layer at each batch,\r\n * i.e. applies a transformation that maintains the mean activation\r\n * close to 0 and the activation standard deviation close to 1.\r\n *\r\n * Input shape:\r\n *   Arbitrary. Use the keyword argument `inputShape` (Array of integers, does\r\n *   not include the sample axis) when calling the constructor of this class,\r\n *   if this layer is used as a first layer in a model.\r\n *\r\n * Output shape:\r\n *   Same shape as input.\r\n *\r\n * References:\r\n *   - [Batch Normalization: Accelerating Deep Network Training by Reducing\r\n * Internal Covariate Shift](https://arxiv.org/abs/1502.03167)\r\n *\r\n * @doc {heading: 'Layers', subheading: 'Normalization', namespace: 'layers'}\r\n */\nexport function batchNormalization(args) {\n  return new BatchNormalization(args);\n}\n/**\r\n * Layer-normalization layer (Ba et al., 2016).\r\n *\r\n * Normalizes the activations of the previous layer for each given example in a\r\n * batch independently, instead of across a batch like in `batchNormalization`.\r\n * In other words, this layer applies a transformation that maintanis the mean\r\n * activation within each example close to0 and activation variance close to 1.\r\n *\r\n * Input shape:\r\n *   Arbitrary. Use the argument `inputShape` when using this layer as the first\r\n *   layer in a model.\r\n *\r\n * Output shape:\r\n *   Same as input.\r\n *\r\n * References:\r\n *   - [Layer Normalization](https://arxiv.org/abs/1607.06450)\r\n *\r\n * @doc {heading: 'Layers', subheading: 'Normalization', namespace: 'layers'}\r\n */\nexport function layerNormalization(args) {\n  return new LayerNormalization(args);\n}\n// Padding Layers.\n/**\r\n * Zero-padding layer for 2D input (e.g., image).\r\n *\r\n * This layer can add rows and columns of zeros\r\n * at the top, bottom, left and right side of an image tensor.\r\n *\r\n * Input shape:\r\n *   4D tensor with shape:\r\n *   - If `dataFormat` is `\"channelsLast\"`:\r\n *     `[batch, rows, cols, channels]`\r\n *   - If `data_format` is `\"channels_first\"`:\r\n *     `[batch, channels, rows, cols]`.\r\n *\r\n * Output shape:\r\n *   4D with shape:\r\n *   - If `dataFormat` is `\"channelsLast\"`:\r\n *     `[batch, paddedRows, paddedCols, channels]`\r\n *    - If `dataFormat` is `\"channelsFirst\"`:\r\n *     `[batch, channels, paddedRows, paddedCols]`.\r\n *\r\n * @doc {heading: 'Layers', subheading: 'Padding', namespace: 'layers'}\r\n */\nexport function zeroPadding2d(args) {\n  return new ZeroPadding2D(args);\n}\n// Pooling Layers.\n/**\r\n * Average pooling operation for spatial data.\r\n *\r\n * Input shape: `[batchSize, inLength, channels]`\r\n *\r\n * Output shape: `[batchSize, pooledLength, channels]`\r\n *\r\n * `tf.avgPool1d` is an alias.\r\n *\r\n * @doc {heading: 'Layers', subheading: 'Pooling', namespace: 'layers'}\r\n */\nexport function averagePooling1d(args) {\n  return new AveragePooling1D(args);\n}\nexport function avgPool1d(args) {\n  return averagePooling1d(args);\n}\n// For backwards compatibility.\n// See https://github.com/tensorflow/tfjs/issues/152\nexport function avgPooling1d(args) {\n  return averagePooling1d(args);\n}\n/**\r\n * Average pooling operation for spatial data.\r\n *\r\n * Input shape:\r\n *  - If `dataFormat === CHANNEL_LAST`:\r\n *      4D tensor with shape:\r\n *      `[batchSize, rows, cols, channels]`\r\n *  - If `dataFormat === CHANNEL_FIRST`:\r\n *      4D tensor with shape:\r\n *      `[batchSize, channels, rows, cols]`\r\n *\r\n * Output shape\r\n *  - If `dataFormat === CHANNEL_LAST`:\r\n *      4D tensor with shape:\r\n *      `[batchSize, pooleRows, pooledCols, channels]`\r\n *  - If `dataFormat === CHANNEL_FIRST`:\r\n *      4D tensor with shape:\r\n *      `[batchSize, channels, pooleRows, pooledCols]`\r\n *\r\n * `tf.avgPool2d` is an alias.\r\n *\r\n * @doc {heading: 'Layers', subheading: 'Pooling', namespace: 'layers'}\r\n */\nexport function averagePooling2d(args) {\n  return new AveragePooling2D(args);\n}\nexport function avgPool2d(args) {\n  return averagePooling2d(args);\n}\n// For backwards compatibility.\n// See https://github.com/tensorflow/tfjs/issues/152\nexport function avgPooling2d(args) {\n  return averagePooling2d(args);\n}\n/**\r\n * Average pooling operation for 3D data.\r\n *\r\n * Input shape\r\n *   - If `dataFormat === channelsLast`:\r\n *       5D tensor with shape:\r\n *       `[batchSize, depths, rows, cols, channels]`\r\n *   - If `dataFormat === channelsFirst`:\r\n *      4D tensor with shape:\r\n *       `[batchSize, channels, depths, rows, cols]`\r\n *\r\n * Output shape\r\n *   - If `dataFormat=channelsLast`:\r\n *       5D tensor with shape:\r\n *       `[batchSize, pooledDepths, pooledRows, pooledCols, channels]`\r\n *   - If `dataFormat=channelsFirst`:\r\n *       5D tensor with shape:\r\n *       `[batchSize, channels, pooledDepths, pooledRows, pooledCols]`\r\n *\r\n * @doc {heading: 'Layers', subheading: 'Pooling', namespace: 'layers'}\r\n */\nexport function averagePooling3d(args) {\n  return new AveragePooling3D(args);\n}\nexport function avgPool3d(args) {\n  return averagePooling3d(args);\n}\n// For backwards compatibility.\n// See https://github.com/tensorflow/tfjs/issues/152\nexport function avgPooling3d(args) {\n  return averagePooling3d(args);\n}\n/**\r\n * Global average pooling operation for temporal data.\r\n *\r\n * Input Shape: 3D tensor with shape: `[batchSize, steps, features]`.\r\n *\r\n * Output Shape:2D tensor with shape: `[batchSize, features]`.\r\n *\r\n * @doc {heading: 'Layers', subheading: 'Pooling', namespace: 'layers'}\r\n */\nexport function globalAveragePooling1d(args) {\n  return new GlobalAveragePooling1D(args);\n}\n/**\r\n * Global average pooling operation for spatial data.\r\n *\r\n * Input shape:\r\n *   - If `dataFormat` is `CHANNEL_LAST`:\r\n *       4D tensor with shape: `[batchSize, rows, cols, channels]`.\r\n *   - If `dataFormat` is `CHANNEL_FIRST`:\r\n *       4D tensor with shape: `[batchSize, channels, rows, cols]`.\r\n *\r\n * Output shape:\r\n *   2D tensor with shape: `[batchSize, channels]`.\r\n *\r\n * @doc {heading: 'Layers', subheading: 'Pooling', namespace: 'layers'}\r\n */\nexport function globalAveragePooling2d(args) {\n  return new GlobalAveragePooling2D(args);\n}\n/**\r\n * Global max pooling operation for temporal data.\r\n *\r\n * Input Shape: 3D tensor with shape: `[batchSize, steps, features]`.\r\n *\r\n * Output Shape:2D tensor with shape: `[batchSize, features]`.\r\n *\r\n * @doc {heading: 'Layers', subheading: 'Pooling', namespace: 'layers'}\r\n */\nexport function globalMaxPooling1d(args) {\n  return new GlobalMaxPooling1D(args);\n}\n/**\r\n * Global max pooling operation for spatial data.\r\n *\r\n * Input shape:\r\n *   - If `dataFormat` is `CHANNEL_LAST`:\r\n *       4D tensor with shape: `[batchSize, rows, cols, channels]`.\r\n *   - If `dataFormat` is `CHANNEL_FIRST`:\r\n *       4D tensor with shape: `[batchSize, channels, rows, cols]`.\r\n *\r\n * Output shape:\r\n *   2D tensor with shape: `[batchSize, channels]`.\r\n *\r\n * @doc {heading: 'Layers', subheading: 'Pooling', namespace: 'layers'}\r\n */\nexport function globalMaxPooling2d(args) {\n  return new GlobalMaxPooling2D(args);\n}\n/**\r\n * Max pooling operation for temporal data.\r\n *\r\n * Input shape:  `[batchSize, inLength, channels]`\r\n *\r\n * Output shape: `[batchSize, pooledLength, channels]`\r\n *\r\n * @doc {heading: 'Layers', subheading: 'Pooling', namespace: 'layers'}\r\n */\nexport function maxPooling1d(args) {\n  return new MaxPooling1D(args);\n}\n/**\r\n * Max pooling operation for spatial data.\r\n *\r\n * Input shape\r\n *   - If `dataFormat === CHANNEL_LAST`:\r\n *       4D tensor with shape:\r\n *       `[batchSize, rows, cols, channels]`\r\n *   - If `dataFormat === CHANNEL_FIRST`:\r\n *      4D tensor with shape:\r\n *       `[batchSize, channels, rows, cols]`\r\n *\r\n * Output shape\r\n *   - If `dataFormat=CHANNEL_LAST`:\r\n *       4D tensor with shape:\r\n *       `[batchSize, pooleRows, pooledCols, channels]`\r\n *   - If `dataFormat=CHANNEL_FIRST`:\r\n *       4D tensor with shape:\r\n *       `[batchSize, channels, pooleRows, pooledCols]`\r\n *\r\n * @doc {heading: 'Layers', subheading: 'Pooling', namespace: 'layers'}\r\n */\nexport function maxPooling2d(args) {\n  return new MaxPooling2D(args);\n}\n/**\r\n * Max pooling operation for 3D data.\r\n *\r\n * Input shape\r\n *   - If `dataFormat === channelsLast`:\r\n *       5D tensor with shape:\r\n *       `[batchSize, depths, rows, cols, channels]`\r\n *   - If `dataFormat === channelsFirst`:\r\n *      5D tensor with shape:\r\n *       `[batchSize, channels, depths, rows, cols]`\r\n *\r\n * Output shape\r\n *   - If `dataFormat=channelsLast`:\r\n *       5D tensor with shape:\r\n *       `[batchSize, pooledDepths, pooledRows, pooledCols, channels]`\r\n *   - If `dataFormat=channelsFirst`:\r\n *       5D tensor with shape:\r\n *       `[batchSize, channels, pooledDepths, pooledRows, pooledCols]`\r\n *\r\n * @doc {heading: 'Layers', subheading: 'Pooling', namespace: 'layers'}\r\n */\nexport function maxPooling3d(args) {\n  return new MaxPooling3D(args);\n}\n// Recurrent Layers.\n/**\r\n * Gated Recurrent Unit - Cho et al. 2014.\r\n *\r\n * This is an `RNN` layer consisting of one `GRUCell`. However, unlike\r\n * the underlying `GRUCell`, the `apply` method of `SimpleRNN` operates\r\n * on a sequence of inputs. The shape of the input (not including the first,\r\n * batch dimension) needs to be at least 2-D, with the first dimension being\r\n * time steps. For example:\r\n *\r\n * ```js\r\n * const rnn = tf.layers.gru({units: 8, returnSequences: true});\r\n *\r\n * // Create an input with 10 time steps.\r\n * const input = tf.input({shape: [10, 20]});\r\n * const output = rnn.apply(input);\r\n *\r\n * console.log(JSON.stringify(output.shape));\r\n * // [null, 10, 8]: 1st dimension is unknown batch size; 2nd dimension is the\r\n * // same as the sequence length of `input`, due to `returnSequences`: `true`;\r\n * // 3rd dimension is the `GRUCell`'s number of units.\r\n *\r\n * @doc {heading: 'Layers', subheading: 'Recurrent', namespace: 'layers'}\r\n */\nexport function gru(args) {\n  return new GRU(args);\n}\n/**\r\n * Cell class for `GRU`.\r\n *\r\n * `GRUCell` is distinct from the `RNN` subclass `GRU` in that its\r\n * `apply` method takes the input data of only a single time step and returns\r\n * the cell's output at the time step, while `GRU` takes the input data\r\n * over a number of time steps. For example:\r\n *\r\n * ```js\r\n * const cell = tf.layers.gruCell({units: 2});\r\n * const input = tf.input({shape: [10]});\r\n * const output = cell.apply(input);\r\n *\r\n * console.log(JSON.stringify(output.shape));\r\n * // [null, 10]: This is the cell's output at a single time step. The 1st\r\n * // dimension is the unknown batch size.\r\n * ```\r\n *\r\n * Instance(s) of `GRUCell` can be used to construct `RNN` layers. The\r\n * most typical use of this workflow is to combine a number of cells into a\r\n * stacked RNN cell (i.e., `StackedRNNCell` internally) and use it to create an\r\n * RNN. For example:\r\n *\r\n * ```js\r\n * const cells = [\r\n *   tf.layers.gruCell({units: 4}),\r\n *   tf.layers.gruCell({units: 8}),\r\n * ];\r\n * const rnn = tf.layers.rnn({cell: cells, returnSequences: true});\r\n *\r\n * // Create an input with 10 time steps and a length-20 vector at each step.\r\n * const input = tf.input({shape: [10, 20]});\r\n * const output = rnn.apply(input);\r\n *\r\n * console.log(JSON.stringify(output.shape));\r\n * // [null, 10, 8]: 1st dimension is unknown batch size; 2nd dimension is the\r\n * // same as the sequence length of `input`, due to `returnSequences`: `true`;\r\n * // 3rd dimension is the last `gruCell`'s number of units.\r\n * ```\r\n *\r\n * To create an `RNN` consisting of only *one* `GRUCell`, use the\r\n * `tf.layers.gru`.\r\n *\r\n * @doc {heading: 'Layers', subheading: 'Recurrent', namespace: 'layers'}\r\n */\nexport function gruCell(args) {\n  return new GRUCell(args);\n}\n/**\r\n * Long-Short Term Memory layer - Hochreiter 1997.\r\n *\r\n * This is an `RNN` layer consisting of one `LSTMCell`. However, unlike\r\n * the underlying `LSTMCell`, the `apply` method of `LSTM` operates\r\n * on a sequence of inputs. The shape of the input (not including the first,\r\n * batch dimension) needs to be at least 2-D, with the first dimension being\r\n * time steps. For example:\r\n *\r\n * ```js\r\n * const lstm = tf.layers.lstm({units: 8, returnSequences: true});\r\n *\r\n * // Create an input with 10 time steps.\r\n * const input = tf.input({shape: [10, 20]});\r\n * const output = lstm.apply(input);\r\n *\r\n * console.log(JSON.stringify(output.shape));\r\n * // [null, 10, 8]: 1st dimension is unknown batch size; 2nd dimension is the\r\n * // same as the sequence length of `input`, due to `returnSequences`: `true`;\r\n * // 3rd dimension is the `LSTMCell`'s number of units.\r\n *\r\n * @doc {heading: 'Layers', subheading: 'Recurrent', namespace: 'layers'}\r\n */\nexport function lstm(args) {\n  return new LSTM(args);\n}\n/**\r\n * Cell class for `LSTM`.\r\n *\r\n * `LSTMCell` is distinct from the `RNN` subclass `LSTM` in that its\r\n * `apply` method takes the input data of only a single time step and returns\r\n * the cell's output at the time step, while `LSTM` takes the input data\r\n * over a number of time steps. For example:\r\n *\r\n * ```js\r\n * const cell = tf.layers.lstmCell({units: 2});\r\n * const input = tf.input({shape: [10]});\r\n * const output = cell.apply(input);\r\n *\r\n * console.log(JSON.stringify(output.shape));\r\n * // [null, 10]: This is the cell's output at a single time step. The 1st\r\n * // dimension is the unknown batch size.\r\n * ```\r\n *\r\n * Instance(s) of `LSTMCell` can be used to construct `RNN` layers. The\r\n * most typical use of this workflow is to combine a number of cells into a\r\n * stacked RNN cell (i.e., `StackedRNNCell` internally) and use it to create an\r\n * RNN. For example:\r\n *\r\n * ```js\r\n * const cells = [\r\n *   tf.layers.lstmCell({units: 4}),\r\n *   tf.layers.lstmCell({units: 8}),\r\n * ];\r\n * const rnn = tf.layers.rnn({cell: cells, returnSequences: true});\r\n *\r\n * // Create an input with 10 time steps and a length-20 vector at each step.\r\n * const input = tf.input({shape: [10, 20]});\r\n * const output = rnn.apply(input);\r\n *\r\n * console.log(JSON.stringify(output.shape));\r\n * // [null, 10, 8]: 1st dimension is unknown batch size; 2nd dimension is the\r\n * // same as the sequence length of `input`, due to `returnSequences`: `true`;\r\n * // 3rd dimension is the last `lstmCell`'s number of units.\r\n * ```\r\n *\r\n * To create an `RNN` consisting of only *one* `LSTMCell`, use the\r\n * `tf.layers.lstm`.\r\n *\r\n * @doc {heading: 'Layers', subheading: 'Recurrent', namespace: 'layers'}\r\n */\nexport function lstmCell(args) {\n  return new LSTMCell(args);\n}\n/**\r\n * Fully-connected RNN where the output is to be fed back to input.\r\n *\r\n * This is an `RNN` layer consisting of one `SimpleRNNCell`. However, unlike\r\n * the underlying `SimpleRNNCell`, the `apply` method of `SimpleRNN` operates\r\n * on a sequence of inputs. The shape of the input (not including the first,\r\n * batch dimension) needs to be at least 2-D, with the first dimension being\r\n * time steps. For example:\r\n *\r\n * ```js\r\n * const rnn = tf.layers.simpleRNN({units: 8, returnSequences: true});\r\n *\r\n * // Create an input with 10 time steps.\r\n * const input = tf.input({shape: [10, 20]});\r\n * const output = rnn.apply(input);\r\n *\r\n * console.log(JSON.stringify(output.shape));\r\n * // [null, 10, 8]: 1st dimension is unknown batch size; 2nd dimension is the\r\n * // same as the sequence length of `input`, due to `returnSequences`: `true`;\r\n * // 3rd dimension is the `SimpleRNNCell`'s number of units.\r\n * ```\r\n *\r\n * @doc {heading: 'Layers', subheading: 'Recurrent', namespace: 'layers'}\r\n */\nexport function simpleRNN(args) {\n  return new SimpleRNN(args);\n}\n/**\r\n * Cell class for `SimpleRNN`.\r\n *\r\n * `SimpleRNNCell` is distinct from the `RNN` subclass `SimpleRNN` in that its\r\n * `apply` method takes the input data of only a single time step and returns\r\n * the cell's output at the time step, while `SimpleRNN` takes the input data\r\n * over a number of time steps. For example:\r\n *\r\n * ```js\r\n * const cell = tf.layers.simpleRNNCell({units: 2});\r\n * const input = tf.input({shape: [10]});\r\n * const output = cell.apply(input);\r\n *\r\n * console.log(JSON.stringify(output.shape));\r\n * // [null, 10]: This is the cell's output at a single time step. The 1st\r\n * // dimension is the unknown batch size.\r\n * ```\r\n *\r\n * Instance(s) of `SimpleRNNCell` can be used to construct `RNN` layers. The\r\n * most typical use of this workflow is to combine a number of cells into a\r\n * stacked RNN cell (i.e., `StackedRNNCell` internally) and use it to create an\r\n * RNN. For example:\r\n *\r\n * ```js\r\n * const cells = [\r\n *   tf.layers.simpleRNNCell({units: 4}),\r\n *   tf.layers.simpleRNNCell({units: 8}),\r\n * ];\r\n * const rnn = tf.layers.rnn({cell: cells, returnSequences: true});\r\n *\r\n * // Create an input with 10 time steps and a length-20 vector at each step.\r\n * const input = tf.input({shape: [10, 20]});\r\n * const output = rnn.apply(input);\r\n *\r\n * console.log(JSON.stringify(output.shape));\r\n * // [null, 10, 8]: 1st dimension is unknown batch size; 2nd dimension is the\r\n * // same as the sequence length of `input`, due to `returnSequences`: `true`;\r\n * // 3rd dimension is the last `SimpleRNNCell`'s number of units.\r\n * ```\r\n *\r\n * To create an `RNN` consisting of only *one* `SimpleRNNCell`, use the\r\n * `tf.layers.simpleRNN`.\r\n *\r\n * @doc {heading: 'Layers', subheading: 'Recurrent', namespace: 'layers'}\r\n */\nexport function simpleRNNCell(args) {\n  return new SimpleRNNCell(args);\n}\n/**\r\n * Convolutional LSTM layer - Xingjian Shi 2015.\r\n *\r\n * This is an `ConvRNN2D` layer consisting of one `ConvLSTM2DCell`. However,\r\n * unlike the underlying `ConvLSTM2DCell`, the `apply` method of `ConvLSTM2D`\r\n * operates on a sequence of inputs. The shape of the input (not including the\r\n * first, batch dimension) needs to be 4-D, with the first dimension being time\r\n * steps. For example:\r\n *\r\n * ```js\r\n * const filters = 3;\r\n * const kernelSize = 3;\r\n *\r\n * const batchSize = 4;\r\n * const sequenceLength = 2;\r\n * const size = 5;\r\n * const channels = 3;\r\n *\r\n * const inputShape = [batchSize, sequenceLength, size, size, channels];\r\n * const input = tf.ones(inputShape);\r\n *\r\n * const layer = tf.layers.convLstm2d({filters, kernelSize});\r\n *\r\n * const output = layer.apply(input);\r\n * ```\r\n */\n/** @doc {heading: 'Layers', subheading: 'Recurrent', namespace: 'layers'} */\nexport function convLstm2d(args) {\n  return new ConvLSTM2D(args);\n}\n/**\r\n * Cell class for `ConvLSTM2D`.\r\n *\r\n * `ConvLSTM2DCell` is distinct from the `ConvRNN2D` subclass `ConvLSTM2D` in\r\n * that its `call` method takes the input data of only a single time step and\r\n * returns the cell's output at the time step, while `ConvLSTM2D` takes the\r\n * input data over a number of time steps. For example:\r\n *\r\n * ```js\r\n * const filters = 3;\r\n * const kernelSize = 3;\r\n *\r\n * const sequenceLength = 1;\r\n * const size = 5;\r\n * const channels = 3;\r\n *\r\n * const inputShape = [sequenceLength, size, size, channels];\r\n * const input = tf.ones(inputShape);\r\n *\r\n * const cell = tf.layers.convLstm2dCell({filters, kernelSize});\r\n *\r\n * cell.build(input.shape);\r\n *\r\n * const outputSize = size - kernelSize + 1;\r\n * const outShape = [sequenceLength, outputSize, outputSize, filters];\r\n *\r\n * const initialH = tf.zeros(outShape);\r\n * const initialC = tf.zeros(outShape);\r\n *\r\n * const [o, h, c] = cell.call([input, initialH, initialC], {});\r\n * ```\r\n */\n/** @doc {heading: 'Layers', subheading: 'Recurrent', namespace: 'layers'} */\nexport function convLstm2dCell(args) {\n  return new ConvLSTM2DCell(args);\n}\n/**\r\n * Base class for recurrent layers.\r\n *\r\n * Input shape:\r\n *   3D tensor with shape `[batchSize, timeSteps, inputDim]`.\r\n *\r\n * Output shape:\r\n *   - if `returnState`, an Array of tensors (i.e., `tf.Tensor`s). The first\r\n *     tensor is the output. The remaining tensors are the states at the\r\n *     last time step, each with shape `[batchSize, units]`.\r\n *   - if `returnSequences`, the output will have shape\r\n *     `[batchSize, timeSteps, units]`.\r\n *   - else, the output will have shape `[batchSize, units]`.\r\n *\r\n * Masking:\r\n *   This layer supports masking for input data with a variable number\r\n *   of timesteps. To introduce masks to your data,\r\n *   use an embedding layer with the `mask_zero` parameter\r\n *   set to `True`.\r\n *\r\n * Notes on using statefulness in RNNs:\r\n *   You can set RNN layers to be 'stateful', which means that the states\r\n *   computed for the samples in one batch will be reused as initial states\r\n *   for the samples in the next batch. This assumes a one-to-one mapping\r\n *   between samples in different successive batches.\r\n *\r\n *   To enable statefulness:\r\n *     - specify `stateful: true` in the layer constructor.\r\n *     - specify a fixed batch size for your model, by passing\r\n *       if sequential model:\r\n *         `batchInputShape=[...]` to the first layer in your model.\r\n *       else for functional model with 1 or more Input layers:\r\n *         `batchShape=[...]` to all the first layers in your model.\r\n *       This is the expected shape of your inputs *including the batch size*.\r\n *       It should be a tuple of integers, e.g. `(32, 10, 100)`.\r\n *     - specify `shuffle=False` when calling fit().\r\n *\r\n *   To reset the states of your model, call `.resetStates()` on either\r\n *   a specific layer, or on your entire model.\r\n *\r\n * Note on specifying the initial state of RNNs\r\n *   You can specify the initial state of RNN layers symbolically by\r\n *   calling them with the option `initialState`. The value of\r\n *   `initialState` should be a tensor or list of tensors representing\r\n *   the initial state of the RNN layer.\r\n *\r\n *   You can specify the initial state of RNN layers numerically by\r\n *   calling `resetStates` with the keyword argument `states`. The value of\r\n *   `states` should be a numpy array or list of numpy arrays representing\r\n *   the initial state of the RNN layer.\r\n *\r\n * Note on passing external constants to RNNs\r\n *   You can pass \"external\" constants to the cell using the `constants`\r\n *   keyword argument of `RNN.call` method. This requires that the `cell.call`\r\n *   method accepts the same keyword argument `constants`. Such constants\r\n *   can be used to conditon the cell transformation on additional static inputs\r\n *   (not changing over time), a.k.a an attention mechanism.\r\n *\r\n * @doc {heading: 'Layers', subheading: 'Recurrent', namespace: 'layers'}\r\n */\nexport function rnn(args) {\n  return new RNN(args);\n}\n/**\r\n * Wrapper allowing a stack of RNN cells to behave as a single cell.\r\n *\r\n * Used to implement efficient stacked RNNs.\r\n *\r\n * @doc {heading: 'Layers', subheading: 'Recurrent', namespace: 'layers'}\r\n */\nexport function stackedRNNCells(args) {\n  return new StackedRNNCells(args);\n}\n// Wrapper Layers.\n/** @doc {heading: 'Layers', subheading: 'Wrapper', namespace: 'layers'} */\nexport function bidirectional(args) {\n  return new Bidirectional(args);\n}\n/**\r\n * This wrapper applies a layer to every temporal slice of an input.\r\n *\r\n * The input should be at least 3D,  and the dimension of the index `1` will be\r\n * considered to be the temporal dimension.\r\n *\r\n * Consider a batch of 32 samples, where each sample is a sequence of 10 vectors\r\n * of 16 dimensions. The batch input shape of the layer is then `[32,  10,\r\n * 16]`, and the `inputShape`, not including the sample dimension, is\r\n * `[10, 16]`.\r\n *\r\n * You can then use `TimeDistributed` to apply a `Dense` layer to each of the 10\r\n * timesteps, independently:\r\n *\r\n * ```js\r\n * const model = tf.sequential();\r\n * model.add(tf.layers.timeDistributed({\r\n *   layer: tf.layers.dense({units: 8}),\r\n *   inputShape: [10, 16],\r\n * }));\r\n *\r\n * // Now model.outputShape = [null, 10, 8].\r\n * // The output will then have shape `[32, 10, 8]`.\r\n *\r\n * // In subsequent layers, there is no need for `inputShape`:\r\n * model.add(tf.layers.timeDistributed({layer: tf.layers.dense({units: 32})}));\r\n * console.log(JSON.stringify(model.outputs[0].shape));\r\n * // Now model.outputShape = [null, 10, 32].\r\n * ```\r\n *\r\n * The output will then have shape `[32, 10, 32]`.\r\n *\r\n * `TimeDistributed` can be used with arbitrary layers, not just `Dense`, for\r\n * instance a `Conv2D` layer.\r\n *\r\n * ```js\r\n * const model = tf.sequential();\r\n * model.add(tf.layers.timeDistributed({\r\n *   layer: tf.layers.conv2d({filters: 64, kernelSize: [3, 3]}),\r\n *   inputShape: [10, 299, 299, 3],\r\n * }));\r\n * console.log(JSON.stringify(model.outputs[0].shape));\r\n * ```\r\n *\r\n * @doc {heading: 'Layers', subheading: 'Wrapper', namespace: 'layers'}\r\n */\nexport function timeDistributed(args) {\n  return new TimeDistributed(args);\n}\n// Aliases for pooling.\nexport const globalMaxPool1d = globalMaxPooling1d;\nexport const globalMaxPool2d = globalMaxPooling2d;\nexport const maxPool1d = maxPooling1d;\nexport const maxPool2d = maxPooling2d;\nexport { Layer, RNN, RNNCell, input /* alias for tf.input */ };\n/**\r\n * Apply additive zero-centered Gaussian noise.\r\n *\r\n * As it is a regularization layer, it is only active at training time.\r\n *\r\n * This is useful to mitigate overfitting\r\n * (you could see it as a form of random data augmentation).\r\n * Gaussian Noise (GS) is a natural choice as corruption process\r\n * for real valued inputs.\r\n *\r\n * # Arguments\r\n *     stddev: float, standard deviation of the noise distribution.\r\n *\r\n * # Input shape\r\n *         Arbitrary. Use the keyword argument `input_shape`\r\n *         (tuple of integers, does not include the samples axis)\r\n *         when using this layer as the first layer in a model.\r\n *\r\n * # Output shape\r\n *         Same shape as input.\r\n *\r\n * @doc {heading: 'Layers', subheading: 'Noise', namespace: 'layers'}\r\n */\nexport function gaussianNoise(args) {\n  return new GaussianNoise(args);\n}\n/**\r\n * Apply multiplicative 1-centered Gaussian noise.\r\n *\r\n * As it is a regularization layer, it is only active at training time.\r\n *\r\n * Arguments:\r\n *   - `rate`: float, drop probability (as with `Dropout`).\r\n *     The multiplicative noise will have\r\n *     standard deviation `sqrt(rate / (1 - rate))`.\r\n *\r\n * Input shape:\r\n *   Arbitrary. Use the keyword argument `inputShape`\r\n *   (tuple of integers, does not include the samples axis)\r\n *   when using this layer as the first layer in a model.\r\n *\r\n * Output shape:\r\n *   Same shape as input.\r\n *\r\n * References:\r\n *   - [Dropout: A Simple Way to Prevent Neural Networks from Overfitting](\r\n *      http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf)\r\n *\r\n * @doc {heading: 'Layers', subheading: 'Noise', namespace: 'layers'}\r\n */\nexport function gaussianDropout(args) {\n  return new GaussianDropout(args);\n}\n/**\r\n * Applies Alpha Dropout to the input.\r\n *\r\n * As it is a regularization layer, it is only active at training time.\r\n *\r\n * Alpha Dropout is a `Dropout` that keeps mean and variance of inputs\r\n * to their original values, in order to ensure the self-normalizing property\r\n * even after this dropout.\r\n * Alpha Dropout fits well to Scaled Exponential Linear Units\r\n * by randomly setting activations to the negative saturation value.\r\n *\r\n * Arguments:\r\n *   - `rate`: float, drop probability (as with `Dropout`).\r\n *     The multiplicative noise will have\r\n *     standard deviation `sqrt(rate / (1 - rate))`.\r\n *   - `noise_shape`: A 1-D `Tensor` of type `int32`, representing the\r\n *     shape for randomly generated keep/drop flags.\r\n *\r\n * Input shape:\r\n *   Arbitrary. Use the keyword argument `inputShape`\r\n *   (tuple of integers, does not include the samples axis)\r\n *   when using this layer as the first layer in a model.\r\n *\r\n * Output shape:\r\n *   Same shape as input.\r\n *\r\n * References:\r\n *   - [Self-Normalizing Neural Networks](https://arxiv.org/abs/1706.02515)\r\n *\r\n * @doc {heading: 'Layers', subheading: 'Noise', namespace: 'layers'}\r\n */\nexport function alphaDropout(args) {\n  return new AlphaDropout(args);\n}\n/**\r\n * Masks a sequence by using a mask value to skip timesteps.\r\n *\r\n * If all features for a given sample timestep are equal to `mask_value`,\r\n * then the sample timestep will be masked (skipped) in all downstream layers\r\n * (as long as they support masking).\r\n *\r\n * If any downstream layer does not support masking yet receives such\r\n * an input mask, an exception will be raised.\r\n *\r\n * Arguments:\r\n *   - `maskValue`: Either None or mask value to skip.\r\n *\r\n * Input shape:\r\n *   Arbitrary. Use the keyword argument `inputShape`\r\n *   (tuple of integers, does not include the samples axis)\r\n *   when using this layer as the first layer in a model.\r\n *\r\n * Output shape:\r\n *   Same shape as input.\r\n *\r\n * @doc {heading: 'Layers', subheading: 'Mask', namespace: 'layers'}\r\n */\nexport function masking(args) {\n  return new Masking(args);\n}","map":null,"metadata":{},"sourceType":"module"}