{"ast":null,"code":"/**\r\n * @license\r\n * Copyright 2018 Google LLC\r\n *\r\n * Use of this source code is governed by an MIT-style\r\n * license that can be found in the LICENSE file or at\r\n * https://opensource.org/licenses/MIT.\r\n * =============================================================================\r\n */\nimport * as tfc from '@tensorflow/tfjs-core';\nimport { variableGrads } from '@tensorflow/tfjs-core';\nimport { getNextUniqueTensorId } from './backend/state';\nimport { getScopedTensorName, getUniqueTensorName } from './common';\nimport { NotImplementedError } from './errors';\nconst DEFAULT_VARIABLE_NAME_PREFIX = 'Variable';\n/**\r\n * A `tf.layers.LayerVariable` is similar to a `tf.Tensor` in that it has a\r\n * dtype and shape, but its value is mutable.  The value is itself represented\r\n * as a`tf.Tensor`, and can be read with the `read()` method and updated with\r\n * the `write()` method.\r\n */\nexport class LayerVariable {\n  /**\r\n   * Construct Variable from a `tf.Tensor`.\r\n   *\r\n   * If not explicitly named, the Variable will be given a name with the\r\n   * prefix 'Variable'. Variable names are unique. In the case of name\r\n   * collision, suffixies '_<num>' will be added to the name.\r\n   *\r\n   * @param val Initial value of the Variable.\r\n   * @param name Name of the variable. If `null` or `undefined` is provided, it\r\n   *   will default a name with the prefix 'Variable'.\r\n   * @param constraint Optional, projection function to be applied to the\r\n   * variable after optimize updates\r\n   * @throws ValueError if `name` is `null` or `undefined`.\r\n   */\n  constructor(val, dtype = 'float32', name = DEFAULT_VARIABLE_NAME_PREFIX, trainable = true, constraint = null) {\n    this.dtype = dtype == null ? 'float32' : dtype;\n    this.shape = val.shape;\n    this.id = getNextUniqueTensorId();\n    name = name == null ? DEFAULT_VARIABLE_NAME_PREFIX : name;\n    this.originalName = getScopedTensorName(name);\n    this.name = getUniqueTensorName(this.originalName);\n    this.trainable_ = trainable;\n    this.constraint = constraint;\n    this.val = tfc.variable(val, this.trainable_, this.name, this.dtype);\n  }\n  /**\r\n   * Get a snapshot of the Variable's value.\r\n   *\r\n   * The returned value is a snapshot of the Variable's value at the time of\r\n   * the invocation. Future mutations in the value of the tensor will only\r\n   * be reflected by future calls to this method.\r\n   */\n  read() {\n    this.assertNotDisposed();\n    return this.val;\n  }\n  /**\r\n   * Update the value of the Variable.\r\n   *\r\n   * @param newVal: The new value to update to. Must be consistent with the\r\n   *   dtype and shape of the Variable.\r\n   * @return This Variable.\r\n   */\n  write(newVal) {\n    // TODO(cais): Once  TF.js Core supports Tensor.dtype, check dtype match.\n    this.assertNotDisposed();\n    checkShapesMatch(this.val, newVal);\n    // Skip updating if this is the exact same tensor.\n    if (this.val.id !== newVal.id) {\n      this.val.assign(newVal);\n      if (this.constraint != null) {\n        this.val.assign(this.constraint.apply(this.val));\n      }\n    }\n    return this;\n  }\n  /**\r\n   * Dispose this LayersVariable instance from memory.\r\n   */\n  dispose() {\n    this.assertNotDisposed();\n    this.val.dispose();\n  }\n  assertNotDisposed() {\n    if (this.val.isDisposed) {\n      throw new Error(`LayersVariable ${this.name} is already disposed.`);\n    }\n  }\n  get trainable() {\n    return this.trainable_;\n  }\n  set trainable(trainable) {\n    this.trainable_ = trainable;\n    this.val.trainable = trainable;\n  }\n}\nfunction checkShapesMatch(x, y) {\n  if (x.shape.toString() !== y.shape.toString()) {\n    throw new Error('Shape mismatch: ' + JSON.stringify(x.shape) + ' vs. ' + JSON.stringify(y.shape));\n  }\n}\n/**\r\n * Create a Variable.\r\n * @param x The initial value of the `Variable`.\r\n * @param dtype optional, the type of the variable.\r\n * @param name optional, the name of the variable, default provided by\r\n * Variable.\r\n * @param constraint optional, a constraint to be applied after every update.\r\n * @return The newly instantiated `Variable`.\r\n */\nexport function variable(x, dtype, name, constraint) {\n  return new LayerVariable(x, dtype, name, true, constraint);\n}\n/**\r\n * Instantiates an all-zeros Variable and returns it.\r\n *\r\n * @param shape Shape of the tensor.\r\n * @param dtype DType of the tensor.\r\n * @param name Name of the tensor.\r\n * @return An all-zero Variable.\r\n */\nexport function zerosVariable(shape, dtype, name) {\n  // TODO(cais): Implement logic for dtype.\n  return new LayerVariable(tfc.zeros(shape), dtype, name);\n}\n/**\r\n * Instantiates an all-zeros tensor of the same shape as another tensor.\r\n *\r\n * @param x The other tensor.\r\n * @param dtype DType of the tensor.\r\n * @param name Name of the tensor.\r\n * @return A newly instantiated Variable.\r\n */\nexport function zerosLike(x, dtype, name) {\n  return new LayerVariable(tfc.zerosLike(x), dtype, name);\n}\n/**\r\n * Instantiates an all-ones tensor and returns it.\r\n *\r\n * @param shape Shape of the tensor.\r\n * @param dtype DType of the tensor.\r\n * @param name Name of the tensor.\r\n * @return An all-ones Variable.\r\n */\nexport function onesVariable(shape, dtype, name) {\n  // TODO(cais): Implement logic for dtype.\n  const allocated = tfc.ones(shape);\n  return new LayerVariable(allocated, dtype, name);\n}\n/**\r\n * Instantiates an all-ones tensor of the same shape as another tensor.\r\n *\r\n * @param x The other tensor.\r\n * @param dtype DType of the tensor.\r\n * @param name Name of the tensor.\r\n * @return A newly instantiated Variable.\r\n */\nexport function onesLike(x, dtype, name) {\n  const allocated = tfc.onesLike(x);\n  return new LayerVariable(allocated, dtype, name);\n}\n/**\r\n * Instantiate an identity matrix and returns it, as a Variable\r\n *\r\n * @param size Number of rows/columns.\r\n * @param dtype Data type of returned Variable.\r\n * @param name Name of returned Variable.\r\n * @return A Variable, an identity matrix.\r\n */\nexport function eyeVariable(size, dtype, name) {\n  return new LayerVariable(tfc.eye(size), dtype, name);\n}\n/**\r\n * Get a Variable with uniform distribution of values.\r\n * @param shape Shape of the tensor.\r\n * @param minval Lower bound of the uniform distribution.\r\n * @param maxval Upper bound of the uniform distribution.\r\n * @param dtype\r\n * @param seed\r\n * @param name Optional name.\r\n * @return The uniform-random Variable.\r\n */\nexport function randomUniformVariable(shape, minval, maxval, dtype, seed, name = 'randomUniform') {\n  return new LayerVariable(tfc.randomUniform(shape, minval, maxval, dtype), dtype, name);\n}\n/**\r\n * Get a Variable with truncated-normal distribution of values.\r\n * @param shape Shape of the tensor.\r\n * @param mean mean value of the normal distribution.\r\n * @param stddev standard deviation of the normal distribution.\r\n * @param dtype\r\n * @param seed\r\n * @param name Optional name.\r\n * @return The truncated-normal-random Variable.\r\n */\nexport function truncatedNormalVariable(shape, mean = 0.0, stddev = 1.0, dtype, seed, name = 'truncatedNormal') {\n  // TODO(cais): Implement logic for dtype and seed once they are supported\n  // by deeplearn.js.\n  dtype = dtype || 'float32';\n  if (dtype !== 'float32' && dtype !== 'int32') {\n    throw new NotImplementedError(`randomNormal does not support dType ${dtype}.`);\n  }\n  return new LayerVariable(tfc.truncatedNormal(shape, mean, stddev, dtype, seed), dtype, name);\n}\n/**\r\n * Get a Variable with normal distribution of values.\r\n * @param shape Shape of the tensor.\r\n * @param mean mean value of the normal distribution.\r\n * @param stddev standard deviation of the normal distribution.\r\n * @param dtype\r\n * @param seed\r\n * @param name Optional name.\r\n * @return The truncated-normal-random Variable.\r\n */\nexport function randomNormalVariable(shape, mean = 0.0, stddev = 1.0, dtype, seed, name = 'randomNormal') {\n  dtype = dtype || 'float32';\n  if (dtype !== 'float32' && dtype !== 'int32') {\n    throw new NotImplementedError(`randomNormalVariable does not support dType ${dtype}.`);\n  }\n  return new LayerVariable(tfc.randomNormal(shape, mean, stddev, dtype, seed), dtype, name);\n}\n/**\r\n * Update the value of a Variable.\r\n * @param x The Variable to be updated.\r\n * @param xNew The new value to update to.\r\n * @return The Variable updated.\r\n */\nexport function update(x, xNew) {\n  return x.write(xNew);\n}\n/**\r\n * Update the value of a Variable by adding an increment.\r\n * @param x The Variable to be updated.\r\n * @param increment The incrment to add to `x`.\r\n * @return The Variable updated.\r\n */\nexport function updateAdd(x, increment) {\n  return x.write(tfc.add(x.read(), increment));\n}\n/**\r\n * Update the value of a Variable by subtracting a decrement.\r\n * @param x The Variable to be updated.\r\n * @param decrement The decrement to subtract from `x`.\r\n * @return The Variable updated.\r\n */\nexport function updateSub(x, decrement) {\n  return x.write(tfc.sub(x.read(), decrement));\n}\n/**\r\n * Get the values of an array of Variables.\r\n *\r\n * @param tensors An `Array` of `Variable`s to get the values of.\r\n * @return The values of the inputs, as an `Array` of`tf.Tensor`s.\r\n */\nexport function batchGetValue(xs) {\n  return xs.map(x => x.read());\n}\n/**\r\n * Update the value of multiple Variables at once.\r\n *\r\n * @param variablesAndValues An `Array`, each element is of type\r\n *   [Variable, Tensor]. The first item is the\r\n *   `Variable` of which the value is to be updated. The second item\r\n *   carries the new value.\r\n */\nexport function batchSetValue(variablesAndValues) {\n  variablesAndValues.forEach(variableAndValue => {\n    const variable = variableAndValue[0];\n    variable.write(variableAndValue[1]);\n  });\n}\n/**\r\n * Returns the gradients of `variables` w.r.t. the return value of `lossFn`.\r\n * @param lossFn A function which returns a Scalar to be used as the function\r\n *   value (i.e., numerator) for differentiation.\r\n * @param variables List of variables to be used as the independent variables\r\n *   (i.e., denominator) for differentiation.\r\n * @returns An Array of gradients tensors.\r\n */\nexport function gradients(lossFn, variables) {\n  // TODO(cais): The return type signature can be simplified if deeplearn makes\n  //   the corresponding type public.\n  const variableList = variables.map(variable => variable.read());\n  const valudAndGrads = variableGrads(lossFn, variableList);\n  return variables.map(variable => valudAndGrads.grads[variable.name]);\n}","map":null,"metadata":{},"sourceType":"module"}