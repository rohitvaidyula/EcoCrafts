{"ast":null,"code":"/**\r\n * @license\r\n * Copyright 2018 Google LLC\r\n *\r\n * Use of this source code is governed by an MIT-style\r\n * license that can be found in the LICENSE file or at\r\n * https://opensource.org/licenses/MIT.\r\n * =============================================================================\r\n */\n/**\r\n * deeplearn.js backend.\r\n */\nimport * as tfc from '@tensorflow/tfjs-core';\nimport { onesLike as coreOnesLike, scalar, tensor1d, tidy, where, zerosLike as coreZerosLike } from '@tensorflow/tfjs-core';\nimport { checkDataFormat } from '../common';\nimport { NotImplementedError, ValueError } from '../errors';\nimport * as math_utils from '../utils/math_utils';\nimport { imageDataFormat } from './common';\n// tslint:enable\n/* Setting and getting backend from deeplearn.js. */\n// Default deeplearn.js backend is WebGL (GPU).\nlet backend = 'webgl';\nexport function setBackend(requestedBackend) {\n  tfc.setBackend(requestedBackend);\n  backend = requestedBackend;\n}\nexport function getBackend() {\n  return backend;\n}\n/**\r\n * Indicates whether the backend is operating symbolically.\r\n *\r\n * This function will be used to determine how to interpret user code. If\r\n * it returns true, calls to the backend construct a symbolic graph; if\r\n * it returns false, calls to the backend execute immediately.\r\n */\nexport function isBackendSymbolic() {\n  return false;\n}\n/**\r\n * Get the number of elements in a Tensor.\r\n * @param x The Tensor.\r\n * @return Number of elements in `x`.\r\n */\nexport function countParams(x) {\n  const shape = x.shape;\n  if (shape.length > 0) {\n    return shape.reduce((a, b) => a * b);\n  } else {\n    // Scalar.\n    return 1;\n  }\n}\n/**\r\n * Casts a tensor to a different dtype and returns it.\r\n * @param x Input tensor.\r\n * @param dtype String: 'float32'|'int32'|'bool'.\r\n * @returns Tensor of the specified `dtype`.\r\n */\nexport function cast(x, dtype) {\n  return x.asType(dtype);\n}\n/**\r\n * Adds a 1-sized dimension at index \"axis\".\r\n * @param x Input tensor.\r\n * @param axis Position where to add the new axis.\r\n * @returns Result of the dimension expansion.\r\n */\nexport function expandDims(x, axis = -1) {\n  const outShape = x.shape.slice();\n  if (axis < 0) {\n    axis = outShape.length + axis + 1;\n  }\n  outShape.splice(axis, 0, 1);\n  return x.reshape(outShape);\n}\n/**\r\n * Repeats a 2D tensor.\r\n *\r\n * If `x` has shape `[samples, dim]` and `n` is 2, for example, the output\r\n * will have shape `[samples, 2, dim]`.\r\n *\r\n * @param x Input tensor.\r\n * @param n Integer, number of times to repeat.\r\n * @returns The result of the repeat operation.\r\n * @throws ValueError: If input tensor is not 2D.\r\n */\nexport function repeat(x, n) {\n  return tidy(() => {\n    if (x.shape.length !== 2) {\n      throw new ValueError(`repeat() expects a rank-2 tensor, but received a ` + `rank-${x.shape.length} tensor.`);\n    }\n    const y = expandDims(x, 1);\n    return tile(y, [1, n, 1]);\n  });\n}\n/**\r\n * Flatten a Tensor into 1D.\r\n * @param x Input tensor.\r\n * @return The result of the flattening `x`.\r\n */\nexport function flatten(x) {\n  const newShape = [math_utils.arrayProd(x.shape)];\n  return x.reshape(newShape);\n}\n/**\r\n * Turn a nD tensor into a 2D tensor with same 0th dimension.\r\n * In other words, it flattens each data samples of a batch.\r\n *\r\n * @param x The tensor to flatten. The rank of this tensor is required to be 2\r\n *   or higher.\r\n * @return The result of the flattening.\r\n */\nexport function batchFlatten(x) {\n  if (x.rank <= 1) {\n    throw new ValueError(`batchFlatten requires a minimum rank of 2. Got rank: ${x.rank}.`);\n  }\n  const newShape = [x.shape[0], math_utils.arrayProd(x.shape, 1)];\n  return x.reshape(newShape);\n}\n/**\r\n * Do slicing along the first axis.\r\n * @param array input `tf.Tensor`.\r\n * @param start starting index, inclusive.\r\n * @param size size of the slice along the first axis.\r\n * @returns result of the slicing.\r\n * @throws ValueError: If `array` is of an unsupported subtype of `tf.Tensor`.\r\n */\nexport function sliceAlongFirstAxis(array, start, size) {\n  return tidy(() => {\n    switch (array.rank) {\n      case 1:\n        return tfc.slice1d(array, start, size);\n      case 2:\n        return tfc.slice2d(array, [start, 0], [size, array.shape[1]]);\n      case 3:\n        return tfc.slice3d(array, [start, 0, 0], [size, array.shape[1], array.shape[2]]);\n      case 4:\n        return tfc.slice4d(array, [start, 0, 0, 0], [size, array.shape[1], array.shape[2], array.shape[3]]);\n      case 5:\n        return tfc.slice(array, [start, 0, 0, 0, 0], [size, array.shape[1], array.shape[2], array.shape[3], array.shape[4]]);\n      case 6:\n        return tfc.slice(array, [start, 0, 0, 0, 0, 0], [size, array.shape[1], array.shape[2], array.shape[3], array.shape[4], array.shape[5]]);\n      default:\n        throw new ValueError(`sliceAlongFirstAxis() received an unsupported tensor rank: ` + `${array.rank}`);\n    }\n  });\n}\n/**\r\n * Do slicing along the last axis.\r\n * @param array input `tf.Tensor`.\r\n * @param start starting index, inclusive.\r\n * @param size size of the slice along the last axis.\r\n * @returns result of the slicing.\r\n * @throws ValueError: If `array` is of an unsupported subtype of `tf.Tensor`.\r\n */\nexport function sliceAlongLastAxis(array, start, size) {\n  return tidy(() => {\n    switch (array.rank) {\n      case 1:\n        return tfc.slice1d(array, start, size);\n      case 2:\n        return tfc.slice2d(array, [0, start], [array.shape[0], size]);\n      case 3:\n        return tfc.slice3d(array, [0, 0, start], [array.shape[0], array.shape[1], size]);\n      case 4:\n        return tfc.slice4d(array, [0, 0, 0, start], [array.shape[0], array.shape[1], array.shape[2], size]);\n      default:\n        throw new ValueError(`sliceAlongLastAxis() received an unsupported tensor rank: ` + `${array.rank}`);\n    }\n  });\n}\n/**\r\n * Do slicing along the sepcified axis.\r\n * @param array input `tf.Tensor`.\r\n * @param start starting index, inclusive.\r\n * @param size of the slice along the chosen axis.\r\n * @param choose an axis.\r\n * @returns result of the slicing.\r\n * @throws ValueError: If `array` is of an unsupported subtype of `tf.Tensor`.\r\n */\nexport function sliceAlongAxis(array, start, size, axis) {\n  return tidy(() => {\n    switch (array.rank) {\n      case 1:\n        return tfc.slice1d(array, start, size);\n      case 2:\n        switch (axis) {\n          case 1:\n            return sliceAlongFirstAxis(array, start, size);\n          case 2:\n            return sliceAlongLastAxis(array, start, size);\n          default:\n            throw new ValueError(`The axis is not within the rank of the tensor ` + `${axis}`);\n        }\n      case 3:\n        switch (axis) {\n          case 1:\n            return sliceAlongFirstAxis(array, start, size);\n          case 2:\n            return tfc.slice3d(array, [0, start, 0], [array.shape[0], size, array.shape[2]]);\n          case 3:\n            return sliceAlongLastAxis(array, start, size);\n          default:\n            throw new ValueError(`The axis is not within the rank of the tensor ` + `${axis}`);\n        }\n      case 4:\n        switch (axis) {\n          case 1:\n            return sliceAlongFirstAxis(array, start, size);\n          case 2:\n            return tfc.slice4d(array, [0, start, 0, 0], [array.shape[0], size, array.shape[2], array.shape[3]]);\n          case 3:\n            return tfc.slice4d(array, [0, 0, start, 0], [array.shape[0], array.shape[1], size, array.shape[3]]);\n          case 4:\n            return sliceAlongLastAxis(array, start, size);\n          default:\n            throw new ValueError(`The axis is not within the rank of the tensor ` + `${axis}`);\n        }\n      default:\n        throw new ValueError(`sliceAlongLastAxis() received an unsupported tensor rank: ` + `${array.rank}`);\n    }\n  });\n}\n/**\r\n * Concatenates a list of tensors alongside the specified axis.\r\n * @param tensors `Array` of tensors to concatenate.\r\n * @param axis Concatenation axis.\r\n * @returns The result of the concatenation.\r\n */\nexport function concatenate(tensors, axis = -1) {\n  let rank;\n  if (axis < 0) {\n    rank = tensors[0].rank;\n    if (rank !== 0) {\n      axis = rank;\n    } else {\n      axis = 0;\n    }\n  }\n  if (axis === tensors[0].rank) {\n    // Porting Note: This is necessary because tfc.concat() requires axis to be\n    //   in the interval [-rank, rank).\n    axis = -1;\n  }\n  // Porting Note: Sparse concat is not supported yet.\n  return tfc.concat(tensors, axis);\n}\n/**\r\n * Concatenate two arrays along the first dimension.\r\n * @param a The 1st `tf.Tensor` to concatenate.\r\n * @param b The 2nd `tf.Tensor` to concatenate.\r\n * @returns Result of the concatenation.\r\n * @throws ValueError: If `a` is of an unsupported subtype of `tf.Tensor`.\r\n */\nexport function concatAlongFirstAxis(a, b) {\n  switch (a.rank) {\n    case 1:\n      return tfc.concat1d([a, b]);\n    case 2:\n      return tfc.concat2d([a, b], 0);\n    case 3:\n      return tfc.concat3d([a, b], 0);\n    case 4:\n      return tfc.concat4d([a, b], 0);\n    default:\n      throw new ValueError(`concatAlongFirstAxis() received an unsupported ` + `tensor rank: ${a.rank}`);\n  }\n}\n/**\r\n * Creates a tensor by tiling `x` by `n`.\r\n * @param x A tensor.\r\n * @param n An Array of integers or a single integer. If an Array, the length\r\n *   must be the same as the number of dimensions in `x`. If a single integer,\r\n *   it will be treated as an Array of length 1.\r\n */\nexport function tile(x, n) {\n  if (!Array.isArray(n)) {\n    n = [n];\n  }\n  if (x.rank !== n.length) {\n    throw new ValueError(`The length of input n (${n.length}) does not match ` + `the number of dimensions in input x (${x.rank})`);\n  }\n  return tfc.tile(x, n);\n}\n/* Creation of random tensors. */\n/**\r\n * Get a tensor with normal distribution of values.\r\n *\r\n * @param shape Shape of the tensor.\r\n * @param mean mean value of the normal distribution.\r\n * @param stddev standard deviation of the normal distribution.\r\n * @param dtype\r\n * @param seed\r\n * @return The normal tensor.\r\n */\nexport function randomNormal(shape, mean = 0.0, stddev = 1.0, dtype, seed) {\n  return tfc.randomNormal(shape, mean, stddev, dtype, seed);\n}\n/* Linear Algebra */\n/**\r\n * Multiply two tensors and returns the result as a tensor.\r\n *\r\n * For 2D tensors, this is equivalent to matrix multiplication (matMul).\r\n * For tensors of higher ranks, it follows the Theano behavior,\r\n * (e.g. `(2, 3) * (4, 3, 5) -> (2, 4, 5)`).  From the Theano documentation:\r\n *\r\n * For N dimensions it is a sum product over the last axis of x and the\r\n * second-to-last of y:\r\n *\r\n * @param a A tensor of at least rank 2.\r\n * @param b A tensor of at least rank 2.\r\n * @param activation (optional) A string identifying the activation\r\n *   function.\r\n * @return Result of the dot operation.\r\n */\nexport function dot(a, b, activation, bias) {\n  if (a.rank < 2 || b.rank < 2) {\n    throw new NotImplementedError(`dot requires both inputs to be rank >= 2` + ` but got x shape = ${a.shape} and y shape = ${b.shape}`);\n  }\n  if (b.rank >= 3) {\n    const xLastDim = a.shape.slice(-1)[0];\n    const ySecondLastDim = b.shape.slice(-2)[0];\n    if (xLastDim !== ySecondLastDim) {\n      throw new NotImplementedError(`If rank y >= 3, then the second last dim` + ` of y must equal the last dim of x but got x shape = ${a.shape} and ` + ` y shape = ${b.shape}`);\n    }\n  }\n  // Handle basic 2D x 2D case.\n  if (a.rank === 2 && b.rank === 2) {\n    const transposeA = false;\n    const transposeB = false;\n    // tfc.fused.matMul only fuses certain activation functions. Unsupported\n    // activation functions are treated as 'linear' activations, which is\n    // equivalent to a no-op.\n    return tfc.fused.matMul({\n      a,\n      b: b,\n      transposeA,\n      transposeB,\n      bias: bias ? reshapeBias(a.rank, bias, imageDataFormat()) : null,\n      activation\n    });\n  } else {\n    // Reshape x into the analogous 2D Tensor.\n    const aFirstDims = a.shape.slice(); // Holds all but the last dim of x.\n    const aLastDim = aFirstDims.pop();\n    a = a.reshape([-1, aLastDim]);\n    // Reshape y into the analogous 2D Tensor, and keep track of the\n    // required dimensions to reproduce the output shape.\n    const bShape = b.shape.slice();\n    const bLastDim = bShape.pop();\n    const ySecondLastDim = bShape.pop();\n    const yOtherDims = [...bShape, bLastDim];\n    // permutation should be like [r-2, 0, 1, 2, ... r-4, r-3, r-1]\n    // where r is the rank of y.\n    const perm = Array.from({\n      length: b.rank\n    }, (_, i) => {\n      if (i === 0) {\n        return b.rank - 2;\n      } else if (i <= b.rank - 2) {\n        return i - 1;\n      }\n      return i;\n    });\n    b = b.transpose(perm).reshape([ySecondLastDim, -1]);\n    // Multiply x and y as 2D Tensors, and then reshape back to original.\n    const outputShape = [...aFirstDims, ...yOtherDims];\n    const transposeA = false;\n    const transposeB = false;\n    return tfc.fused.matMul({\n      a,\n      b,\n      transposeA,\n      transposeB,\n      bias: bias ? reshapeBias(a.rank, bias, imageDataFormat()) : null,\n      activation\n    }).reshape(outputShape);\n  }\n}\n/**\r\n * Compute the sign Tensor of an input Tensor.\r\n *\r\n * Elements of the input `tf.Tensor` that are === 0 are mapped to 0.\r\n * Elements of the input `tf.Tensor` that are > 0 are mapped to 1.\r\n * Elements of the input `tf.Tensor` that are < 0 are mapped to -1.\r\n *\r\n * @param x Input `tf.Tensor`.\r\n * @return The sign `tf.Tensor`.\r\n */\nexport function sign(x) {\n  // TODO(cais): Move to the core.\n  return tidy(() => {\n    const zerosLikeX = coreZerosLike(x);\n    const onesLikeX = coreOnesLike(x);\n    return where(tfc.equal(x, zerosLikeX), zerosLikeX, where(tfc.greater(x, coreZerosLike(x)), onesLikeX, tfc.mul(-1, onesLikeX)));\n  });\n}\n/**\r\n * Computes the one-hot representation of an integer tensor.\r\n * @param indices nD integer tensor of shape\r\n *   `(batch_size, dim1, dim2, ... dim(n-1))`\r\n * @param numClasses Integer, number of classes to consider.\r\n * @returns (n + 1)D one hot representation of the input\r\n *   with shape `(batch_size, dim1, dim2, ... dim(n-1), num_classes)`\r\n */\nexport function oneHot(indices, numClasses) {\n  return tidy(() => {\n    if (indices.rank !== 1) {\n      throw new Error('Only 1D one-hot tensors are supported in the ' + 'deeplearn backend, at present.');\n    }\n    indices = indices.toInt();\n    return tfc.oneHot(indices, numClasses).toFloat();\n  });\n}\n/* Elementary math functions. */\n/**\r\n * Retrieves the elements of indices `indices` in the tensor `reference`.\r\n * @param reference A tensor.\r\n * @param indices An integer tensor of indices or an `Array` of integers.\r\n * @param axis Axis along which to perform the gather operation.\r\n * @returns The result of the gathering as a tensor.\r\n */\nexport function gather(reference, indices, axis) {\n  return tidy(() => {\n    if (Array.isArray(indices)) {\n      indices = tensor1d(indices, 'int32');\n    } else {\n      indices = indices.toInt();\n    }\n    return tfc.gather(reference, indices, axis);\n  });\n}\n/**\r\n * Element-wise square.\r\n * @param x Input tensor.\r\n * @return element-wise x^2\r\n */\nexport function square(x) {\n  return tfc.mul(x, x);\n}\n/**\r\n * Element-wise exponentiation.\r\n *\r\n * Porting Note: In PyKeras, `a` (the exponent) is a Python integer, which\r\n *   takes advatnage of the backend's (e.g., TensorFlow's) automatic\r\n * conversion to tensor. Here we allow `a` to be either a number or a tensor.\r\n *\r\n * @param x The base tensor.\r\n * @param a The exponent, tensor or number. If a number, it is rounded to the\r\n *   nearest integer and converted to a tensor.\r\n * @returns A tensor of the same shape as `x`.\r\n */\nexport function pow(x, a) {\n  return tidy(() => {\n    if (typeof a === 'number') {\n      a = scalar(Math.round(a), 'int32');\n    }\n    if (a.dtype !== 'int32') {\n      throw new NotImplementedError(`Non-int32 dtype (${a.dtype}) is not supported by pow() yet`);\n    }\n    return tfc.pow(x, a);\n  });\n}\n/**\r\n * Reshapes bias tensor according to rank of x.\r\n */\nfunction reshapeBias(xRank, bias, dataFormat) {\n  const biasShape = bias.shape;\n  if (bias.rank !== 1 && bias.rank !== xRank) {\n    throw new ValueError(`Unexpected bias dimensions: ${bias.rank}` + `; expected it to be 1 or ${xRank}`);\n  }\n  if (xRank === 5) {\n    if (dataFormat === 'channelsFirst') {\n      if (biasShape.length === 1) {\n        return bias.reshape([1, biasShape[0], 1, 1, 1]);\n      } else {\n        return bias.reshape([1, biasShape[3], biasShape[0], biasShape[1], biasShape[2]]);\n      }\n    } else if (dataFormat === 'channelsLast') {\n      if (biasShape.length === 1) {\n        return bias.reshape([1, 1, 1, 1, biasShape[0]]);\n      } else {\n        return bias.reshape([1].concat(biasShape));\n      }\n    }\n  } else if (xRank === 4) {\n    if (dataFormat === 'channelsFirst') {\n      if (biasShape.length === 1) {\n        return bias.reshape([1, biasShape[0], 1, 1]);\n      } else {\n        return bias.reshape([1, biasShape[2], biasShape[0], biasShape[1]]);\n      }\n    } else if (dataFormat === 'channelsLast') {\n      if (biasShape.length === 1) {\n        return bias.reshape([1, 1, 1, biasShape[0]]);\n      } else {\n        return bias.reshape([1].concat(biasShape));\n      }\n    }\n  } else if (xRank === 3) {\n    if (dataFormat === 'channelsFirst') {\n      if (biasShape.length === 1) {\n        return bias.reshape([1, biasShape[0], 1]);\n      } else {\n        return bias.reshape([1, biasShape[1], biasShape[0]]);\n      }\n    } else if (dataFormat === 'channelsLast') {\n      if (biasShape.length === 1) {\n        return bias.reshape([1, 1, biasShape[0]]);\n      } else {\n        return bias.reshape([1].concat(biasShape));\n      }\n    }\n  } else if (xRank < 3) {\n    return bias;\n  }\n  throw new ValueError(`Unsupported input rank by biasAdd: ${bias.rank}`);\n}\n/* Neural-network operations. */\n/**\r\n * Add a bias to a tensor.\r\n *\r\n * @param x The tensor to add the bias to.\r\n * @param bias The bias to add to `x`. Must be 1D or the same rank as `x`.\r\n * @return Result of the bias adding.\r\n * @throws ValueError: If the rank of `bias` is incorrect.\r\n */\nexport function biasAdd(x, bias, dataFormat) {\n  return tidy(() => {\n    if (dataFormat == null) {\n      dataFormat = imageDataFormat();\n    }\n    checkDataFormat(dataFormat);\n    return x.add(reshapeBias(x.rank, bias, dataFormat));\n  });\n}\n/**\r\n * Exponential linear unit (ELU).\r\n * @param x A tensor or variable to compute the activation function for.\r\n * @param alpha: A scalar, a scaling factor for the negative section.\r\n * @return Output of the ELU operation.\r\n */\nexport function elu(x, alpha = 1) {\n  // TODO(cais): Add support for alpha values other than 1.\n  if (alpha !== 1) {\n    throw new NotImplementedError(`Support for alpha values other than 1 (${alpha}) is not implemented ` + `yet.`);\n  }\n  return tfc.elu(x);\n}\n/**\r\n * Softsign of a tensor.\r\n *\r\n * Defined as x / (abs(x) + 1), element-wise.\r\n *\r\n * @param x: Input.\r\n * @returns Output.\r\n */\nexport function softsign(x) {\n  return tidy(() => tfc.div(x, tfc.abs(x).add(1)));\n}\n/**\r\n * Sets entries in `x` to zero at random, while scaling the entire tensor.\r\n *\r\n * @param x input tensor.\r\n * @param level fraction of the entries in the tensor that will be set to 0.\r\n * @param noiseShape shape of randomly generated keep/drop flags, must be\r\n *   broadcastable to the shape of `x`. Optional.\r\n * @param seed random seed to ensure determinism. Optional.\r\n * @returns Result of the dropout operation.\r\n */\nexport function dropout(x, level, noiseShape, seed) {\n  return tidy(() => tfc.dropout(x, level, noiseShape, seed));\n}\n/**\r\n * Element-wise, segment-wise linear approximation of sigmoid.\r\n *\r\n * Returns `0.` if `x < -2.5`, `1.` if `x > 2.5`.\r\n * In `-2.5 <= x <= 2.5`, returns `0.2 * x + 0.5`.\r\n *\r\n * @param x Input tensor.\r\n * @returns Output tensor.\r\n */\nexport function hardSigmoid(x) {\n  return tidy(() => {\n    const y = tfc.add(.5, tfc.mul(.2, x));\n    return tfc.clipByValue(y, 0, 1);\n  });\n}\n/**\r\n * Invoke `x` in the training phase, and `alt` otherwise.\r\n *\r\n * Porting Note: We do not create placeholder tensors for the `training`\r\n * boolean flag here, because there is no such thing in the TF.js imperative\r\n * backend.\r\n *\r\n * @param x The function to invoke iff `training` is `true`.\r\n * @param alt The function to invoke iff `training` is `false`.\r\n * @param training Boolean flag for whether training phase is active.\r\n * @returns The return value of `x()` if `training` is `true`, or the return\r\n *   value of `alt()` if `training` is `false`.\r\n */\nexport function inTrainPhase(x, alt, training = false) {\n  return training ? x() : alt();\n}","map":null,"metadata":{},"sourceType":"module"}