{"ast":null,"code":"/**\r\n * @license\r\n * Copyright 2018 Google LLC. All Rights Reserved.\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n * http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n * =============================================================================\r\n */\nimport { KernelBackend } from './backends/backend';\nimport { Environment, setEnvironmentGlobal } from './environment';\nimport { getGlobalNamespace } from './global_util';\nimport { Add, Cast } from './kernel_names';\nimport { getGradient, getKernel, getKernelsForBackend } from './kernel_registry';\nimport { Profiler } from './profiler';\nimport { backpropagateGradients, getFilteredNodesXToY } from './tape';\nimport { setTensorTracker, Tensor, Variable } from './tensor';\nimport { getTensorsInContainer } from './tensor_util';\nimport * as util from './util';\nimport { bytesFromStringArray, makeOnesTypedArray, now, sizeFromShape } from './util';\nclass EngineState {\n  constructor() {\n    // Public since optimizers will use it.\n    this.registeredVariables = {};\n    this.nextTapeNodeId = 0;\n    this.numBytes = 0;\n    this.numTensors = 0;\n    this.numStringTensors = 0;\n    this.numDataBuffers = 0;\n    // Number of nested tf.grad() statements when computing higher-order\n    // gradients. E.g. `1` for first-order gradients and `2` for second-order\n    // gradients. Used to track if the tape should be removed after a backprop.\n    this.gradientDepth = 0;\n    // Number of nested kernel calls. When kernel depth is greater than 1, we turn\n    // off the tape.\n    this.kernelDepth = 0;\n    this.scopeStack = [];\n    /**\r\n     * Keeps track of the number of data moves during a kernel execution. We\r\n     * maintain a stack since kernels can call other kernels, recursively.\r\n     */\n    this.numDataMovesStack = [];\n    this.nextScopeId = 0;\n    this.tensorInfo = new WeakMap();\n    this.profiling = false;\n    this.activeProfile = {\n      newBytes: 0,\n      newTensors: 0,\n      peakBytes: 0,\n      kernels: [],\n      result: null,\n      get kernelNames() {\n        return Array.from(new Set(this.kernels.map(k => k.name)));\n      }\n    };\n  }\n  dispose() {\n    for (const variableName in this.registeredVariables) {\n      this.registeredVariables[variableName].dispose();\n    }\n  }\n}\nexport class Engine {\n  constructor(ENV) {\n    this.ENV = ENV;\n    this.registry = {};\n    this.registryFactory = {};\n    this.pendingBackendInitId = 0;\n    this.state = new EngineState();\n  }\n  async ready() {\n    if (this.pendingBackendInit != null) {\n      return this.pendingBackendInit.then(() => {});\n    }\n    if (this.backendInstance != null) {\n      return;\n    }\n    const sortedBackends = this.getSortedBackends();\n    for (let i = 0; i < sortedBackends.length; i++) {\n      const backendName = sortedBackends[i];\n      const success = await this.initializeBackend(backendName).success;\n      if (success) {\n        await this.setBackend(backendName);\n        return;\n      }\n    }\n    throw new Error(`Could not initialize any backends, all backend initializations ` + `failed.`);\n  }\n  get backend() {\n    if (this.pendingBackendInit != null) {\n      throw new Error(`Backend '${this.backendName}' has not yet been initialized. Make ` + `sure to await tf.ready() or await tf.setBackend() before calling ` + `other methods`);\n    }\n    if (this.backendInstance == null) {\n      const {\n        name,\n        asyncInit\n      } = this.initializeBackendsAndReturnBest();\n      if (asyncInit) {\n        throw new Error(`The highest priority backend '${name}' has not yet been ` + `initialized. Make sure to await tf.ready() or ` + `await tf.setBackend() before calling other methods`);\n      }\n      this.setBackend(name);\n    }\n    return this.backendInstance;\n  }\n  backendNames() {\n    return Object.keys(this.registryFactory);\n  }\n  findBackend(backendName) {\n    if (!(backendName in this.registry)) {\n      // If the backend hasn't been initialized but we have a registry entry for\n      // it, initialize it and return it.\n      if (backendName in this.registryFactory) {\n        const {\n          asyncInit\n        } = this.initializeBackend(backendName);\n        if (asyncInit) {\n          // Backend is not ready yet.\n          return null;\n        }\n      } else {\n        return null;\n      }\n    }\n    return this.registry[backendName];\n  }\n  findBackendFactory(backendName) {\n    if (!(backendName in this.registryFactory)) {\n      return null;\n    }\n    return this.registryFactory[backendName].factory;\n  }\n  registerBackend(backendName, factory, priority = 1) {\n    if (backendName in this.registryFactory) {\n      console.warn(`${backendName} backend was already registered. ` + `Reusing existing backend factory.`);\n      return false;\n    }\n    this.registryFactory[backendName] = {\n      factory,\n      priority\n    };\n    return true;\n  }\n  async setBackend(backendName) {\n    if (this.registryFactory[backendName] == null) {\n      throw new Error(`Backend name '${backendName}' not found in registry`);\n    }\n    this.backendName = backendName;\n    if (this.registry[backendName] == null) {\n      this.backendInstance = null;\n      const {\n        success,\n        asyncInit\n      } = this.initializeBackend(backendName);\n      const result = asyncInit ? await success : success;\n      if (!result) {\n        return false;\n      }\n    }\n    this.backendInstance = this.registry[backendName];\n    this.setupRegisteredKernels();\n    // Reset the profiler.\n    this.profiler = new Profiler(this.backendInstance);\n    return true;\n  }\n  setupRegisteredKernels() {\n    const kernels = getKernelsForBackend(this.backendName);\n    kernels.forEach(kernel => {\n      if (kernel.setupFunc != null) {\n        kernel.setupFunc(this.backendInstance);\n      }\n    });\n  }\n  disposeRegisteredKernels(backendName) {\n    const kernels = getKernelsForBackend(backendName);\n    kernels.forEach(kernel => {\n      if (kernel.disposeFunc != null) {\n        kernel.disposeFunc(this.registry[backendName]);\n      }\n    });\n  }\n  /**\r\n   * Initializes a backend by looking up the backend name in the factory\r\n   * registry and calling the factory method. Returns a boolean representing\r\n   * whether the initialization of the backend suceeded. Throws an error if\r\n   * there is no backend in the factory registry.\r\n   */\n  initializeBackend(backendName) {\n    const registryFactoryEntry = this.registryFactory[backendName];\n    if (registryFactoryEntry == null) {\n      throw new Error(`Cannot initialize backend ${backendName}, no registration found.`);\n    }\n    try {\n      const backend = registryFactoryEntry.factory();\n      /* Test if the factory returns a promise.\r\n      Done in a more liberal way than\r\n      previous 'Promise.resolve(backend)===backend'\r\n      as we needed to account for custom Promise\r\n      implementations (e.g. Angular) */\n      if (backend && !(backend instanceof KernelBackend) && typeof backend.then === 'function') {\n        const promiseId = ++this.pendingBackendInitId;\n        const success = backend.then(backendInstance => {\n          // Outdated promise. Another backend was set in the meantime.\n          if (promiseId < this.pendingBackendInitId) {\n            return false;\n          }\n          this.registry[backendName] = backendInstance;\n          this.pendingBackendInit = null;\n          return true;\n        }).catch(err => {\n          // Outdated promise. Another backend was set in the meantime.\n          if (promiseId < this.pendingBackendInitId) {\n            return false;\n          }\n          this.pendingBackendInit = null;\n          console.warn(`Initialization of backend ${backendName} failed`);\n          console.warn(err.stack || err.message);\n          return false;\n        });\n        this.pendingBackendInit = success;\n        return {\n          success,\n          asyncInit: true\n        };\n      } else {\n        this.registry[backendName] = backend;\n        return {\n          success: true,\n          asyncInit: false\n        };\n      }\n    } catch (err) {\n      console.warn(`Initialization of backend ${backendName} failed`);\n      console.warn(err.stack || err.message);\n      return {\n        success: false,\n        asyncInit: false\n      };\n    }\n  }\n  removeBackend(backendName) {\n    if (!(backendName in this.registryFactory)) {\n      throw new Error(`${backendName} backend not found in registry`);\n    }\n    if (this.backendName === backendName && this.pendingBackendInit != null) {\n      // There is a pending promise of the backend we want to remove. Make it\n      // obsolete.\n      this.pendingBackendInitId++;\n    }\n    if (backendName in this.registry) {\n      this.disposeRegisteredKernels(backendName);\n      this.registry[backendName].dispose();\n      delete this.registry[backendName];\n    }\n    delete this.registryFactory[backendName];\n    // Unset the backend if it is active.\n    if (this.backendName === backendName) {\n      this.pendingBackendInit = null;\n      this.backendName = null;\n      this.backendInstance = null;\n    }\n  }\n  getSortedBackends() {\n    if (Object.keys(this.registryFactory).length === 0) {\n      throw new Error('No backend found in registry.');\n    }\n    return Object.keys(this.registryFactory).sort((a, b) => {\n      // Highest priority comes first.\n      return this.registryFactory[b].priority - this.registryFactory[a].priority;\n    });\n  }\n  initializeBackendsAndReturnBest() {\n    const sortedBackends = this.getSortedBackends();\n    for (let i = 0; i < sortedBackends.length; i++) {\n      const backendName = sortedBackends[i];\n      const {\n        success,\n        asyncInit\n      } = this.initializeBackend(backendName);\n      if (asyncInit || success) {\n        return {\n          name: backendName,\n          asyncInit\n        };\n      }\n    }\n    throw new Error(`Could not initialize any backends, all backend initializations ` + `failed.`);\n  }\n  moveData(backend, dataId) {\n    const info = this.state.tensorInfo.get(dataId);\n    const srcBackend = info.backend;\n    const values = this.readSync(dataId);\n    // Delete the tensor from the old backend and move it to the new\n    // backend.\n    srcBackend.disposeData(dataId);\n    info.backend = backend;\n    backend.move(dataId, values, info.shape, info.dtype);\n    if (this.shouldCheckForMemLeaks()) {\n      // Track the number of moves during a kernel execution to correctly\n      // detect memory leaks.\n      this.state.numDataMovesStack[this.state.numDataMovesStack.length - 1]++;\n    }\n  }\n  tidy(nameOrFn, fn) {\n    let name = null;\n    if (fn == null) {\n      // Called with only 1 argument.\n      if (typeof nameOrFn !== 'function') {\n        throw new Error('Please provide a function to tidy()');\n      }\n      fn = nameOrFn;\n    } else {\n      // Called with 2 arguments.\n      if (typeof nameOrFn !== 'string' && !(nameOrFn instanceof String)) {\n        throw new Error('When calling with two arguments, the first argument ' + 'to tidy() must be a string');\n      }\n      if (typeof fn !== 'function') {\n        throw new Error('When calling with two arguments, the 2nd argument ' + 'to tidy() must be a function');\n      }\n      name = nameOrFn;\n      // TODO(nsthorat,smilkov): Do operation logging and performance\n      // profiling.\n    }\n\n    let result;\n    return this.scopedRun(() => this.startScope(name), () => this.endScope(result), () => {\n      result = fn();\n      if (result instanceof Promise) {\n        console.error('Cannot return a Promise inside of tidy.');\n      }\n      return result;\n    });\n  }\n  scopedRun(start, end, f) {\n    start();\n    try {\n      const res = f();\n      end();\n      return res;\n    } catch (ex) {\n      end();\n      throw ex;\n    }\n  }\n  nextTensorId() {\n    return Engine.nextTensorId++;\n  }\n  nextVariableId() {\n    return Engine.nextVariableId++;\n  }\n  /**\r\n   * This method is called instead of the public-facing tensor.clone() when\r\n   * saving a tensor for backwards pass. It makes sure to add the clone\r\n   * operation to the tape regardless of being called inside a kernel\r\n   * execution.\r\n   *\r\n   * This method will go away once all kernels are modularized since we won't\r\n   * need to turn off the tape inside runKernel().\r\n   */\n  clone(x) {\n    const y = this.makeTensorFromDataId(x.dataId, x.shape, x.dtype);\n    const inputs = {\n      x\n    };\n    const grad = dy => ({\n      x: () => {\n        const dtype = 'float32';\n        const gradInputs = {\n          x: dy\n        };\n        const attrs = {\n          dtype\n        };\n        return ENGINE.runKernelFunc(backend => backend.cast(dy, dtype), gradInputs, null /* grad */, Cast, attrs);\n      }\n    });\n    const saved = [];\n    this.addTapeNode(this.state.activeScope.name, inputs, [y], grad, saved, {});\n    return y;\n  }\n  /**\r\n   * Execute a kernel with the given name and return the output tensor.\r\n   *\r\n   * @param kernelName The name of the kernel to execute.\r\n   * @param inputs A map of input names to tensors.\r\n   * @param attrs A map of attribute names to their values. An attribute is a\r\n   *     primitive (non-tensor) input to the kernel.\r\n   * @param inputsToSave A list of tensors, inputs to save for the backprop\r\n   *     computation.\r\n   * @param outputsToSave A list of booleans, specifying which output to save\r\n   *     for the backprop computation. These are booleans since the output\r\n   * tensors are not visible to the user.\r\n   */\n  runKernel(kernelName, inputs, attrs, inputsToSave, outputsToSave) {\n    const forwardFunc = null;\n    const backwardsFunc = null;\n    // Call runKernel as a stop-gap until we modularize all kernels.\n    // Once we modularize all kernels, we will remove the existing\n    // `runKernelFunc`.\n    return this.runKernelFunc(forwardFunc, inputs, backwardsFunc, kernelName, attrs, inputsToSave, outputsToSave);\n  }\n  shouldCheckForMemLeaks() {\n    return this.ENV.getBool('IS_TEST');\n  }\n  checkKernelForMemLeak(kernelName, numDataIdsBefore, outInfos) {\n    const numDataIdsAfter = this.backend.numDataIds();\n    // Count the number of data ids associated with the result of the kernel.\n    let numOutputDataIds = 0;\n    outInfos.forEach(info => {\n      // Complex numbers allocate 3 data ids, one for 'real', one for\n      // 'imaginary', and one for the container that holds the former two.\n      numOutputDataIds += info.dtype === 'complex64' ? 3 : 1;\n    });\n    // Account for the number of moves during kernel execution. A \"data move\"\n    // can happen in the middle of a kernel execution, placing a new (key,value)\n    // pair in the data storage. Since data moves have net zero effect (we\n    // always remove the data from the old backend), we have to cancel them out\n    // when detecting memory leaks.\n    const numMoves = this.state.numDataMovesStack[this.state.numDataMovesStack.length - 1];\n    const dataIdsLeaked = numDataIdsAfter - numDataIdsBefore - numOutputDataIds - numMoves;\n    if (dataIdsLeaked > 0) {\n      throw new Error(`Backend '${this.backendName}' has an internal memory leak ` + `(${dataIdsLeaked} data ids) after running '${kernelName}'`);\n    }\n  }\n  /**\r\n   * @deprecated Use `runKernel` for newly added kernels. Keep using this method\r\n   *     only for kernels that are not yet fully modularized.\r\n   */\n  runKernelFunc(forwardFunc, inputs, backwardsFunc, kernelName, attrs, inputsToSave, outputsToSave) {\n    let outputs;\n    let saved = [];\n    const isTapeOn = this.isTapeOn();\n    if (kernelName == null) {\n      kernelName = this.state.activeScope != null ? this.state.activeScope.name : '';\n    }\n    const startingBytecount = this.state.numBytes;\n    const startingNumTensors = this.state.numTensors;\n    if (this.shouldCheckForMemLeaks()) {\n      this.state.numDataMovesStack.push(0);\n    }\n    let kernelFunc;\n    if (this.backendName == null) {\n      // backend has not been initialized yet (backend initialization is lazy\n      // can be deferred until an op/ kernel is run).\n      // The below getter has side effects that will try to initialize the\n      // backend and set properties like this.backendName\n      // tslint:disable-next-line: no-unused-expression\n      this.backend;\n    }\n    const kernel = getKernel(kernelName, this.backendName);\n    let out;\n    if (kernel != null) {\n      kernelFunc = () => {\n        const numDataIdsBefore = this.backend.numDataIds();\n        out = kernel.kernelFunc({\n          inputs,\n          attrs,\n          backend: this.backend\n        });\n        const outInfos = Array.isArray(out) ? out : [out];\n        if (this.shouldCheckForMemLeaks()) {\n          this.checkKernelForMemLeak(kernelName, numDataIdsBefore, outInfos);\n        }\n        const outTensors = outInfos.map(outInfo => {\n          // todo (yassogba) remove this option (Tensor) when node backend\n          // methods have been modularized and they all return tensorInfo.\n          // TensorInfos do not have a rank attribute.\n          if (outInfo.rank != null) {\n            return outInfo;\n          }\n          const {\n            dataId,\n            shape,\n            dtype\n          } = outInfo;\n          return this.makeTensorFromDataId(dataId, shape, dtype);\n        });\n        // Save the inputs and outputs.\n        // Do not save unless we are recording to the tape. Otherwise it would\n        // cause a mem leak since we would never run backprop, which disposes\n        // the kept tensors.\n        if (isTapeOn) {\n          let tensorsToSave = this.getTensorsForGradient(kernelName, inputs, outTensors);\n          if (tensorsToSave == null) {\n            // Fallback for ops that call runKernelFunc and pass in\n            // inputsToSave and outputsToSave. Currently this is the set of ops\n            // with kernel support in the WASM backend. Once those ops and\n            // respective gradients are modularised we can remove this path.\n            if (outputsToSave == null) {\n              outputsToSave = [];\n            }\n            const outsToSave = outTensors.filter((_, i) => outputsToSave[i]);\n            tensorsToSave = (inputsToSave || []).slice().concat(outsToSave);\n          }\n          saved = this.saveTensorsForBackwardMode(tensorsToSave);\n        }\n        return outTensors;\n      };\n    } else {\n      if (forwardFunc == null) {\n        throw new Error(`Error running ${kernelName}: Neither modular kernel nor forward func passed`);\n      }\n      const saveFunc = tensors => {\n        // Do not save unless we are recording to the tape. Otherwise it would\n        // cause a mem leak since we would never run backprop, which disposes\n        // the kept tensors.\n        if (!isTapeOn) {\n          return;\n        }\n        saved = tensors.map(tensor => this.keep(this.clone(tensor)));\n      };\n      kernelFunc = () => {\n        const numDataIdsBefore = this.backend.numDataIds();\n        out = this.tidy(() => forwardFunc(this.backend, saveFunc));\n        const outs = Array.isArray(out) ? out : [out];\n        if (this.shouldCheckForMemLeaks()) {\n          this.checkKernelForMemLeak(kernelName, numDataIdsBefore, outs);\n        }\n        return outs;\n      };\n    }\n    // Stop recording to a tape when running a kernel.\n    let kernelProfile;\n    this.scopedRun(() => this.state.kernelDepth++, () => this.state.kernelDepth--, () => {\n      if (!this.ENV.getBool('DEBUG') && !this.state.profiling) {\n        outputs = kernelFunc();\n      } else {\n        kernelProfile = this.profiler.profileKernel(kernelName, inputs, () => kernelFunc());\n        if (this.ENV.getBool('DEBUG')) {\n          this.profiler.logKernelProfile(kernelProfile);\n        }\n        outputs = kernelProfile.outputs;\n      }\n    });\n    if (isTapeOn) {\n      this.addTapeNode(kernelName, inputs, outputs, backwardsFunc, saved, attrs);\n    }\n    if (this.state.profiling) {\n      this.state.activeProfile.kernels.push({\n        name: kernelName,\n        bytesAdded: this.state.numBytes - startingBytecount,\n        totalBytesSnapshot: this.state.numBytes,\n        tensorsAdded: this.state.numTensors - startingNumTensors,\n        totalTensorsSnapshot: this.state.numTensors,\n        inputShapes: Object.keys(inputs).map(key => inputs[key] != null ? inputs[key].shape : null),\n        outputShapes: outputs.map(item => item.shape),\n        kernelTimeMs: kernelProfile.timeMs,\n        extraInfo: kernelProfile.extraInfo\n      });\n    }\n    return Array.isArray(out) ? outputs : outputs[0];\n  }\n  /**\r\n   * Saves tensors used in forward mode for use in backward mode.\r\n   *\r\n   * @param tensors the list of tensors to save.\r\n   */\n  saveTensorsForBackwardMode(tensors) {\n    const saved = tensors.map(tensor => this.keep(this.clone(tensor)));\n    return saved;\n  }\n  /**\r\n   * Returns a list of tensors to save for a given gradient calculation.\r\n   *\r\n   * Returns undefined if their is no registered gradient for this kernel in the\r\n   * gradient registry.\r\n   *\r\n   * @param kernelName name of kernel to look up gradient for.\r\n   * @param inputs a map of input tensors.\r\n   * @param outputs an array of output tensors from forward mode of kernel.\r\n   */\n  getTensorsForGradient(kernelName, inputs, outputs) {\n    const gradConfig = getGradient(kernelName);\n    if (gradConfig != null) {\n      const inputsToSave = gradConfig.inputsToSave || [];\n      const outputsToSave = gradConfig.outputsToSave || [];\n      // If saveAllInputs is true, all inputs will be saved. Otherwise, inputs\n      // specified in inputsToSave will be saved.\n      let inputTensorsToSave;\n      if (gradConfig.saveAllInputs) {\n        util.assert(Array.isArray(inputs), () => 'saveAllInputs is true, expected inputs to be an array.');\n        inputTensorsToSave = Object.keys(inputs).map(key => inputs[key]);\n      } else {\n        inputTensorsToSave = inputsToSave.map(inputName => inputs[inputName]);\n      }\n      const outputTensorsToSave = outputs.filter((_, i) => outputsToSave[i]);\n      return inputTensorsToSave.concat(outputTensorsToSave);\n    }\n    // TODO(yassogba) throw exception here once all runkernelFunc calls with\n    // inputsToSave/outputsToSave are removed\n    return null;\n  }\n  /**\r\n   * Internal method used by public APIs for tensor creation. Makes a new\r\n   * tensor with the provided shape, dtype and values. It always\r\n   * creates a new data id and writes the values to the underlying backend.\r\n   */\n  makeTensor(values, shape, dtype, backend) {\n    if (values == null) {\n      throw new Error('Values passed to engine.makeTensor() are null');\n    }\n    dtype = dtype || 'float32';\n    backend = backend || this.backend;\n    let backendVals = values;\n    if (dtype === 'string' && util.isString(values[0])) {\n      backendVals = values.map(d => util.encodeString(d));\n    }\n    const dataId = backend.write(backendVals, shape, dtype);\n    const t = new Tensor(shape, dtype, dataId, this.nextTensorId());\n    this.incRef(t, backend);\n    // Count bytes for string tensors.\n    if (dtype === 'string') {\n      const info = this.state.tensorInfo.get(dataId);\n      const newBytes = bytesFromStringArray(backendVals);\n      this.state.numBytes += newBytes - info.bytes;\n      info.bytes = newBytes;\n    }\n    return t;\n  }\n  /**\r\n   * Internal method used by backends. Makes a new tensor\r\n   * that is a wrapper around an existing data id. It doesn't create\r\n   * a new data id, only increments the ref count used in memory tracking.\r\n   */\n  makeTensorFromDataId(dataId, shape, dtype, backend) {\n    dtype = dtype || 'float32';\n    const t = new Tensor(shape, dtype, dataId, this.nextTensorId());\n    this.incRef(t, backend);\n    return t;\n  }\n  makeVariable(initialValue, trainable = true, name, dtype) {\n    name = name || this.nextVariableId().toString();\n    if (dtype != null && dtype !== initialValue.dtype) {\n      initialValue = initialValue.cast(dtype);\n    }\n    const v = new Variable(initialValue, trainable, name, this.nextTensorId());\n    if (this.state.registeredVariables[v.name] != null) {\n      throw new Error(`Variable with name ${v.name} was already registered`);\n    }\n    this.state.registeredVariables[v.name] = v;\n    this.incRef(v, this.backend);\n    return v;\n  }\n  incRef(a, backend) {\n    const refCount = this.state.tensorInfo.has(a.dataId) ? this.state.tensorInfo.get(a.dataId).refCount : 0;\n    this.state.numTensors++;\n    if (a.dtype === 'string') {\n      this.state.numStringTensors++;\n    }\n    if (refCount === 0) {\n      this.state.numDataBuffers++;\n      // Bytes for complex numbers are counted by their components. Bytes for\n      // string tensors are counted when writing values.\n      let bytes = 0;\n      if (a.dtype !== 'complex64' && a.dtype !== 'string') {\n        bytes = a.size * util.bytesPerElement(a.dtype);\n      }\n      this.state.tensorInfo.set(a.dataId, {\n        backend: backend || this.backend,\n        dtype: a.dtype,\n        shape: a.shape,\n        bytes,\n        refCount: 0\n      });\n      this.state.numBytes += bytes;\n    }\n    this.state.tensorInfo.get(a.dataId).refCount++;\n    if (!(a instanceof Variable)) {\n      this.track(a);\n    }\n  }\n  disposeTensor(a) {\n    if (!this.state.tensorInfo.has(a.dataId)) {\n      return;\n    }\n    this.state.numTensors--;\n    if (a.dtype === 'string') {\n      this.state.numStringTensors--;\n    }\n    const info = this.state.tensorInfo.get(a.dataId);\n    const refCount = info.refCount;\n    if (refCount <= 1) {\n      // Don't count bytes for complex numbers as they are counted by their\n      // components.\n      if (a.dtype !== 'complex64') {\n        this.state.numBytes -= info.bytes;\n      }\n      this.state.numDataBuffers--;\n      info.backend.disposeData(a.dataId);\n      this.state.tensorInfo.delete(a.dataId);\n    } else {\n      // Notify the backend to descrease the ref count for complex tensor\n      // components. This method is only implemented in WebGL right now. When\n      // there are multiple references, complex tensor cannot dispose the\n      // components if ref count is not in sync with engine.\n      info.backend.decComplexRef(a.dataId);\n      this.state.tensorInfo.get(a.dataId).refCount--;\n    }\n    // TODO(nsthorat): Construct an error and save the stack trace for\n    // debugging when in debug mode. Creating a stack trace is too expensive\n    // to do unconditionally.\n  }\n\n  disposeVariables() {\n    for (const varName in this.state.registeredVariables) {\n      const v = this.state.registeredVariables[varName];\n      this.disposeVariable(v);\n    }\n  }\n  disposeVariable(v) {\n    this.disposeTensor(v);\n    if (this.state.registeredVariables[v.name] != null) {\n      delete this.state.registeredVariables[v.name];\n    }\n  }\n  memory() {\n    const info = this.backend.memory();\n    info.numTensors = this.state.numTensors;\n    info.numDataBuffers = this.state.numDataBuffers;\n    info.numBytes = this.state.numBytes;\n    if (this.state.numStringTensors > 0) {\n      info.unreliable = true;\n      if (info.reasons == null) {\n        info.reasons = [];\n      }\n      info.reasons.push('Memory usage by string tensors is approximate ' + '(2 bytes per character)');\n    }\n    return info;\n  }\n  async profile(query) {\n    this.state.profiling = true;\n    const startBytes = this.state.numBytes;\n    const startNumTensors = this.state.numTensors;\n    this.state.activeProfile.kernels = [];\n    this.state.activeProfile.result = await query();\n    this.state.profiling = false;\n    this.state.activeProfile.peakBytes = Math.max(...this.state.activeProfile.kernels.map(d => d.totalBytesSnapshot));\n    this.state.activeProfile.newBytes = this.state.numBytes - startBytes;\n    this.state.activeProfile.newTensors = this.state.numTensors - startNumTensors;\n    for (const kernel of this.state.activeProfile.kernels) {\n      kernel.kernelTimeMs = await kernel.kernelTimeMs;\n      kernel.extraInfo = await kernel.extraInfo;\n    }\n    return this.state.activeProfile;\n  }\n  isTapeOn() {\n    return this.state.gradientDepth > 0 && this.state.kernelDepth === 0;\n  }\n  addTapeNode(kernelName, inputs, outputs, gradientsFunc, saved, attrs) {\n    const tapeNode = {\n      id: this.state.nextTapeNodeId++,\n      kernelName,\n      inputs,\n      outputs,\n      saved\n    };\n    const gradConfig = getGradient(kernelName);\n    if (gradConfig != null) {\n      gradientsFunc = gradConfig.gradFunc;\n    }\n    if (gradientsFunc != null) {\n      tapeNode.gradient = dys => {\n        // TODO(smilkov): To optimize back-prop, pass dys that are not used in\n        // the backprop graph to the user as null instead of zeros\n        dys = dys.map((dy, i) => {\n          if (dy == null) {\n            const output = outputs[i];\n            const vals = util.makeZerosTypedArray(output.size, output.dtype);\n            return this.makeTensor(vals, output.shape, output.dtype);\n          }\n          return dy;\n        });\n        // Grad functions of ops with single outputs expect a dy, while ops\n        // with multiple outputs expect dys (array of dy).\n        return gradientsFunc(dys.length > 1 ? dys : dys[0], saved, attrs);\n      };\n    }\n    this.state.activeTape.push(tapeNode);\n  }\n  keep(result) {\n    result.kept = true;\n    return result;\n  }\n  startTape() {\n    if (this.state.gradientDepth === 0) {\n      this.state.activeTape = [];\n    }\n    this.state.gradientDepth++;\n  }\n  endTape() {\n    this.state.gradientDepth--;\n  }\n  /**\r\n   * Start a scope. Use this with endScope() to achieve the same functionality\r\n   * as scope() without the need for a function closure.\r\n   */\n  startScope(name) {\n    const scopeInfo = {\n      track: [],\n      name: 'unnamed scope',\n      id: this.state.nextScopeId++\n    };\n    if (name) {\n      scopeInfo.name = name;\n    }\n    this.state.scopeStack.push(scopeInfo);\n    this.state.activeScope = scopeInfo;\n  }\n  /**\r\n   * End a scope. Use this with startScope() to achieve the same functionality\r\n   * as scope() without the need for a function closure.\r\n   */\n  endScope(result) {\n    const tensorsToTrackInParent = getTensorsInContainer(result);\n    const tensorsToTrackInParentSet = new Set(tensorsToTrackInParent.map(t => t.id));\n    // Dispose the arrays tracked in this scope.\n    for (let i = 0; i < this.state.activeScope.track.length; i++) {\n      const tensor = this.state.activeScope.track[i];\n      if (!tensor.kept && !tensorsToTrackInParentSet.has(tensor.id)) {\n        tensor.dispose();\n      }\n    }\n    const oldScope = this.state.scopeStack.pop();\n    this.state.activeScope = this.state.scopeStack.length === 0 ? null : this.state.scopeStack[this.state.scopeStack.length - 1];\n    // Track the current result in the parent scope.\n    tensorsToTrackInParent.forEach(tensor => {\n      // Only track the tensor if was allocated in the inner scope and is not\n      // globally kept.\n      if (!tensor.kept && tensor.scopeId === oldScope.id) {\n        this.track(tensor);\n      }\n    });\n  }\n  /**\r\n   * Returns gradients of `f` with respect to each of the `xs`. The gradients\r\n   * returned are of the same length as `xs`, but some might be null if `f`\r\n   * was not a function of that `x`. It also takes optional dy to multiply the\r\n   * gradient, which defaults to `1`.\r\n   */\n  gradients(f, xs, dy, allowNoGradients = false) {\n    util.assert(xs.length > 0, () => 'gradients() received an empty list of xs.');\n    if (dy != null && dy.dtype !== 'float32') {\n      throw new Error(`dy must have 'float32' dtype, but has '${dy.dtype}'`);\n    }\n    const y = this.scopedRun(() => this.startTape(), () => this.endTape(), () => this.tidy('forward', f));\n    util.assert(y instanceof Tensor, () => 'The result y returned by f() must be a tensor.');\n    // Filter out the nodes that don't connect x => y.\n    const filteredTape = getFilteredNodesXToY(this.state.activeTape, xs, y);\n    if (!allowNoGradients && filteredTape.length === 0 && xs.length > 0) {\n      throw new Error('Cannot compute gradient of y=f(x) with respect to x. Make sure ' + 'that the f you passed encloses all operations that lead from x ' + 'to y.');\n    }\n    return this.tidy('backward', () => {\n      const accumulatedGradientMap = {};\n      accumulatedGradientMap[y.id] = dy == null ? ones(y.shape) : dy;\n      // Backprop gradients through the filtered nodes.\n      backpropagateGradients(accumulatedGradientMap, filteredTape,\n      // Pass the tidy function to avoid circular dep with `tape.ts`.\n      f => this.tidy(f),\n      // Pass an add function to avoide a circular dep with `tape.ts`.\n      add);\n      const grads = xs.map(x => accumulatedGradientMap[x.id]);\n      if (this.state.gradientDepth === 0) {\n        // This means that we are not computing higher-order gradients\n        // and can clean up the tape.\n        this.state.activeTape.forEach(node => {\n          for (const tensor of node.saved) {\n            tensor.dispose();\n          }\n        });\n        this.state.activeTape = null;\n      }\n      return {\n        value: y,\n        grads\n      };\n    });\n  }\n  customGrad(f) {\n    util.assert(util.isFunction(f), () => 'The f passed in customGrad(f) must be a function.');\n    return (...inputs) => {\n      util.assert(inputs.every(t => t instanceof Tensor), () => 'The args passed in customGrad(f)(x1, x2,...) must all be ' + 'tensors');\n      let res;\n      const inputMap = {};\n      inputs.forEach((input, i) => {\n        inputMap[i] = input;\n      });\n      return this.runKernelFunc((_, save) => {\n        res = f(...[...inputs, save]);\n        util.assert(res.value instanceof Tensor, () => 'The function f passed in customGrad(f) must return an ' + 'object where `obj.value` is a tensor');\n        util.assert(util.isFunction(res.gradFunc), () => 'The function f passed in customGrad(f) must return an ' + 'object where `obj.gradFunc` is a function.');\n        return res.value;\n      }, inputMap, (dy, saved) => {\n        const gradRes = res.gradFunc(dy, saved);\n        const grads = Array.isArray(gradRes) ? gradRes : [gradRes];\n        util.assert(grads.length === inputs.length, () => 'The function f passed in customGrad(f) must return an ' + 'object where `obj.gradFunc` is a function that returns ' + 'the same number of tensors as inputs passed to f(...).');\n        util.assert(grads.every(t => t instanceof Tensor), () => 'The function f passed in customGrad(f) must return an ' + 'object where `obj.gradFunc` is a function that returns ' + 'a list of only tensors.');\n        const gradMap = {};\n        grads.forEach((grad, i) => {\n          gradMap[i] = () => grad;\n        });\n        return gradMap;\n      });\n    };\n  }\n  readSync(dataId) {\n    // Route the read to the correct backend.\n    const info = this.state.tensorInfo.get(dataId);\n    return info.backend.readSync(dataId);\n  }\n  read(dataId) {\n    // Route the read to the correct backend.\n    const info = this.state.tensorInfo.get(dataId);\n    return info.backend.read(dataId);\n  }\n  async time(query) {\n    const start = now();\n    const timingInfo = await this.backend.time(query);\n    timingInfo.wallMs = now() - start;\n    return timingInfo;\n  }\n  /**\r\n   * Tracks a Tensor in the current scope to be automatically cleaned up\r\n   * when the current scope ends, and returns the value.\r\n   *\r\n   * @param result The Tensor to track in the current scope.\r\n   */\n  track(result) {\n    if (this.state.activeScope != null) {\n      result.scopeId = this.state.activeScope.id;\n      this.state.activeScope.track.push(result);\n    }\n    return result;\n  }\n  get registeredVariables() {\n    return this.state.registeredVariables;\n  }\n  /**\r\n   * Resets the engine state. Removes all backends but does not remove\r\n   * registered backend factories.\r\n   */\n  reset() {\n    // Make any pending promise obsolete.\n    this.pendingBackendInitId++;\n    this.state.dispose();\n    this.ENV.reset();\n    this.state = new EngineState();\n    for (const backendName in this.registry) {\n      this.disposeRegisteredKernels(backendName);\n      this.registry[backendName].dispose();\n      delete this.registry[backendName];\n    }\n    this.backendName = null;\n    this.backendInstance = null;\n    this.pendingBackendInit = null;\n  }\n}\nEngine.nextTensorId = 0;\nEngine.nextVariableId = 0;\nfunction ones(shape) {\n  const values = makeOnesTypedArray(sizeFromShape(shape), 'float32');\n  return ENGINE.makeTensor(values, shape, 'float32');\n}\nexport function getOrMakeEngine() {\n  const ns = getGlobalNamespace();\n  if (ns._tfengine == null) {\n    const environment = new Environment(ns);\n    ns._tfengine = new Engine(environment);\n  }\n  setEnvironmentGlobal(ns._tfengine.ENV);\n  // Tell the current tensor interface that the global engine is responsible\n  // for tracking.\n  setTensorTracker(() => ns._tfengine);\n  return ns._tfengine;\n}\nexport const ENGINE = getOrMakeEngine();\n/**\r\n * A implementation of the add op for use within engine and tape.\r\n *\r\n * This allows us to avoid a circular dependency between add.ts and engine.\r\n * It is exported to be available in tape tests.\r\n */\nexport function add(a, b) {\n  // We duplicate Add here to avoid a circular dependency with add.ts.\n  const inputs = {\n    a,\n    b\n  };\n  return ENGINE.runKernel(Add, inputs);\n}","map":null,"metadata":{},"sourceType":"module"}