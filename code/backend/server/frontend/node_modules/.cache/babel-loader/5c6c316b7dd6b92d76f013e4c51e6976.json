{"ast":null,"code":"/**\r\n * @license\r\n * Copyright 2018 Google LLC\r\n *\r\n * Use of this source code is governed by an MIT-style\r\n * license that can be found in the LICENSE file or at\r\n * https://opensource.org/licenses/MIT.\r\n * =============================================================================\r\n */\n// tslint:disable-next-line:max-line-length\nimport { Constant, GlorotNormal, GlorotUniform, HeNormal, HeUniform, Identity, LeCunNormal, LeCunUniform, Ones, Orthogonal, RandomNormal, RandomUniform, TruncatedNormal, VarianceScaling, Zeros } from './initializers';\n/**\r\n * Initializer that generates tensors initialized to 0.\r\n *\r\n * @doc {heading: 'Initializers', namespace: 'initializers'}\r\n */\nexport function zeros() {\n  return new Zeros();\n}\n/**\r\n * Initializer that generates tensors initialized to 1.\r\n *\r\n * @doc {heading: 'Initializers', namespace: 'initializers'}\r\n */\nexport function ones() {\n  return new Ones();\n}\n/**\r\n * Initializer that generates values initialized to some constant.\r\n *\r\n * @doc {heading: 'Initializers', namespace: 'initializers'}\r\n */\nexport function constant(args) {\n  return new Constant(args);\n}\n/**\r\n * Initializer that generates random values initialized to a uniform\r\n * distribution.\r\n *\r\n * Values will be distributed uniformly between the configured minval and\r\n * maxval.\r\n *\r\n * @doc {heading: 'Initializers', namespace: 'initializers'}\r\n */\nexport function randomUniform(args) {\n  return new RandomUniform(args);\n}\n/**\r\n * Initializer that generates random values initialized to a normal\r\n * distribution.\r\n *\r\n * @doc {heading: 'Initializers', namespace: 'initializers'}\r\n */\nexport function randomNormal(args) {\n  return new RandomNormal(args);\n}\n/**\r\n * Initializer that generates random values initialized to a truncated normal.\r\n * distribution.\r\n *\r\n * These values are similar to values from a `RandomNormal` except that values\r\n * more than two standard deviations from the mean are discarded and re-drawn.\r\n * This is the recommended initializer for neural network weights and filters.\r\n *\r\n * @doc {heading: 'Initializers', namespace: 'initializers'}\r\n */\nexport function truncatedNormal(args) {\n  return new TruncatedNormal(args);\n}\n/**\r\n * Initializer that generates the identity matrix.\r\n * Only use for square 2D matrices.\r\n *\r\n * @doc {heading: 'Initializers', namespace: 'initializers'}\r\n */\nexport function identity(args) {\n  return new Identity(args);\n}\n/**\r\n * Initializer capable of adapting its scale to the shape of weights.\r\n * With distribution=NORMAL, samples are drawn from a truncated normal\r\n * distribution centered on zero, with `stddev = sqrt(scale / n)` where n is:\r\n *   - number of input units in the weight tensor, if mode = FAN_IN.\r\n *   - number of output units, if mode = FAN_OUT.\r\n *   - average of the numbers of input and output units, if mode = FAN_AVG.\r\n * With distribution=UNIFORM,\r\n * samples are drawn from a uniform distribution\r\n * within [-limit, limit], with `limit = sqrt(3 * scale / n)`.\r\n *\r\n * @doc {heading: 'Initializers',namespace: 'initializers'}\r\n */\nexport function varianceScaling(config) {\n  return new VarianceScaling(config);\n}\n/**\r\n * Glorot uniform initializer, also called Xavier uniform initializer.\r\n * It draws samples from a uniform distribution within [-limit, limit]\r\n * where `limit` is `sqrt(6 / (fan_in + fan_out))`\r\n * where `fan_in` is the number of input units in the weight tensor\r\n * and `fan_out` is the number of output units in the weight tensor\r\n *\r\n * Reference:\r\n *   Glorot & Bengio, AISTATS 2010\r\n *       http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf.\r\n *\r\n * @doc {heading: 'Initializers', namespace: 'initializers'}\r\n */\nexport function glorotUniform(args) {\n  return new GlorotUniform(args);\n}\n/**\r\n * Glorot normal initializer, also called Xavier normal initializer.\r\n * It draws samples from a truncated normal distribution centered on 0\r\n * with `stddev = sqrt(2 / (fan_in + fan_out))`\r\n * where `fan_in` is the number of input units in the weight tensor\r\n * and `fan_out` is the number of output units in the weight tensor.\r\n *\r\n * Reference:\r\n *   Glorot & Bengio, AISTATS 2010\r\n *       http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf\r\n *\r\n * @doc {heading: 'Initializers', namespace: 'initializers'}\r\n */\nexport function glorotNormal(args) {\n  return new GlorotNormal(args);\n}\n/**\r\n * He normal initializer.\r\n *\r\n * It draws samples from a truncated normal distribution centered on 0\r\n * with `stddev = sqrt(2 / fanIn)`\r\n * where `fanIn` is the number of input units in the weight tensor.\r\n *\r\n * Reference:\r\n *     He et al., http://arxiv.org/abs/1502.01852\r\n *\r\n * @doc {heading: 'Initializers', namespace: 'initializers'}\r\n */\nexport function heNormal(args) {\n  return new HeNormal(args);\n}\n/**\r\n * He uniform initializer.\r\n *\r\n * It draws samples from a uniform distribution within [-limit, limit]\r\n * where `limit` is `sqrt(6 / fan_in)`\r\n * where `fanIn` is the number of input units in the weight tensor.\r\n *\r\n * Reference:\r\n *     He et al., http://arxiv.org/abs/1502.01852\r\n *\r\n * @doc {heading: 'Initializers',namespace: 'initializers'}\r\n */\nexport function heUniform(args) {\n  return new HeUniform(args);\n}\n/**\r\n * LeCun normal initializer.\r\n *\r\n * It draws samples from a truncated normal distribution centered on 0\r\n * with `stddev = sqrt(1 / fanIn)`\r\n * where `fanIn` is the number of input units in the weight tensor.\r\n *\r\n * References:\r\n *   [Self-Normalizing Neural Networks](https://arxiv.org/abs/1706.02515)\r\n *   [Efficient Backprop](http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf)\r\n *\r\n * @doc {heading: 'Initializers', namespace: 'initializers'}\r\n */\nexport function leCunNormal(args) {\n  return new LeCunNormal(args);\n}\n/**\r\n * LeCun uniform initializer.\r\n *\r\n * It draws samples from a uniform distribution in the interval\r\n * `[-limit, limit]` with `limit = sqrt(3 / fanIn)`,\r\n * where `fanIn` is the number of input units in the weight tensor.\r\n *\r\n * @doc {heading: 'Initializers', namespace: 'initializers'}\r\n */\nexport function leCunUniform(args) {\n  return new LeCunUniform(args);\n}\n/**\r\n * Initializer that generates a random orthogonal matrix.\r\n *\r\n * Reference:\r\n * [Saxe et al., http://arxiv.org/abs/1312.6120](http://arxiv.org/abs/1312.6120)\r\n *\r\n * @doc {heading: 'Initializers', namespace: 'initializers'}\r\n */\nexport function orthogonal(args) {\n  return new Orthogonal(args);\n}","map":null,"metadata":{},"sourceType":"module"}