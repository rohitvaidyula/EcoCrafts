{"ast":null,"code":"/**\r\n * @license\r\n * Copyright 2018 Google LLC. All Rights Reserved.\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n * http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n *\r\n * =============================================================================\r\n */\nimport * as tf from '@tensorflow/tfjs-core';\nimport * as seedrandom from 'seedrandom';\nimport { deepClone } from '../util/deep_clone';\nimport { deepMapAndAwaitAll, deepZip, zipToList } from '../util/deep_map';\nimport { GrowingRingBuffer } from '../util/growing_ring_buffer';\nimport { RingBuffer } from '../util/ring_buffer';\n// Here we implement a simple asynchronous iterator.\n// This lets us avoid using either third-party stream libraries or\n// recent TypeScript language support requiring polyfills.\n/**\r\n * Create a `LazyIterator` from an array of items.\r\n */\nexport function iteratorFromItems(items) {\n  return new ArrayIterator(items);\n}\n/**\r\n * Create a `LazyIterator` of incrementing integers.\r\n */\nexport function iteratorFromIncrementing(start) {\n  let i = start;\n  return iteratorFromFunction(() => ({\n    value: i++,\n    done: false\n  }));\n}\n/**\r\n * Create a `LazyIterator` from a function.\r\n *\r\n * ```js\r\n * let i = -1;\r\n * const func = () =>\r\n *    ++i < 5 ? {value: i, done: false} : {value: null, done: true};\r\n * const iter = tf.data.iteratorFromFunction(func);\r\n * await iter.forEachAsync(e => console.log(e));\r\n * ```\r\n *\r\n * @param func A function that produces data on each call.\r\n */\nexport function iteratorFromFunction(func) {\n  return new FunctionCallIterator(func);\n}\n/**\r\n * Create a `LazyIterator` by concatenating underlying streams, which are\r\n * themselves provided as a stream.\r\n *\r\n * This can also be thought of as a \"stream flatten\" operation.\r\n *\r\n * @param baseIterators A stream of streams to be concatenated.\r\n * @param baseErrorHandler An optional function that can intercept `Error`s\r\n *   raised during a `next()` call on the base stream.  This function can decide\r\n *   whether the error should be propagated, whether the error should be\r\n *   ignored, or whether the base stream should be terminated.\r\n */\nexport function iteratorFromConcatenated(baseIterators, baseErrorHandler) {\n  return new ChainedIterator(baseIterators, baseErrorHandler);\n}\n/**\r\n * Create a `LazyIterator` by concatenating streams produced by calling a\r\n * stream-generating function a given number of times.\r\n *\r\n * Since a `LazyIterator` is read-once, it cannot be repeated, but this\r\n * function can be used to achieve a similar effect:\r\n *\r\n *   LazyIterator.ofConcatenatedFunction(() => new MyIterator(), 6);\r\n *\r\n * @param iteratorFunc: A function that produces a new stream on each call.\r\n * @param count: The number of times to call the function.\r\n * @param baseErrorHandler An optional function that can intercept `Error`s\r\n *   raised during a `next()` call on the base stream.  This function can decide\r\n *   whether the error should be propagated, whether the error should be\r\n *   ignored, or whether the base stream should be terminated.\r\n */\nexport function iteratorFromConcatenatedFunction(iteratorFunc, count, baseErrorHandler) {\n  return iteratorFromConcatenated(iteratorFromFunction(iteratorFunc).take(count), baseErrorHandler);\n}\n/**\r\n * Create a `LazyIterator` by zipping together an array, dict, or nested\r\n * structure of `LazyIterator`s (and perhaps additional constants).\r\n *\r\n * The underlying streams must provide elements in a consistent order such\r\n * that they correspond.\r\n *\r\n * Typically, the underlying streams should have the same number of\r\n * elements. If they do not, the behavior is determined by the\r\n * `mismatchMode` argument.\r\n *\r\n * The nested structure of the `iterators` argument determines the\r\n * structure of elements in the resulting iterator.\r\n *\r\n * @param iterators: An array or object containing LazyIterators at the\r\n * leaves.\r\n * @param mismatchMode: Determines what to do when one underlying iterator\r\n * is exhausted before the others.  `ZipMismatchMode.FAIL` (the default)\r\n * causes an error to be thrown in this case.  `ZipMismatchMode.SHORTEST`\r\n * causes the zipped iterator to terminate with the furst underlying\r\n * streams, so elements remaining on the longer streams are ignored.\r\n * `ZipMismatchMode.LONGEST` causes the zipped stream to continue, filling\r\n * in nulls for the exhausted streams, until all streams are exhausted.\r\n */\nexport function iteratorFromZipped(iterators, mismatchMode = ZipMismatchMode.FAIL) {\n  return new ZipIterator(iterators, mismatchMode);\n}\n/**\r\n * An asynchronous iterator, providing lazy access to a potentially\r\n * unbounded stream of elements.\r\n *\r\n * Iterator can be obtained from a dataset:\r\n * `const iter = await dataset.iterator();`\r\n */\nexport class LazyIterator {\n  /**\r\n   * Collect all remaining elements of a bounded stream into an array.\r\n   * Obviously this will succeed only for small streams that fit in memory.\r\n   * Useful for testing.\r\n   *\r\n   * @returns A Promise for an array of stream elements, which will resolve\r\n   *   when the stream is exhausted.\r\n   */\n  async toArray() {\n    const result = [];\n    let x = await this.next();\n    while (!x.done) {\n      result.push(x.value);\n      x = await this.next();\n    }\n    return result;\n  }\n  /**\r\n   * Collect all elements of this dataset into an array with prefetching 100\r\n   * elements. This is useful for testing, because the prefetch changes the\r\n   * order in which the Promises are resolved along the processing pipeline.\r\n   * This may help expose bugs where results are dependent on the order of\r\n   * Promise resolution rather than on the logical order of the stream (i.e.,\r\n   * due to hidden mutable state).\r\n   *\r\n   * @returns A Promise for an array of stream elements, which will resolve\r\n   *   when the stream is exhausted.\r\n   */\n  async toArrayForTest() {\n    const stream = this.prefetch(100);\n    const result = [];\n    let x = await stream.next();\n    while (!x.done) {\n      result.push(x.value);\n      x = await stream.next();\n    }\n    return result;\n  }\n  /**\r\n   * Draw items from the stream until it is exhausted.\r\n   *\r\n   * This can be useful when the stream has side effects but no output.  In\r\n   * that case, calling this function guarantees that the stream will be\r\n   * fully processed.\r\n   */\n  async resolveFully() {\n    let x = await this.next();\n    while (!x.done) {\n      x = await this.next();\n    }\n  }\n  /**\r\n   * Draw items from the stream until it is exhausted, or a predicate fails.\r\n   *\r\n   * This can be useful when the stream has side effects but no output.  In\r\n   * that case, calling this function guarantees that the stream will be\r\n   * fully processed.\r\n   */\n  async resolveWhile(predicate) {\n    let x = await this.next();\n    let shouldContinue = predicate(x.value);\n    while (!x.done && shouldContinue) {\n      x = await this.next();\n      shouldContinue = predicate(x.value);\n    }\n  }\n  /**\r\n   * Handles errors thrown on this stream using a provided handler function.\r\n   *\r\n   * @param handler A function that handles any `Error` thrown during a `next()`\r\n   *   call and returns true if the stream should continue (dropping the failed\r\n   *   call) or false if the stream should quietly terminate.  If the handler\r\n   *   itself throws (or rethrows) an `Error`, that will be propagated.\r\n   *\r\n   * @returns A `LazyIterator` of elements passed through from upstream,\r\n   *   possibly filtering or terminating on upstream `next()` calls that\r\n   *   throw an `Error`.\r\n   */\n  handleErrors(handler) {\n    return new ErrorHandlingLazyIterator(this, handler);\n  }\n  // TODO(soergel): Implement reduce() etc.\n  /**\r\n   * Filters this stream according to `predicate`.\r\n   *\r\n   * @param predicate A function mapping a stream element to a boolean or a\r\n   * `Promise` for one.\r\n   *\r\n   * @returns A `LazyIterator` of elements for which the predicate was true.\r\n   */\n  filter(predicate) {\n    return new FilterIterator(this, predicate);\n  }\n  /**\r\n   * Maps this stream through a 1-to-1 transform.\r\n   *\r\n   * @param transform A function mapping a stream element to a transformed\r\n   *   element.\r\n   *\r\n   * @returns A `LazyIterator` of transformed elements.\r\n   */\n  map(transform) {\n    return new MapIterator(this, transform);\n  }\n  /**\r\n   * Maps this stream through an async 1-to-1 transform.\r\n   *\r\n   * @param transform A function mapping a stream element to a `Promise` for a\r\n   *   transformed stream element.\r\n   *\r\n   * @returns A `LazyIterator` of transformed elements.\r\n   */\n  mapAsync(transform) {\n    return new AsyncMapIterator(this, transform);\n  }\n  /**\r\n   * Maps this stream through a 1-to-1 transform, forcing serial execution.\r\n   *\r\n   * @param transform A function mapping a stream element to a transformed\r\n   *   element.\r\n   *\r\n   * @returns A `LazyIterator` of transformed elements.\r\n   */\n  serialMapAsync(transform) {\n    return new AsyncMapIterator(this, transform).serial();\n  }\n  /**\r\n   * Maps this stream through a 1-to-many transform.\r\n   *\r\n   * @param transform A function mapping a stream element to an array of\r\n   *   transformed elements.\r\n   *\r\n   * @returns A `DataStream` of transformed elements.\r\n   */\n  flatmap(transform) {\n    return new FlatmapIterator(this, transform);\n  }\n  /**\r\n   * Apply a function to every element of the stream.\r\n   *\r\n   * @param f A function to apply to each stream element.\r\n   */\n  async forEachAsync(f) {\n    return this.map(f).resolveFully();\n  }\n  /**\r\n   * Apply a function to every element of the stream, forcing serial execution.\r\n   *\r\n   * @param f A function to apply to each stream element.  Should return 'true'\r\n   *   to indicate that the stream should continue, or 'false' to cause it to\r\n   *   terminate.\r\n   */\n  async serialForEach(f) {\n    return this.serialMapAsync(f).resolveWhile(x => x === true);\n  }\n  /**\r\n   * Groups elements into batches, represented as arrays of elements.\r\n   *\r\n   * We can think of the elements of this iterator as 'rows' (even if they are\r\n   * nested structures).  By the same token, consecutive values for a given\r\n   * key within the elements form a 'column'.  This matches the usual sense of\r\n   * 'row' and 'column' when processing tabular data (e.g., parsing a CSV).\r\n   *\r\n   * Thus, \"Row-major\" means that the resulting batch is simply a collection of\r\n   * rows: `[row1, row2, row3, ...]`.  This is contrast to the column-major\r\n   * form, which is needed for vectorized computation.\r\n   *\r\n   * @param batchSize The number of elements desired per batch.\r\n   * @param smallLastBatch Whether to emit the final batch when it has fewer\r\n   *   than batchSize elements. Default true.\r\n   * @returns A `LazyIterator` of batches of elements, represented as arrays\r\n   *   of the original element type.\r\n   */\n  rowMajorBatch(batchSize, smallLastBatch = true) {\n    return new RowMajorBatchIterator(this, batchSize, smallLastBatch);\n  }\n  /**\r\n   * Groups elements into batches, represented in column-major form.\r\n   *\r\n   * We can think of the elements of this iterator as 'rows' (even if they are\r\n   * nested structures).  By the same token, consecutive values for a given\r\n   * key within the elements form a 'column'.  This matches the usual sense of\r\n   * 'row' and 'column' when processing tabular data (e.g., parsing a CSV).\r\n   *\r\n   * Thus, \"column-major\" means that the resulting batch is a (potentially\r\n   * nested) structure representing the columns.  Each column entry, then,\r\n   * contains a collection of the values found in that column for a range of\r\n   * input elements.  This representation allows for vectorized computation, in\r\n   * contrast to the row-major form.\r\n   *\r\n   * The inputs should all have the same nested structure (i.e., of arrays and\r\n   * dicts).  The result is a single object with the same nested structure,\r\n   * where the leaves are arrays collecting the values of the inputs at that\r\n   * location (or, optionally, the result of a custom function applied to those\r\n   * arrays).\r\n   *\r\n   * @param batchSize The number of elements desired per batch.\r\n   * @param smallLastBatch Whether to emit the final batch when it has fewer\r\n   *   than batchSize elements. Default true.\r\n   * @param zipFn: (optional) A function that expects an array of elements at a\r\n   *   single node of the object tree, and returns a `DeepMapResult`.  The\r\n   *   `DeepMapResult` either provides a result value for that node (i.e.,\r\n   *   representing the subtree), or indicates that the node should be processed\r\n   *   recursively.  The default zipFn recurses as far as possible and places\r\n   *   arrays at the leaves.\r\n   * @returns A `LazyIterator` of batches of elements, represented as an object\r\n   *   with collections at the leaves.\r\n   */\n  columnMajorBatch(batchSize, smallLastBatch = true,\n  // tslint:disable-next-line:no-any\n  zipFn = zipToList) {\n    // First collect the desired number of input elements as a row-major batch.\n    const rowBatches = this.rowMajorBatch(batchSize, smallLastBatch);\n    // Now 'rotate' or 'pivot' the data, collecting all values from each column\n    // in the batch (i.e., for each key within the elements) into an array.\n    return rowBatches.map(x => deepZip(x, zipFn));\n  }\n  /**\r\n   * Concatenate this `LazyIterator` with another.\r\n   *\r\n   * @param iterator A `LazyIterator` to be concatenated onto this one.\r\n   * @param baseErrorHandler An optional function that can intercept `Error`s\r\n   *   raised during a `next()` call on the base stream.  This function can\r\n   *   decide whether the error should be propagated, whether the error should\r\n   *   be ignored, or whether the base stream should be terminated.\r\n   * @returns A `LazyIterator`.\r\n   */\n  concatenate(iterator, baseErrorHandler) {\n    return new ChainedIterator(iteratorFromItems([this, iterator]), baseErrorHandler);\n  }\n  /**\r\n   * Limits this stream to return at most `count` items.\r\n   *\r\n   * @param count The maximum number of items to provide from the stream. If\r\n   * a negative or undefined value is given, the entire stream is returned\r\n   *   unaltered.\r\n   */\n  take(count) {\n    if (count < 0 || count == null) {\n      return this;\n    }\n    return new TakeIterator(this, count);\n  }\n  /**\r\n   * Skips the first `count` items in this stream.\r\n   *\r\n   * @param count The number of items to skip.  If a negative or undefined\r\n   * value is given, the entire stream is returned unaltered.\r\n   */\n  skip(count) {\n    if (count < 0 || count == null) {\n      return this;\n    }\n    return new SkipIterator(this, count);\n  }\n  /**\r\n   * Prefetch the first `bufferSize` items in this stream.\r\n   *\r\n   * Note this prefetches Promises, but makes no guarantees about when those\r\n   * Promises resolve.\r\n   *\r\n   * @param bufferSize: An integer specifying the number of elements to be\r\n   *   prefetched.\r\n   */\n  prefetch(bufferSize) {\n    return new PrefetchIterator(this, bufferSize);\n  }\n  // TODO(soergel): deep sharded shuffle, where supported\n  /**\r\n   * Randomly shuffles the elements of this stream.\r\n   *\r\n   * @param bufferSize: An integer specifying the number of elements from\r\n   * this stream from which the new stream will sample.\r\n   * @param seed: (Optional.) An integer specifying the random seed that\r\n   * will be used to create the distribution.\r\n   */\n  shuffle(windowSize, seed) {\n    return new ShuffleIterator(this, windowSize, seed);\n  }\n  /**\r\n   * Force an iterator to execute serially: each next() call will await the\r\n   * prior one, so that they cannot execute concurrently.\r\n   */\n  serial() {\n    return new SerialIterator(this);\n  }\n}\n// ============================================================================\n// The following private classes serve to implement the chainable methods\n// on LazyIterator.  Unfortunately they can't be placed in separate files,\n// due to resulting trouble with circular imports.\n// ============================================================================\n// Iterators that just extend LazyIterator directly\n// ============================================================================\nclass ArrayIterator extends LazyIterator {\n  constructor(items) {\n    super();\n    this.items = items;\n    this.trav = 0;\n  }\n  summary() {\n    return `Array of ${this.items.length} items`;\n  }\n  async next() {\n    if (this.trav >= this.items.length) {\n      return {\n        value: null,\n        done: true\n      };\n    }\n    const item = this.items[this.trav];\n    this.trav++;\n    return {\n      value: deepClone(item),\n      done: false\n    };\n  }\n}\nclass FunctionCallIterator extends LazyIterator {\n  constructor(nextFn) {\n    super();\n    this.nextFn = nextFn;\n  }\n  summary() {\n    return `Function call`;\n  }\n  async next() {\n    try {\n      return this.nextFn();\n    } catch (e) {\n      // Modify the error message but leave the stack trace intact\n      e.message = `Error thrown while iterating through a dataset: ${e.message}`;\n      throw e;\n    }\n  }\n}\nclass SerialIterator extends LazyIterator {\n  constructor(upstream) {\n    super();\n    this.upstream = upstream;\n    this.lastRead = Promise.resolve({\n      value: null,\n      done: false\n    });\n  }\n  summary() {\n    return `${this.upstream.summary()} -> Serial`;\n  }\n  async next() {\n    // This sets this.lastRead to a new Promise right away, as opposed to\n    // saying `await this.lastRead; this.lastRead = this.serialNext();` which\n    // would not work because this.nextRead would be updated only after the\n    // promise resolves.\n    this.lastRead = this.lastRead.then(() => this.serialNext());\n    return this.lastRead;\n  }\n  async serialNext() {\n    return this.upstream.next();\n  }\n}\nclass SkipIterator extends LazyIterator {\n  constructor(upstream, maxCount) {\n    super();\n    this.upstream = upstream;\n    this.maxCount = maxCount;\n    // Local state that should not be clobbered by out-of-order execution.\n    this.count = 0;\n    this.lastRead = Promise.resolve({\n      value: null,\n      done: false\n    });\n  }\n  summary() {\n    return `${this.upstream.summary()} -> Skip`;\n  }\n  async next() {\n    // This sets this.lastRead to a new Promise right away, as opposed to\n    // saying `await this.lastRead; this.lastRead = this.serialNext();` which\n    // would not work because this.nextRead would be updated only after the\n    // promise resolves.\n    this.lastRead = this.lastRead.then(() => this.serialNext());\n    return this.lastRead;\n  }\n  async serialNext() {\n    // TODO(soergel): consider tradeoffs of reading in parallel, eg.\n    // collecting next() promises in an Array and then waiting for\n    // Promise.all() of those. Benefit: pseudo-parallel execution.  Drawback:\n    // maybe delayed GC.\n    while (this.count++ < this.maxCount) {\n      const skipped = await this.upstream.next();\n      // short-circuit if upstream is already empty\n      if (skipped.done) {\n        return skipped;\n      }\n      tf.dispose(skipped.value);\n    }\n    return this.upstream.next();\n  }\n}\nclass TakeIterator extends LazyIterator {\n  constructor(upstream, maxCount) {\n    super();\n    this.upstream = upstream;\n    this.maxCount = maxCount;\n    this.count = 0;\n  }\n  summary() {\n    return `${this.upstream.summary()} -> Take`;\n  }\n  async next() {\n    if (this.count++ >= this.maxCount) {\n      return {\n        value: null,\n        done: true\n      };\n    }\n    return this.upstream.next();\n  }\n}\n// Note this batch just groups items into row-wise element arrays.\n// Rotating these to a column-wise representation happens only at the dataset\n// level.\nclass RowMajorBatchIterator extends LazyIterator {\n  constructor(upstream, batchSize, enableSmallLastBatch = true) {\n    super();\n    this.upstream = upstream;\n    this.batchSize = batchSize;\n    this.enableSmallLastBatch = enableSmallLastBatch;\n    this.lastRead = Promise.resolve({\n      value: null,\n      done: false\n    });\n  }\n  summary() {\n    return `${this.upstream.summary()} -> RowMajorBatch`;\n  }\n  async next() {\n    // This sets this.lastRead to a new Promise right away, as opposed to\n    // saying `await this.lastRead; this.lastRead = this.serialNext();` which\n    // would not work because this.nextRead would be updated only after the\n    // promise resolves.\n    this.lastRead = this.lastRead.then(() => this.serialNext());\n    return this.lastRead;\n  }\n  async serialNext() {\n    const batch = [];\n    while (batch.length < this.batchSize) {\n      const item = await this.upstream.next();\n      if (item.done) {\n        if (this.enableSmallLastBatch && batch.length > 0) {\n          return {\n            value: batch,\n            done: false\n          };\n        }\n        return {\n          value: null,\n          done: true\n        };\n      }\n      batch.push(item.value);\n    }\n    return {\n      value: batch,\n      done: false\n    };\n  }\n}\nclass FilterIterator extends LazyIterator {\n  constructor(upstream, predicate) {\n    super();\n    this.upstream = upstream;\n    this.predicate = predicate;\n    this.lastRead = Promise.resolve({\n      value: null,\n      done: false\n    });\n  }\n  summary() {\n    return `${this.upstream.summary()} -> Filter`;\n  }\n  async next() {\n    // This sets this.lastRead to a new Promise right away, as opposed to\n    // saying `await this.lastRead; this.lastRead = this.serialNext();` which\n    // would not work because this.nextRead would be updated only after the\n    // promise resolves.\n    this.lastRead = this.lastRead.then(() => this.serialNext());\n    return this.lastRead;\n  }\n  async serialNext() {\n    while (true) {\n      const item = await this.upstream.next();\n      if (item.done || this.predicate(item.value)) {\n        return item;\n      }\n      tf.dispose(item.value);\n    }\n  }\n}\nclass MapIterator extends LazyIterator {\n  constructor(upstream, transform) {\n    super();\n    this.upstream = upstream;\n    this.transform = transform;\n  }\n  summary() {\n    return `${this.upstream.summary()} -> Map`;\n  }\n  async next() {\n    const item = await this.upstream.next();\n    if (item.done) {\n      return {\n        value: null,\n        done: true\n      };\n    }\n    const inputTensors = tf.tensor_util.getTensorsInContainer(item.value);\n    // Careful: the transform may mutate the item in place.\n    // That's why we have to remember the input Tensors above, and then\n    // below dispose only those that were not passed through to the output.\n    // Note too that the transform function is responsible for tidying\n    // any intermediate Tensors.  Here we are concerned only about the\n    // inputs.\n    const mapped = this.transform(item.value);\n    const outputTensors = tf.tensor_util.getTensorsInContainer(mapped);\n    // TODO(soergel) faster intersection\n    // TODO(soergel) move to tf.disposeExcept(in, out)?\n    for (const t of inputTensors) {\n      if (!tf.tensor_util.isTensorInList(t, outputTensors)) {\n        t.dispose();\n      }\n    }\n    return {\n      value: mapped,\n      done: false\n    };\n  }\n}\nclass ErrorHandlingLazyIterator extends LazyIterator {\n  constructor(upstream, handler) {\n    super();\n    this.upstream = upstream;\n    this.handler = handler;\n    this.count = 0;\n    this.lastRead = Promise.resolve({\n      value: null,\n      done: false\n    });\n  }\n  summary() {\n    return `${this.upstream.summary()} -> handleErrors`;\n  }\n  async next() {\n    // This sets this.lastRead to a new Promise right away, as opposed to\n    // saying `await this.lastRead; this.lastRead = this.serialNext();` which\n    // would not work because this.nextRead would be updated only after the\n    // promise resolves.\n    this.lastRead = this.lastRead.then(() => this.serialNext());\n    return this.lastRead;\n  }\n  async serialNext() {\n    while (true) {\n      try {\n        return await this.upstream.next();\n      } catch (e) {\n        if (!this.handler(e)) {\n          return {\n            value: null,\n            done: true\n          };\n        }\n        // If the handler returns true, loop and fetch the next upstream item.\n        // If the upstream iterator throws an endless stream of errors, and if\n        // the handler says to ignore them, then we loop forever here.  That is\n        // the correct behavior-- it's up to the handler to decide when to stop.\n      }\n    }\n  }\n}\n\nclass AsyncMapIterator extends LazyIterator {\n  constructor(upstream, transform) {\n    super();\n    this.upstream = upstream;\n    this.transform = transform;\n  }\n  summary() {\n    return `${this.upstream.summary()} -> AsyncMap`;\n  }\n  async next() {\n    const item = await this.upstream.next();\n    if (item.done) {\n      return {\n        value: null,\n        done: true\n      };\n    }\n    const inputTensors = tf.tensor_util.getTensorsInContainer(item.value);\n    // Careful: the transform may mutate the item in place.\n    // That's why we have to remember the input Tensors above, and then\n    // below dispose only those that were not passed through to the output.\n    // Note too that the transform function is responsible for tidying\n    // any intermediate Tensors.  Here we are concerned only about the\n    // inputs.\n    const mapped = await this.transform(item.value);\n    const outputTensors = tf.tensor_util.getTensorsInContainer(mapped);\n    // TODO(soergel) faster intersection\n    // TODO(soergel) move to tf.disposeExcept(in, out)?\n    for (const t of inputTensors) {\n      if (!tf.tensor_util.isTensorInList(t, outputTensors)) {\n        t.dispose();\n      }\n    }\n    return {\n      value: mapped,\n      done: false\n    };\n  }\n}\n// Iterators that maintain a queue of pending items\n// ============================================================================\n/**\r\n * A base class for transforming streams that operate by maintaining an\r\n * output queue of elements that are ready to return via next().  This is\r\n * commonly required when the transformation is 1-to-many:  A call to next()\r\n * may trigger a call to the underlying stream, which will produce many\r\n * mapped elements of this stream-- of which we need to return only one, so\r\n * we have to queue the rest.\r\n */\nexport class OneToManyIterator extends LazyIterator {\n  constructor() {\n    super();\n    this.outputQueue = new GrowingRingBuffer();\n    this.lastRead = Promise.resolve({\n      value: null,\n      done: false\n    });\n  }\n  async next() {\n    // This sets this.lastRead to a new Promise right away, as opposed to\n    // saying `await this.lastRead; this.lastRead = this.serialNext();` which\n    // would not work because this.nextRead would be updated only after the\n    // promise resolves.\n    this.lastRead = this.lastRead.then(() => this.serialNext());\n    return this.lastRead;\n  }\n  async serialNext() {\n    // Fetch so that the queue contains at least one item if possible.\n    // If the upstream source is exhausted, AND there are no items left in\n    // the output queue, then this stream is also exhausted.\n    while (this.outputQueue.length() === 0) {\n      // TODO(soergel): consider parallel reads.\n      if (!(await this.pump())) {\n        return {\n          value: null,\n          done: true\n        };\n      }\n    }\n    return {\n      value: this.outputQueue.shift(),\n      done: false\n    };\n  }\n}\nclass FlatmapIterator extends OneToManyIterator {\n  constructor(upstream, transform) {\n    super();\n    this.upstream = upstream;\n    this.transform = transform;\n  }\n  summary() {\n    return `${this.upstream.summary()} -> Flatmap`;\n  }\n  async pump() {\n    const item = await this.upstream.next();\n    if (item.done) {\n      return false;\n    }\n    const inputTensors = tf.tensor_util.getTensorsInContainer(item.value);\n    // Careful: the transform may mutate the item in place.\n    // that's why we have to remember the input Tensors above, and then\n    // below dispose only those that were not passed through to the output.\n    // Note too that the transform function is responsible for tidying any\n    // intermediate Tensors.  Here we are concerned only about the inputs.\n    const mappedArray = this.transform(item.value);\n    const outputTensors = tf.tensor_util.getTensorsInContainer(mappedArray);\n    this.outputQueue.pushAll(mappedArray);\n    // TODO(soergel) faster intersection, and deduplicate outputTensors\n    // TODO(soergel) move to tf.disposeExcept(in, out)?\n    for (const t of inputTensors) {\n      if (!tf.tensor_util.isTensorInList(t, outputTensors)) {\n        t.dispose();\n      }\n    }\n    return true;\n  }\n}\n/**\r\n * Provides a `LazyIterator` that concatenates a stream of underlying\r\n * streams.\r\n *\r\n * Doing this in a concurrency-safe way requires some trickery.  In\r\n * particular, we want this stream to return the elements from the\r\n * underlying streams in the correct order according to when next() was\r\n * called, even if the resulting Promises resolve in a different order.\r\n */\nexport class ChainedIterator extends LazyIterator {\n  constructor(iterators, baseErrorHandler) {\n    super();\n    this.baseErrorHandler = baseErrorHandler;\n    // Strict Promise execution order:\n    // a next() call may not even begin until the previous one completes.\n    this.lastRead = null;\n    // Local state that should not be clobbered by out-of-order execution.\n    this.iterator = null;\n    this.moreIterators = iterators;\n  }\n  summary() {\n    const upstreamSummaries = 'TODO: fill in upstream of chained summaries';\n    return `${upstreamSummaries} -> Chained`;\n  }\n  async next() {\n    this.lastRead = this.readFromChain(this.lastRead);\n    return this.lastRead;\n  }\n  async readFromChain(lastRead) {\n    // Must await on the previous read since the previous read may have advanced\n    // the stream of streams, from which we need to read.\n    // This is unfortunate since we can't parallelize reads. Which means\n    // prefetching of chained streams is a no-op.\n    // One solution is to prefetch immediately upstream of this.\n    await lastRead;\n    if (this.iterator == null) {\n      const iteratorResult = await this.moreIterators.next();\n      if (iteratorResult.done) {\n        // No more streams to stream from.\n        return {\n          value: null,\n          done: true\n        };\n      }\n      this.iterator = iteratorResult.value;\n      if (this.baseErrorHandler != null) {\n        this.iterator = this.iterator.handleErrors(this.baseErrorHandler);\n      }\n    }\n    const itemResult = await this.iterator.next();\n    if (itemResult.done) {\n      this.iterator = null;\n      return this.readFromChain(lastRead);\n    }\n    return itemResult;\n  }\n}\nexport var ZipMismatchMode;\n(function (ZipMismatchMode) {\n  ZipMismatchMode[ZipMismatchMode[\"FAIL\"] = 0] = \"FAIL\";\n  ZipMismatchMode[ZipMismatchMode[\"SHORTEST\"] = 1] = \"SHORTEST\";\n  ZipMismatchMode[ZipMismatchMode[\"LONGEST\"] = 2] = \"LONGEST\"; // use nulls for exhausted streams; use up the longest stream.\n})(ZipMismatchMode || (ZipMismatchMode = {}));\n/**\r\n * Provides a `LazyIterator` that zips together an array, dict, or nested\r\n * structure of `LazyIterator`s (and perhaps additional constants).\r\n *\r\n * The underlying streams must provide elements in a consistent order such\r\n * that they correspond.\r\n *\r\n * Typically, the underlying streams should have the same number of\r\n * elements. If they do not, the behavior is determined by the\r\n * `mismatchMode` argument.\r\n *\r\n * The nested structure of the `iterators` argument determines the\r\n * structure of elements in the resulting iterator.\r\n *\r\n * Doing this in a concurrency-safe way requires some trickery.  In\r\n * particular, we want this stream to return the elements from the\r\n * underlying streams in the correct order according to when next() was\r\n * called, even if the resulting Promises resolve in a different order.\r\n *\r\n * @param iterators: An array or object containing LazyIterators at the\r\n * leaves.\r\n * @param mismatchMode: Determines what to do when one underlying iterator\r\n * is exhausted before the others.  `ZipMismatchMode.FAIL` (the default)\r\n * causes an error to be thrown in this case.  `ZipMismatchMode.SHORTEST`\r\n * causes the zipped iterator to terminate with the furst underlying\r\n * streams, so elements remaining on the longer streams are ignored.\r\n * `ZipMismatchMode.LONGEST` causes the zipped stream to continue, filling\r\n * in nulls for the exhausted streams, until all streams are exhausted.\r\n */\nclass ZipIterator extends LazyIterator {\n  constructor(iterators, mismatchMode = ZipMismatchMode.FAIL) {\n    super();\n    this.iterators = iterators;\n    this.mismatchMode = mismatchMode;\n    this.count = 0;\n    this.currentPromise = null;\n  }\n  summary() {\n    const upstreamSummaries = 'TODO: fill in upstream of zip summaries';\n    return `{${upstreamSummaries}} -> Zip`;\n  }\n  async nextState(afterState) {\n    // This chaining ensures that the underlying next() are not even called\n    // before the previous ones have resolved.\n    await afterState;\n    // Collect underlying iterator \"done\" signals as a side effect in\n    // getNext()\n    let numIterators = 0;\n    let iteratorsDone = 0;\n    function getNext(container) {\n      if (container instanceof LazyIterator) {\n        const result = container.next();\n        return {\n          value: result.then(x => {\n            numIterators++;\n            if (x.done) {\n              iteratorsDone++;\n            }\n            return x.value;\n          }),\n          recurse: false\n        };\n      } else {\n        return {\n          value: null,\n          recurse: true\n        };\n      }\n    }\n    const mapped = await deepMapAndAwaitAll(this.iterators, getNext);\n    if (numIterators === iteratorsDone) {\n      // The streams have all ended.\n      return {\n        value: null,\n        done: true\n      };\n    }\n    if (iteratorsDone > 0) {\n      switch (this.mismatchMode) {\n        case ZipMismatchMode.FAIL:\n          throw new Error('Zipped streams should have the same length. ' + `Mismatched at element ${this.count}.`);\n        case ZipMismatchMode.SHORTEST:\n          return {\n            value: null,\n            done: true\n          };\n        case ZipMismatchMode.LONGEST:\n        default:\n        // Continue.  The exhausted streams already produced value: null.\n      }\n    }\n\n    this.count++;\n    return {\n      value: mapped,\n      done: false\n    };\n  }\n  async next() {\n    this.currentPromise = this.nextState(this.currentPromise);\n    return this.currentPromise;\n  }\n}\n// Iterators that maintain a ring buffer of pending promises\n// ============================================================================\n/**\r\n * A stream that prefetches a given number of items from an upstream source,\r\n * returning them in FIFO order.\r\n *\r\n * Note this prefetches Promises, but makes no guarantees about when those\r\n * Promises resolve.\r\n */\nexport class PrefetchIterator extends LazyIterator {\n  constructor(upstream, bufferSize) {\n    super();\n    this.upstream = upstream;\n    this.bufferSize = bufferSize;\n    this.buffer = new RingBuffer(bufferSize);\n  }\n  summary() {\n    return `${this.upstream.summary()} -> Prefetch`;\n  }\n  /**\r\n   * Refill the prefetch buffer.  Returns only after the buffer is full, or\r\n   * the upstream source is exhausted.\r\n   */\n  refill() {\n    while (!this.buffer.isFull()) {\n      const v = this.upstream.next();\n      this.buffer.push(v);\n    }\n  }\n  next() {\n    this.refill();\n    // This shift will never throw an error because the buffer is always\n    // full after a refill. If the stream is exhausted, the buffer will be\n    // full of Promises that will resolve to the end-of-stream signal.\n    return this.buffer.shift();\n  }\n}\n/**\r\n * A stream that performs a sliding-window random shuffle on an upstream\r\n * source. This is like a `PrefetchIterator` except that the items are\r\n * returned in randomized order.  Mixing naturally improves as the buffer\r\n * size increases.\r\n */\nexport class ShuffleIterator extends PrefetchIterator {\n  constructor(upstream, windowSize, seed) {\n    super(upstream, windowSize);\n    this.upstream = upstream;\n    this.windowSize = windowSize;\n    // Local state that should not be clobbered by out-of-order execution.\n    this.upstreamExhausted = false;\n    this.random = seedrandom.alea(seed || tf.util.now().toString());\n    this.lastRead = Promise.resolve({\n      value: null,\n      done: false\n    });\n  }\n  async next() {\n    // This sets this.lastRead to a new Promise right away, as opposed to\n    // saying `await this.lastRead; this.lastRead = this.serialNext();` which\n    // would not work because this.nextRead would be updated only after the\n    // promise resolves.\n    this.lastRead = this.lastRead.then(() => this.serialNext());\n    return this.lastRead;\n  }\n  randomInt(max) {\n    return Math.floor(this.random() * max);\n  }\n  chooseIndex() {\n    return this.randomInt(this.buffer.length());\n  }\n  async serialNext() {\n    // TODO(soergel): consider performance\n    if (!this.upstreamExhausted) {\n      this.refill();\n    }\n    while (!this.buffer.isEmpty()) {\n      const chosenIndex = this.chooseIndex();\n      const result = await this.buffer.shuffleExcise(chosenIndex);\n      if (result.done) {\n        this.upstreamExhausted = true;\n      } else {\n        this.refill();\n        return result;\n      }\n    }\n    return {\n      value: null,\n      done: true\n    };\n  }\n}","map":null,"metadata":{},"sourceType":"module"}