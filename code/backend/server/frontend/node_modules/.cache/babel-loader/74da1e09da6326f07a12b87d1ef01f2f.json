{"ast":null,"code":"/**\r\n * @license\r\n * Copyright 2020 Google LLC. All Rights Reserved.\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n * http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n * =============================================================================\r\n */\nimport { ENGINE } from '../engine';\nimport { Tile } from '../kernel_names';\nimport { convertToTensor } from '../tensor_util_env';\nimport { clone } from './clone';\nimport { op } from './operation';\nimport { reshape } from './reshape';\n/**\r\n * Broadcast an array to a compatible shape NumPy-style.\r\n *\r\n * The tensor's shape is compared to the broadcast shape from end to beginning.\r\n * Ones are prepended to the tensor's shape until is has the same length as\r\n * the broadcast shape. If input.shape[i]==shape[i], the (i+1)-th axis is\r\n * already broadcast-compatible. If input.shape[i]==1 and shape[i]==N, then\r\n * the input tensor is tiled N times along that axis (using tf.tile).\r\n *\r\n * @param input The tensor that is to be broadcasted.\r\n * @param shape The input is to be broadcast to this shape.\r\n *\r\n * @doc {heading: 'Tensors', subheading: 'Transformations'}\r\n */\nfunction broadcastTo_(x, shape) {\n  let input = convertToTensor(x, 'broadcastTo', 'x');\n  const xShape = input.shape;\n  if (shape.some(d => !(d > 0) || d % 1 !== 0)) {\n    throw new Error(`broadcastTo(): Invalid broadcast shape [${shape}].`);\n  }\n  if (shape.length < input.rank) {\n    throw new Error(`broadcastTo(): shape.length=${shape.length} < input.rank=${input.rank}.`);\n  }\n  if (shape.length > input.rank) {\n    const newShape = input.shape.slice();\n    while (newShape.length < shape.length) {\n      newShape.unshift(1);\n    }\n    input = reshape(input, newShape);\n  }\n  const inputShape = input.shape;\n  const reps = Array.from(shape);\n  for (let i = shape.length - 1; i >= 0; i--) {\n    if (inputShape[i] === shape[i]) {\n      reps[i] = 1;\n    } else if (input.shape[i] !== 1) {\n      throw new Error(`broadcastTo(): [${xShape}] cannot be broadcast to [${shape}].`);\n    }\n  }\n  const axes = reps.map((n, i) => n > 1 ? i : -1).filter(i => i >= 0);\n  if (axes.length === 0) {\n    return clone(input);\n  }\n  // TODO call broadcastTo kernel directly once backends implement broadcstTo\n  const inputs = {\n    x: input\n  };\n  const attrs = {\n    reps\n  };\n  return ENGINE.runKernel(Tile, inputs, attrs);\n}\nexport const broadcastTo = op({\n  broadcastTo_\n});","map":null,"metadata":{},"sourceType":"module"}