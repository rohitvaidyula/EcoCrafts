{"ast":null,"code":"/**\r\n * @license\r\n * Copyright 2018 Google LLC. All Rights Reserved.\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n * http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n * =============================================================================\r\n */\nimport { AdadeltaOptimizer } from './adadelta_optimizer';\nimport { AdagradOptimizer } from './adagrad_optimizer';\nimport { AdamOptimizer } from './adam_optimizer';\nimport { AdamaxOptimizer } from './adamax_optimizer';\nimport { MomentumOptimizer } from './momentum_optimizer';\nimport { RMSPropOptimizer } from './rmsprop_optimizer';\nimport { SGDOptimizer } from './sgd_optimizer';\nexport class OptimizerConstructors {\n  /**\r\n   * Constructs a `tf.SGDOptimizer` that uses stochastic gradient descent.\r\n   *\r\n   * ```js\r\n   * // Fit a quadratic function by learning the coefficients a, b, c.\r\n   * const xs = tf.tensor1d([0, 1, 2, 3]);\r\n   * const ys = tf.tensor1d([1.1, 5.9, 16.8, 33.9]);\r\n   *\r\n   * const a = tf.scalar(Math.random()).variable();\r\n   * const b = tf.scalar(Math.random()).variable();\r\n   * const c = tf.scalar(Math.random()).variable();\r\n   *\r\n   * // y = a * x^2 + b * x + c.\r\n   * const f = x => a.mul(x.square()).add(b.mul(x)).add(c);\r\n   * const loss = (pred, label) => pred.sub(label).square().mean();\r\n   *\r\n   * const learningRate = 0.01;\r\n   * const optimizer = tf.train.sgd(learningRate);\r\n   *\r\n   * // Train the model.\r\n   * for (let i = 0; i < 10; i++) {\r\n   *   optimizer.minimize(() => loss(f(xs), ys));\r\n   * }\r\n   *\r\n   * // Make predictions.\r\n   * console.log(\r\n   *     `a: ${a.dataSync()}, b: ${b.dataSync()}, c: ${c.dataSync()}`);\r\n   * const preds = f(xs).dataSync();\r\n   * preds.forEach((pred, i) => {\r\n   *   console.log(`x: ${i}, pred: ${pred}`);\r\n   * });\r\n   * ```\r\n   *\r\n   * @param learningRate The learning rate to use for the SGD algorithm.\r\n   *\r\n   * @doc {heading: 'Training', subheading: 'Optimizers', namespace: 'train'}\r\n   */\n  static sgd(learningRate) {\n    return new SGDOptimizer(learningRate);\n  }\n  /**\r\n   * Constructs a `tf.MomentumOptimizer` that uses momentum gradient\r\n   * descent.\r\n   *\r\n   * See\r\n   * [http://proceedings.mlr.press/v28/sutskever13.pdf](\r\n   * http://proceedings.mlr.press/v28/sutskever13.pdf)\r\n   *\r\n   * @param learningRate The learning rate to use for the Momentum gradient\r\n   * descent algorithm.\r\n   * @param momentum The momentum to use for the momentum gradient descent\r\n   * algorithm.\r\n   *\r\n   * @doc {heading: 'Training', subheading: 'Optimizers', namespace: 'train'}\r\n   */\n  static momentum(learningRate, momentum, useNesterov = false) {\n    return new MomentumOptimizer(learningRate, momentum, useNesterov);\n  }\n  /**\r\n   * Constructs a `tf.RMSPropOptimizer` that uses RMSProp gradient\r\n   * descent. This implementation uses plain momentum and is not centered\r\n   * version of RMSProp.\r\n   *\r\n   * See\r\n   * [http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf](\r\n   * http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf)\r\n   *\r\n   * @param learningRate The learning rate to use for the RMSProp gradient\r\n   * descent algorithm.\r\n   * @param decay The discounting factor for the history/coming gradient.\r\n   * @param momentum The momentum to use for the RMSProp gradient descent\r\n   * algorithm.\r\n   * @param epsilon Small value to avoid zero denominator.\r\n   * @param centered If true, gradients are normalized by the estimated\r\n   * variance of the gradient.\r\n   *\r\n   * @doc {heading: 'Training', subheading: 'Optimizers', namespace: 'train'}\r\n   */\n  static rmsprop(learningRate, decay = .9, momentum = 0.0, epsilon = null, centered = false) {\n    return new RMSPropOptimizer(learningRate, decay, momentum, epsilon, centered);\n  }\n  /**\r\n   * Constructs a `tf.AdamOptimizer` that uses the Adam algorithm.\r\n   * See [https://arxiv.org/abs/1412.6980](https://arxiv.org/abs/1412.6980)\r\n   *\r\n   * @param learningRate The learning rate to use for the Adam gradient\r\n   * descent algorithm.\r\n   * @param beta1 The exponential decay rate for the 1st moment estimates.\r\n   * @param beta2 The exponential decay rate for the 2nd moment estimates.\r\n   * @param epsilon A small constant for numerical stability.\r\n   *\r\n   * @doc {heading: 'Training', subheading: 'Optimizers', namespace: 'train'}\r\n   */\n  static adam(learningRate = 0.001, beta1 = 0.9, beta2 = 0.999, epsilon = null) {\n    return new AdamOptimizer(learningRate, beta1, beta2, epsilon);\n  }\n  /**\r\n   * Constructs a `tf.AdadeltaOptimizer` that uses the Adadelta algorithm.\r\n   * See [https://arxiv.org/abs/1212.5701](https://arxiv.org/abs/1212.5701)\r\n   *\r\n   * @param learningRate The learning rate to use for the Adadelta gradient\r\n   * descent algorithm.\r\n   * @param rho The learning rate decay over each update.\r\n   * @param epsilon A constant epsilon used to better condition the grad\r\n   * update.\r\n   *\r\n   * @doc {heading: 'Training', subheading: 'Optimizers', namespace: 'train'}\r\n   */\n  static adadelta(learningRate = .001, rho = .95, epsilon = null) {\n    return new AdadeltaOptimizer(learningRate, rho, epsilon);\n  }\n  /**\r\n   * Constructs a `tf.AdamaxOptimizer` that uses the Adamax algorithm.\r\n   * See [https://arxiv.org/abs/1412.6980](https://arxiv.org/abs/1412.6980)\r\n   *\r\n   * @param learningRate The learning rate to use for the Adamax gradient\r\n   * descent algorithm.\r\n   * @param beta1 The exponential decay rate for the 1st moment estimates.\r\n   * @param beta2 The exponential decay rate for the 2nd moment estimates.\r\n   * @param epsilon A small constant for numerical stability.\r\n   * @param decay The learning rate decay over each update.\r\n   *\r\n   * @doc {heading: 'Training', subheading: 'Optimizers', namespace: 'train'}\r\n   */\n  static adamax(learningRate = 0.002, beta1 = 0.9, beta2 = 0.999, epsilon = null, decay = 0.0) {\n    return new AdamaxOptimizer(learningRate, beta1, beta2, epsilon, decay);\n  }\n  /**\r\n   * Constructs a `tf.AdagradOptimizer` that uses the Adagrad algorithm.\r\n   * See\r\n   * [http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf](\r\n   * http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf)\r\n   * or\r\n   * [http://ruder.io/optimizing-gradient-descent/index.html#adagrad](\r\n   * http://ruder.io/optimizing-gradient-descent/index.html#adagrad)\r\n   *\r\n   * @param learningRate The learning rate to use for the Adagrad gradient\r\n   * descent algorithm.\r\n   * @param initialAccumulatorValue Starting value for the accumulators, must be\r\n   * positive.\r\n   *\r\n   * @doc {heading: 'Training', subheading: 'Optimizers', namespace: 'train'}\r\n   */\n  static adagrad(learningRate, initialAccumulatorValue = 0.1) {\n    return new AdagradOptimizer(learningRate, initialAccumulatorValue);\n  }\n}","map":null,"metadata":{},"sourceType":"module"}