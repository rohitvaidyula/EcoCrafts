{"ast":null,"code":"/**\r\n * @license\r\n * Copyright 2020 Google LLC. All Rights Reserved.\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n * http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n * =============================================================================\r\n */\nimport { backend_util, Softmax, util } from '@tensorflow/tfjs-core';\nimport { exp } from './Exp';\nimport { max } from './Max';\nimport { div } from './RealDiv';\nimport { reshape } from './Reshape';\nimport { sub } from './Sub';\nimport { sum } from './Sum';\nexport function softmax(args) {\n  const {\n    inputs,\n    backend,\n    attrs\n  } = args;\n  const {\n    logits\n  } = inputs;\n  const {\n    dim\n  } = attrs;\n  const logitsRank = logits.shape.length;\n  let $dim = dim;\n  if ($dim === -1) {\n    $dim = logitsRank - 1;\n  }\n  if ($dim !== logitsRank - 1) {\n    throw Error('Softmax along a non-last dimension is not yet supported. ' + `Logits was rank ${logitsRank} and dim was ${$dim}`);\n  }\n  const axes = util.parseAxisParam([$dim], logits.shape);\n  const maxLogit = max({\n    inputs: {\n      x: logits\n    },\n    backend,\n    attrs: {\n      reductionIndices: axes,\n      keepDims: false\n    }\n  });\n  const expandedShape = backend_util.expandShapeToKeepDim(maxLogit.shape, axes);\n  const maxLogitReshaped = reshape({\n    inputs: {\n      x: maxLogit\n    },\n    backend,\n    attrs: {\n      shape: expandedShape\n    }\n  });\n  const a = sub({\n    inputs: {\n      a: logits,\n      b: maxLogitReshaped\n    },\n    backend\n  });\n  const b = exp({\n    inputs: {\n      x: a\n    },\n    backend\n  });\n  const sumExp = sum({\n    inputs: {\n      x: b\n    },\n    backend,\n    attrs: {\n      axis: axes,\n      keepDims: false\n    }\n  });\n  const sumReshaped = reshape({\n    inputs: {\n      x: sumExp\n    },\n    backend,\n    attrs: {\n      shape: expandedShape\n    }\n  });\n  const result = div({\n    inputs: {\n      a: b,\n      b: sumReshaped\n    },\n    backend\n  });\n  backend.disposeIntermediateTensorInfo(maxLogit);\n  backend.disposeIntermediateTensorInfo(maxLogitReshaped);\n  backend.disposeIntermediateTensorInfo(a);\n  backend.disposeIntermediateTensorInfo(b);\n  backend.disposeIntermediateTensorInfo(sumExp);\n  backend.disposeIntermediateTensorInfo(sumReshaped);\n  return result;\n}\nexport const softmaxConfig = {\n  kernelName: Softmax,\n  backendName: 'cpu',\n  kernelFunc: softmax\n};","map":null,"metadata":{},"sourceType":"module"}