{"ast":null,"code":"/**\r\n * @license\r\n * Copyright 2017 Google LLC. All Rights Reserved.\r\n * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n * you may not use this file except in compliance with the License.\r\n * You may obtain a copy of the License at\r\n *\r\n * http://www.apache.org/licenses/LICENSE-2.0\r\n *\r\n * Unless required by applicable law or agreed to in writing, software\r\n * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n * See the License for the specific language governing permissions and\r\n * limitations under the License.\r\n * =============================================================================\r\n */\nimport * as util from './util';\n/**\r\n * Computes a list of TapeNodes that connect x to y, filtering everything else\r\n * out and preserving the order of the original tape elements.\r\n *\r\n * @param tape The tape elements to filter.\r\n * @param xs The input Tensors.\r\n * @param y The output Tensor.\r\n */\nexport function getFilteredNodesXToY(tape, xs, y) {\n  // Forward pass to compute all the nodes and Tensors that are transitively a\n  // function of x.\n  const tensorsFromX = {};\n  const nodesFromX = {};\n  for (let i = 0; i < xs.length; i++) {\n    tensorsFromX[xs[i].id] = true;\n  }\n  for (let i = 0; i < tape.length; i++) {\n    const node = tape[i];\n    const nodeInputs = node.inputs;\n    for (const inputName in nodeInputs) {\n      const input = nodeInputs[inputName];\n      let anyInputFromX = false;\n      for (let j = 0; j < xs.length; j++) {\n        if (tensorsFromX[input.id]) {\n          node.outputs.forEach(output => tensorsFromX[output.id] = true);\n          anyInputFromX = true;\n          nodesFromX[node.id] = true;\n          break;\n        }\n      }\n      if (anyInputFromX) {\n        break;\n      }\n    }\n  }\n  // Backward pass to find all of the nodes and Tensors that lead to y.\n  const tensorsLeadToY = {};\n  tensorsLeadToY[y.id] = true;\n  const nodesToY = {};\n  for (let i = tape.length - 1; i >= 0; i--) {\n    const node = tape[i];\n    const nodeInputs = node.inputs;\n    // If any of the outputs lead to y, mark all of the inputs as leading to y.\n    for (let j = 0; j < node.outputs.length; j++) {\n      if (tensorsLeadToY[node.outputs[j].id]) {\n        for (const inputName in nodeInputs) {\n          tensorsLeadToY[nodeInputs[inputName].id] = true;\n          nodesToY[node.id] = true;\n        }\n        break;\n      }\n    }\n  }\n  // Return the paths that come from x and lead to y.\n  const filteredTape = [];\n  for (let i = 0; i < tape.length; i++) {\n    const node = tape[i];\n    if (nodesFromX[node.id] && nodesToY[node.id]) {\n      // Prune the inputs from the node that aren't a function of x.\n      const prunedInputs = {};\n      for (const inputName in node.inputs) {\n        const nodeInput = node.inputs[inputName];\n        if (tensorsFromX[nodeInput.id]) {\n          prunedInputs[inputName] = nodeInput;\n        }\n      }\n      // Copy the node and overwrite inputsAndArgs to the pruned version.\n      const prunedNode = Object.assign({}, node);\n      prunedNode.inputs = prunedInputs;\n      prunedNode.outputs = node.outputs;\n      filteredTape.push(prunedNode);\n    }\n  }\n  return filteredTape;\n}\n/**\r\n * Backpropagate gradients through the filtered TapeNodes.\r\n *\r\n * @param tensorAccumulatedGradientMap A map of Tensor to its gradient. This map\r\n * is mutated by this method.\r\n * @param filteredTape The filtered TapeNodes to backprop through.\r\n */\nexport function backpropagateGradients(tensorAccumulatedGradientMap, filteredTape, tidy, add) {\n  // Walk the tape backward and keep a map of Tensor to its gradient.\n  for (let i = filteredTape.length - 1; i >= 0; i--) {\n    const node = filteredTape[i];\n    const dys = [];\n    node.outputs.forEach(o => {\n      const gradTensor = tensorAccumulatedGradientMap[o.id];\n      if (gradTensor != null) {\n        dys.push(gradTensor);\n      } else {\n        // This particular output is not in the back-propagation subgraph, so it\n        // does not affect the final output, thus we put null for its dy.\n        dys.push(null);\n      }\n    });\n    if (node.gradient == null) {\n      throw new Error(`Cannot compute gradient: gradient function not found ` + `for ${node.kernelName}.`);\n    }\n    // Backprop dy through this node and accumulate gradients over the inputs.\n    const inputGradients = node.gradient(dys);\n    for (const inputName in node.inputs) {\n      if (!(inputName in inputGradients)) {\n        throw new Error(`Cannot backprop through input ${inputName}. ` + `Available gradients found: ${Object.keys(inputGradients)}.`);\n      }\n      // Call the gradient function.\n      const dx = tidy(() => inputGradients[inputName]());\n      if (dx.dtype !== 'float32') {\n        throw new Error(`Error in gradient for op ${node.kernelName}. The gradient of input ` + `${inputName} must have 'float32' dtype, but has '${dx.dtype}'`);\n      }\n      const x = node.inputs[inputName];\n      if (!util.arraysEqual(dx.shape, x.shape)) {\n        throw new Error(`Error in gradient for op ${node.kernelName}. The gradient of input ` + `'${inputName}' has shape '${dx.shape}', which does not match ` + `the shape of the input '${x.shape}'`);\n      }\n      if (tensorAccumulatedGradientMap[x.id] == null) {\n        tensorAccumulatedGradientMap[x.id] = dx;\n      } else {\n        const curGradient = tensorAccumulatedGradientMap[x.id];\n        tensorAccumulatedGradientMap[x.id] = add(curGradient, dx);\n        curGradient.dispose();\n      }\n    }\n  }\n}","map":null,"metadata":{},"sourceType":"module"}