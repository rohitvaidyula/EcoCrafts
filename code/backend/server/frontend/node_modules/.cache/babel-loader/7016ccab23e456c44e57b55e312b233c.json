{"ast":null,"code":"/**\r\n * @license\r\n * Copyright 2018 Google LLC\r\n *\r\n * Use of this source code is governed by an MIT-style\r\n * license that can be found in the LICENSE file or at\r\n * https://opensource.org/licenses/MIT.\r\n * =============================================================================\r\n */\n// Layer activation functions\nimport * as tfc from '@tensorflow/tfjs-core';\nimport { serialization, tidy } from '@tensorflow/tfjs-core';\nimport * as K from './backend/tfjs_backend';\nimport { deserializeKerasObject } from './utils/generic_utils';\n/**\r\n * Base class for Activations.\r\n *\r\n * Special note: due to cross-language compatibility reasons, the\r\n * static readonly className field in this family of classes must be set to\r\n * the initialLowerCamelCase name of the activation.\r\n */\nexport class Activation extends serialization.Serializable {\n  getConfig() {\n    return {};\n  }\n}\n/**\r\n * Exponential linear unit (ELU).\r\n * Reference: https://arxiv.org/abs/1511.07289\r\n */\nexport class Elu extends Activation {\n  /**\r\n   * Calculate the activation function.\r\n   *\r\n   * @param x: Input.\r\n   * @param alpha: Scaling factor the negative section.\r\n   * @return Output of the ELU activation.\r\n   */\n  apply(x, alpha = 1) {\n    return K.elu(x, alpha);\n  }\n}\n/** @nocollapse */\nElu.className = 'elu';\nserialization.registerClass(Elu);\n/**\r\n * Scaled Exponential Linear Unit. (Klambauer et al., 2017).\r\n * Reference: Self-Normalizing Neural Networks, https://arxiv.org/abs/1706.02515\r\n * Notes:\r\n *   - To be used together with the initialization \"lecunNormal\".\r\n *   - To be used together with the dropout variant \"AlphaDropout\".\r\n */\nexport class Selu extends Activation {\n  apply(x) {\n    return tfc.selu(x);\n  }\n}\n/** @nocollapse */\nSelu.className = 'selu';\nserialization.registerClass(Selu);\n/**\r\n *  Rectified linear unit\r\n */\nexport class Relu extends Activation {\n  apply(x) {\n    return tfc.relu(x);\n  }\n}\n/** @nocollapse */\nRelu.className = 'relu';\nserialization.registerClass(Relu);\n/**\r\n * Rectified linear unit activation maxing out at 6.0.\r\n */\nexport class Relu6 extends Activation {\n  apply(x) {\n    return tidy(() => tfc.minimum(6.0, tfc.relu(x)));\n  }\n}\n/** @nocollapse */\nRelu6.className = 'relu6';\nserialization.registerClass(Relu6);\n//* Linear activation (no-op) */\nexport class Linear extends Activation {\n  apply(x) {\n    return x;\n  }\n}\n/** @nocollapse */\nLinear.className = 'linear';\nserialization.registerClass(Linear);\n/**\r\n * Sigmoid activation function.\r\n */\nexport class Sigmoid extends Activation {\n  apply(x) {\n    return tfc.sigmoid(x);\n  }\n}\n/** @nocollapse */\nSigmoid.className = 'sigmoid';\nserialization.registerClass(Sigmoid);\n/**\r\n * Segment-wise linear approximation of sigmoid.\r\n */\nexport class HardSigmoid extends Activation {\n  apply(x) {\n    return K.hardSigmoid(x);\n  }\n}\n/** @nocollapse */\nHardSigmoid.className = 'hardSigmoid';\nserialization.registerClass(HardSigmoid);\n/**\r\n * Softplus activation function.\r\n */\nexport class Softplus extends Activation {\n  apply(x) {\n    return tfc.softplus(x);\n  }\n}\n/** @nocollapse */\nSoftplus.className = 'softplus';\nserialization.registerClass(Softplus);\n/**\r\n * Softsign activation function.\r\n */\nexport class Softsign extends Activation {\n  apply(x) {\n    return K.softsign(x);\n  }\n}\n/** @nocollapse */\nSoftsign.className = 'softsign';\nserialization.registerClass(Softsign);\n/**\r\n * Hyperbolic tangent function.\r\n */\nexport class Tanh extends Activation {\n  apply(x) {\n    return tfc.tanh(x);\n  }\n}\n/** @nocollapse */\nTanh.className = 'tanh';\nserialization.registerClass(Tanh);\n/**\r\n * Softmax activation function\r\n */\nexport class Softmax extends Activation {\n  /**\r\n   * Calculate the activation function.\r\n   *\r\n   * @param x Tensor.\r\n   * @param axis Integer, axis along which the softmax normalization is applied.\r\n   * Invalid if < 2, as softmax across 1 (the batch dimension) is assumed to be\r\n   * an error.\r\n   *\r\n   * @returns a Tensor of the same shape as x\r\n   *\r\n   * @throws ValueError: In case `dim(x) < 2`.\r\n   */\n  apply(x, axis = -1) {\n    return tfc.softmax(x, axis);\n  }\n}\n/** @nocollapse */\nSoftmax.className = 'softmax';\nserialization.registerClass(Softmax);\n/**\r\n * Log softmax activation function\r\n */\nexport class LogSoftmax extends Activation {\n  /**\r\n   * Calculate the activation function of log softmax:\r\n   * log( exp(x_i) / sum(exp(x)) )\r\n   *\r\n   * @param x Tensor.\r\n   * @param axis Integer, axis along which the softmax normalization is applied.\r\n   * Invalid if < 2, as softmax across 1 (the batch dimension) is assumed to be\r\n   * an error.\r\n   *\r\n   * @returns a Tensor of the same shape as x\r\n   *\r\n   * @throws ValueError: In case `dim(x) < 2`.\r\n   */\n  apply(x, axis = -1) {\n    return tfc.logSoftmax(x, axis);\n  }\n}\n/** @nocollapse */\nLogSoftmax.className = 'logSoftmax';\nserialization.registerClass(LogSoftmax);\n/**\r\n * Swish activation function\r\n */\nexport class Swish extends Activation {\n  /**\r\n   * Calculate the activation function.\r\n   *\r\n   * @param x Tensor.\r\n   * @param alpha Scaling factor for the sigmoid function.\r\n   * @returns a Tensor of the same shape as x\r\n   */\n  apply(x, alpha = 1) {\n    return tidy(() => tfc.sigmoid(x.mul(alpha)).mul(x));\n  }\n}\n/** @nocollapse */\nSwish.className = 'swish';\nserialization.registerClass(Swish);\nexport function serializeActivation(activation) {\n  return activation.getClassName();\n}\nexport function deserializeActivation(config, customObjects = {}) {\n  return deserializeKerasObject(config, serialization.SerializationMap.getMap().classNameMap, customObjects, 'activation');\n}\nexport function getActivation(identifier) {\n  if (identifier == null) {\n    const config = {};\n    config['className'] = 'linear';\n    config['config'] = {};\n    return deserializeActivation(config);\n  }\n  if (typeof identifier === 'string') {\n    const config = {};\n    config['className'] = identifier;\n    config['config'] = {};\n    return deserializeActivation(config);\n  } else if (identifier instanceof Activation) {\n    return identifier;\n  } else {\n    return deserializeActivation(identifier);\n  }\n}","map":null,"metadata":{},"sourceType":"module"}